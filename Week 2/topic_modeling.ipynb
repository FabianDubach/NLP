{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d81604-025d-4fe1-a130-6a978f5ba135",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "In this exercise, we will do topic modeling with gensim. Use the [topics and transformations tutorial](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html) as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e45876ae-0f77-4bf8-8da4-b18618005327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import gensim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6efd1",
   "metadata": {},
   "source": [
    "For tokenizing words and stopword removal, download the NLTK punkt tokenizer and stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edf524f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fabia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fabia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee84f40-20bf-47da-b0b4-a0ff28f9b5cd",
   "metadata": {},
   "source": [
    "First, we load the [Lee Background Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf) included with gensim that contains 300 news articles of the Australian Broadcasting Corporation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d72e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "train_file = datapath('lee_background.cor')\n",
    "articles_orig = open(train_file).read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b2e56f",
   "metadata": {},
   "source": [
    "Preprocess the text by lowercasing, removing stopwords, stemming, and removing rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88a870af-9f6b-43ea-940f-558e9a21bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stopword list\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords = stopwords | {'\\\"', '\\'', '\\'\\'', '`', '``', '\\'s'}\n",
    "\n",
    "# initialize stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def preprocess(article):\n",
    "    # tokenize\n",
    "    article = nltk.word_tokenize(article)\n",
    "\n",
    "    # lowercase all words\n",
    "    article = [word.lower() for word in article]\n",
    "\n",
    "    # remove stopwords\n",
    "    article = [word for word in article if word not in stopwords]\n",
    "\n",
    "    # optional: stem\n",
    "    # article = [stemmer.stem(word) for word in article]\n",
    "    return article\n",
    "\n",
    "articles = [preprocess(article) for article in articles_orig]\n",
    "\n",
    "# create the dictionary and corpus objects that gensim uses for topic modeling\n",
    "dictionary = gensim.corpora.Dictionary(articles)\n",
    "\n",
    "# remove words that occur in less than 2 documents, or more than 50% of documents\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.5)\n",
    "temp = dictionary[0]  # load the dictionary by calling it once\n",
    "corpus_bow = [dictionary.doc2bow(article) for article in articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ae61a",
   "metadata": {},
   "source": [
    "\n",
    "Now we create a TF-IDF model and transform the corpus into TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fab13db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 7), (42, 1), (43, 1), (44, 1), (45, 3), (46, 1), (47, 1), (48, 2), (49, 2), (50, 3), (51, 3), (52, 1), (53, 2), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 2), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 8), (73, 1), (74, 1), (75, 1), (76, 2), (77, 1), (78, 1), (79, 2), (80, 1), (81, 1), (82, 3), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 5), (90, 1), (91, 2), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 3), (99, 1), (100, 1), (101, 3), (102, 1), (103, 1), (104, 1), (105, 4), (106, 2), (107, 1), (108, 1), (109, 1), (110, 1)]\n",
      "[(0, 0.045163832296308125), (1, 0.049004990699027966), (2, 0.09398031720792203), (3, 0.06797874731615453), (4, 0.08637534553463992), (5, 0.10158528888120417), (6, 0.058872481173046734), (7, 0.045871696227162966), (8, 0.04660732651093343), (9, 0.03476708703034139), (10, 0.09174339245432593), (11, 0.06379342938648586), (12, 0.08097953226203827), (13, 0.08637534553463992), (14, 0.06576958891547403), (15, 0.05748249959948285), (16, 0.07679421433236962), (17, 0.09398031720792203), (18, 0.04197717742438698), (19, 0.06379342938648586), (20, 0.09398031720792203), (21, 0.07679421433236962), (22, 0.08097953226203827), (23, 0.058872481173046734), (24, 0.05497796237027076), (25, 0.05497796237027076), (26, 0.07337456058875615), (27, 0.05497796237027076), (28, 0.08637534553463992), (29, 0.058872481173046734), (30, 0.062005775644911734), (31, 0.08637534553463992), (32, 0.09398031720792203), (33, 0.04737299069698862), (34, 0.07048328454536662), (35, 0.09398031720792203), (36, 0.09398031720792203), (37, 0.07679421433236962), (38, 0.06379342938648586), (39, 0.09398031720792203), (40, 0.05276880396959025), (41, 0.3161468260741569), (42, 0.06576958891547403), (43, 0.06576958891547403), (44, 0.04197717742438698), (45, 0.1860173269347352), (46, 0.08637534553463992), (47, 0.09398031720792203), (48, 0.17275069106927984), (49, 0.15358842866473923), (50, 0.1973087667464221), (51, 0.19138028815945754), (52, 0.06379342938648586), (53, 0.18796063441584407), (54, 0.07679421433236962), (55, 0.05384087678041912), (56, 0.07679421433236962), (57, 0.07679421433236962), (58, 0.08637534553463992), (59, 0.04318767276731996), (60, 0.13595749463230905), (61, 0.07048328454536662), (62, 0.06797874731615453), (63, 0.04318767276731996), (64, 0.08637534553463992), (65, 0.04448171465359908), (66, 0.049877527926200725), (67, 0.07337456058875615), (68, 0.05175471008582299), (69, 0.029876861457627475), (70, 0.043823535964961836), (71, 0.07337456058875615), (72, 0.1663540992526395), (73, 0.048171245973727274), (74, 0.09398031720792203), (75, 0.062005775644911734), (76, 0.04274284161044218), (77, 0.07337456058875615), (78, 0.06037377564287238), (79, 0.18796063441584407), (80, 0.09398031720792203), (81, 0.06379342938648586), (82, 0.23038264299710884), (83, 0.05618845771320373), (84, 0.08097953226203827), (85, 0.06379342938648586), (86, 0.07048328454536662), (87, 0.05384087678041912), (88, 0.06797874731615453), (89, 0.14342796675805272), (90, 0.07679421433236962), (91, 0.10995592474054151), (92, 0.06379342938648586), (93, 0.03976801902370649), (94, 0.0360042057531442), (95, 0.06797874731615453), (96, 0.07679421433236962), (97, 0.058872481173046734), (98, 0.11930405707111948), (99, 0.07679421433236962), (100, 0.030502124955654616), (101, 0.1860173269347352), (102, 0.05618845771320373), (103, 0.058872481173046734), (104, 0.08097953226203827), (105, 0.17529414385984735), (106, 0.11237691542640746), (107, 0.045871696227162966), (108, 0.08097953226203827), (109, 0.06037377564287238), (110, 0.03398546693692743)]\n"
     ]
    }
   ],
   "source": [
    "model_tfidf = gensim.models.TfidfModel(corpus_bow)\n",
    "corpus_tfidf = model_tfidf[corpus_bow]\n",
    "\n",
    "print(corpus_bow[0])\n",
    "print(corpus_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24df8cb",
   "metadata": {},
   "source": [
    "Now we train an [LDA model](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html) with 10 topics on the TF-IDF corpus. Save it to a variable `model_lda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ded6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lda = gensim.models.LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91845654",
   "metadata": {},
   "source": [
    "Let's inspect the first 5 topics of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca3a357e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9,\n",
       "  '0.002*\"government\" + 0.002*\"party\" + 0.002*\"federal\" + 0.002*\"group\" + 0.001*\"asylum\" + 0.001*\"cut\" + 0.001*\"palestinian\" + 0.001*\"pacific\" + 0.001*\"workers\" + 0.001*\"economy\"'),\n",
       " (5,\n",
       "  '0.002*\"asic\" + 0.002*\"afghanistan\" + 0.001*\"best\" + 0.001*\"guides\" + 0.001*\"river\" + 0.001*\"company\" + 0.001*\"adventure\" + 0.001*\"canyoning\" + 0.001*\"interlaken\" + 0.001*\"afghan\"'),\n",
       " (2,\n",
       "  '0.003*\"metres\" + 0.002*\"50\" + 0.002*\"agreement\" + 0.002*\"event\" + 0.001*\"bill\" + 0.001*\"militants\" + 0.001*\"palestinian\" + 0.001*\"reid\" + 0.001*\"mr\" + 0.001*\"us\"'),\n",
       " (1,\n",
       "  '0.002*\"palestinian\" + 0.002*\"south\" + 0.002*\"arafat\" + 0.002*\"israel\" + 0.002*\"hamas\" + 0.002*\"australia\" + 0.001*\"fire\" + 0.001*\"hewitt\" + 0.001*\"new\" + 0.001*\"club\"'),\n",
       " (6,\n",
       "  '0.003*\"palestinian\" + 0.002*\"people\" + 0.002*\"israeli\" + 0.002*\"security\" + 0.002*\"south\" + 0.002*\"suicide\" + 0.001*\"test\" + 0.001*\"police\" + 0.001*\"west\" + 0.001*\"mr\"')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lda.print_topics(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ce453",
   "metadata": {},
   "source": [
    "We see the 5 topics with the highest importance. For each topic, the 10 most important words are shown, together with their coefficient of \"alignment\" to the topic.\n",
    "\n",
    "## Document Similarity\n",
    "We now use our LDA model to compare the similarity of new documents (*queries*) to documents in our collection.\n",
    "\n",
    "First, create an index of the news articles in our corpus. Use the `MatrixSimilarity` transformation as described in gensim's [similarity queries tutorial](https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4eb44cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = gensim.similarities.MatrixSimilarity(model_lda[corpus_tfidf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7b2c1f",
   "metadata": {},
   "source": [
    "Now, write a function that takes a query string as input and returns the LDA representation for it. Make sure to apply the same preprocessing as we did to the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dabf9dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.050017152), (1, 0.050016955), (2, 0.5498435), (3, 0.050016955), (4, 0.050018985), (5, 0.05001799), (6, 0.050017077), (7, 0.050017323), (8, 0.050017122), (9, 0.050016955)]\n"
     ]
    }
   ],
   "source": [
    "query = \"Human computer interaction\"\n",
    "pre_query = preprocess(query)\n",
    "\n",
    "vec_bow = dictionary.doc2bow(pre_query)\n",
    "vec_lsi = model_lda[vec_bow]  # convert the query to LSI space\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77753be",
   "metadata": {},
   "source": [
    "Print the top 5 most similar documents, together with their similarities, using your index created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7696f2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.08776023), (1, 0.11016472), (2, 0.11762812), (3, 0.109708965), (4, 0.11271897), (5, 0.109133095), (6, 0.96472156), (7, 0.114336915), (8, 0.10710348), (9, 0.12098105), (10, 0.1111706), (11, 0.11224067), (12, 0.1226941), (13, 0.97582626), (14, 0.114903174), (15, 0.9758115), (16, 0.11691949), (17, 0.109932184), (18, 0.11802079), (19, 0.11478705), (20, 0.1250358), (21, 0.9758907), (22, 0.117768794), (23, 0.11247062), (24, 0.11598759), (25, 0.9733644), (26, 0.114097595), (27, 0.11557376), (28, 0.11066702), (29, 0.11033342), (30, 0.11582889), (31, 0.11038124), (32, 0.12694208), (33, 0.08775689), (34, 0.11050568), (35, 0.110986635), (36, 0.9750492), (37, 0.08775702), (38, 0.11258303), (39, 0.11051073), (40, 0.100570664), (41, 0.97649825), (42, 0.11038694), (43, 0.11252522), (44, 0.12987262), (45, 0.9749904), (46, 0.11437751), (47, 0.11200681), (48, 0.1062413), (49, 0.1267344), (50, 0.112948805), (51, 0.11688079), (52, 0.10707758), (53, 0.1154563), (54, 0.11446591), (55, 0.13366735), (56, 0.114365615), (57, 0.11373123), (58, 0.10815232), (59, 0.11015782), (60, 0.10673221), (61, 0.10993047), (62, 0.111522004), (63, 0.11474405), (64, 0.10931401), (65, 0.11234887), (66, 0.10966126), (67, 0.13827522), (68, 0.11752555), (69, 0.11022549), (70, 0.11243043), (71, 0.12449018), (72, 0.11608486), (73, 0.11585433), (74, 0.08776023), (75, 0.121104866), (76, 0.08775667), (77, 0.11048019), (78, 0.10886186), (79, 0.10629508), (80, 0.11494741), (81, 0.08775667), (82, 0.31802392), (83, 0.15597509), (84, 0.12120862), (85, 0.9766524), (86, 0.11572535), (87, 0.08775849), (88, 0.92973834), (89, 0.10971986), (90, 0.151568), (91, 0.1153852), (92, 0.12760146), (93, 0.14668047), (94, 0.11229997), (95, 0.9767218), (96, 0.12152849), (97, 0.113359906), (98, 0.08776023), (99, 0.10981461), (100, 0.97691214), (101, 0.111316755), (102, 0.113021955), (103, 0.11680487), (104, 0.08775702), (105, 0.1040323), (106, 0.08776023), (107, 0.1240128), (108, 0.11198478), (109, 0.13291672), (110, 0.114008635), (111, 0.11698598), (112, 0.08775702), (113, 0.9752624), (114, 0.08776023), (115, 0.1260206), (116, 0.106116384), (117, 0.11150044), (118, 0.08775702), (119, 0.12546284), (120, 0.11150044), (121, 0.114474736), (122, 0.112929985), (123, 0.11667664), (124, 0.11948753), (125, 0.1115274), (126, 0.11187676), (127, 0.97366774), (128, 0.11182793), (129, 0.107328355), (130, 0.13251871), (131, 0.115064934), (132, 0.14791119), (133, 0.29215494), (134, 0.123401344), (135, 0.1570519), (136, 0.11534336), (137, 0.11575052), (138, 0.10529662), (139, 0.109044954), (140, 0.10740156), (141, 0.97496074), (142, 0.11286671), (143, 0.11638212), (144, 0.109301426), (145, 0.14460675), (146, 0.1121355), (147, 0.13307445), (148, 0.11748142), (149, 0.115407005), (150, 0.106348336), (151, 0.08775697), (152, 0.10428898), (153, 0.08775689), (154, 0.12911081), (155, 0.1187135), (156, 0.10634829), (157, 0.08775667), (158, 0.11769054), (159, 0.9745663), (160, 0.109653205), (161, 0.10215949), (162, 0.97773826), (163, 0.39652622), (164, 0.11685721), (165, 0.9748619), (166, 0.1123095), (167, 0.13301916), (168, 0.17225769), (169, 0.11471385), (170, 0.10959379), (171, 0.11321484), (172, 0.10892815), (173, 0.11309168), (174, 0.12061389), (175, 0.11297351), (176, 0.1351417), (177, 0.11072677), (178, 0.111416146), (179, 0.107516415), (180, 0.115749866), (181, 0.11068625), (182, 0.108152874), (183, 0.110156074), (184, 0.10701243), (185, 0.8916089), (186, 0.11543628), (187, 0.14717034), (188, 0.11105055), (189, 0.12323232), (190, 0.11435445), (191, 0.14763471), (192, 0.10660312), (193, 0.11610917), (194, 0.111351624), (195, 0.11153841), (196, 0.97844607), (197, 0.11002263), (198, 0.1142994), (199, 0.113413855), (200, 0.11178835), (201, 0.13641417), (202, 0.112435594), (203, 0.14009637), (204, 0.109150454), (205, 0.11664913), (206, 0.11165622), (207, 0.980014), (208, 0.11233067), (209, 0.11658621), (210, 0.11149886), (211, 0.10943662), (212, 0.10954048), (213, 0.9447719), (214, 0.10668087), (215, 0.11547985), (216, 0.11552243), (217, 0.10866373), (218, 0.11561847), (219, 0.11561512), (220, 0.106645286), (221, 0.13252835), (222, 0.1223959), (223, 0.11064569), (224, 0.10757132), (225, 0.12082891), (226, 0.9768128), (227, 0.9746617), (228, 0.113463365), (229, 0.11152113), (230, 0.109355494), (231, 0.11249052), (232, 0.9729246), (233, 0.08775849), (234, 0.11037837), (235, 0.11255174), (236, 0.10935537), (237, 0.11510973), (238, 0.11480535), (239, 0.11285121), (240, 0.113929905), (241, 0.96472156), (242, 0.11507035), (243, 0.60932446), (244, 0.109339476), (245, 0.12383981), (246, 0.11516282), (247, 0.11223804), (248, 0.11481145), (249, 0.08775849), (250, 0.13797522), (251, 0.112346776), (252, 0.1117133), (253, 0.11678924), (254, 0.1288797), (255, 0.12624873), (256, 0.11429479), (257, 0.11303639), (258, 0.11589103), (259, 0.8330814), (260, 0.106861435), (261, 0.9759899), (262, 0.111861505), (263, 0.11119035), (264, 0.10879589), (265, 0.14154279), (266, 0.115870014), (267, 0.41135192), (268, 0.12526335), (269, 0.11474734), (270, 0.10893063), (271, 0.11119035), (272, 0.10842021), (273, 0.10840543), (274, 0.10863311), (275, 0.120104216), (276, 0.12089749), (277, 0.1100649), (278, 0.14125001), (279, 0.10947147), (280, 0.1195593), (281, 0.10951531), (282, 0.11264857), (283, 0.13849303), (284, 0.12545465), (285, 0.11039481), (286, 0.11511865), (287, 0.12569454), (288, 0.10951524), (289, 0.113003105), (290, 0.12634088), (291, 0.9745918), (292, 0.112949565), (293, 0.11145227), (294, 0.11541547), (295, 0.9758917), (296, 0.11428923), (297, 0.11640553), (298, 0.11711328), (299, 0.1263662)]\n",
      "0.980014 ['geoff', 'huegill', 'continued', 'record-breaking', 'ways', 'world', 'cup', 'short', 'course', 'swimming', 'melbourne', ',', 'bettering', 'australian', 'record', '100', 'metres', 'butterfly', '.', 'huegill', 'beat', 'fellow', 'australian', 'michael', 'klim', ',', 'backing', 'last', 'night', 'setting', 'world', 'record', '50', 'metres', 'butterfly', '.']\n",
      "0.97844607 ['olympic', '400', 'metres', 'champion', 'cathy', 'freeman', 'return', 'competition', 'melbourne', 'track', 'classic', 'march', '7', '.', 'freeman', 'began', 'training', 'six', 'weeks', 'ago', 'taking', 'break', 'sport', 'following', 'sydney', 'olympics', '.', 'melbourne', 'track', 'classic', 'lead-up', 'event', 'australian', 'championships', 'brisbane', ',', 'double', 'commonwealth', 'games', 'selection', 'trials', '.']\n",
      "0.97773826 ['federal', 'agriculture', 'minister', ',', 'warren', 'truss', ',', 'says', 'able', 'win', 'changes', 'farm', 'bill', 'debated', 'united', 'states', 'congress', '.', 'mr', 'truss', 'led', 'delegation', 'australian', 'farmers', 'washington', 'lobbying', 'government', 'subsidies', 'farmers', 'removed', '.', 'says', 'achieved', 'changes', 'amount', 'government', 'protection', 'us', 'farmers', '.', 'mr', 'truss', 'says', 'mean', 'australian', 'farmers', 'suffer', '.', \"'re\", 'especially', 'concerned', 'clear', 'intent', 'farm', 'lobby', 'seek', 'entrench', 'mentality', 'farm', 'subsidies', 'usa', '.', 'obvious', 'us', ',', 'proudly', 'boasted', 'efficient', 'farmers', 'world', ',', 'degenerated', 'situation', 'us', 'farmers', 'dependent', 'taxpayers', 'around', 'half', 'income', ',', 'mr', 'truss', 'said', '.']\n",
      "0.97691214 ['northern', 'territory', 'coroner', 'found', 'aboriginal', 'boy', 'died', 'custody', 'nearly', 'two', 'years', 'ago', 'detention', '.', 'february', 'last', 'year', ',', 'teenager', 'died', 'hospital', 'compression', 'neck', 'hanging', 'dale', 'juvenile', 'detention', 'centre', '.', 'report', ',', 'coroner', ',', 'dick', 'wallace', ',', 'said', '15-year-old', 'lonely', 'negelected', 'orphan', 'offending', 'since', '13', '.', 'january', 'last', 'year', ',', 'teenager', 'given', 'mandatory', '28-day', 'sentence', 'stealing', 'stationery', '.', 'however', ',', 'coroner', 'said', 'boy', 'eligible', 'undertake', 'diversionary', 'program', 'victim-offender', 'conferencing', 'instead', 'sent', 'detention', '.', 'court', 'considered', 'option', 'neither', 'prosecutor', 'boy', 'lawyer', 'told', 'magistrate', 'alternative', 'custodial', 'sentence', '.']\n",
      "0.9768128 ['three', 'us', 'soldiers', 'killed', 'misguided', 'us', 'bomb', 'afghanistan', 'us', 'army', 'special', 'forces', 'unit', 'based', 'fort', 'campbell', ',', 'kentucky', ',', 'pentagon', 'said', '.', 'three', 'identified', 'master', 'sergeant', 'jefferson', 'donald', 'davis', ',', '39', ',', 'tennessee', ';', 'sergeant', 'first', 'class', 'daniel', 'henry', 'petithory', ',', '32', ',', 'massachusetts', ';', 'staff', 'sergeant', 'brian', 'cody', 'prosser', ',', '28', ',', 'california', '.', 'pentagon', 'said', 'served', '3rd', 'battalion', ',', '5th', 'special', 'forces', 'group', ',', 'based', 'fort', 'campbell', ',', 'kentucky', '.', 'three', 'killed', '900', 'kilogram', 'bomb', 'dropped', 'air', 'force', 'b-52', 'bomber', 'landed', 'close', 'position', 'north', 'kandahar', 'southern', 'afghanistan', '.']\n"
     ]
    }
   ],
   "source": [
    "sims = index[vec_lsi]\n",
    "print(list(enumerate(sims)))\n",
    "\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "for doc_position, doc_score in sims[:5]:\n",
    "    print(doc_score, articles[doc_position])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e05dba",
   "metadata": {},
   "source": [
    "Run your code again, now training an LDA model with 100 topics. Do you see a qualitative difference in the top-5 most similar documents?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
