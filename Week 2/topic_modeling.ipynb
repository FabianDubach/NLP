{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d81604-025d-4fe1-a130-6a978f5ba135",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "In this exercise, we will do topic modeling with gensim. Use the [topics and transformations tutorial](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html) as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e45876ae-0f77-4bf8-8da4-b18618005327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import gensim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6efd1",
   "metadata": {},
   "source": [
    "For tokenizing words and stopword removal, download the NLTK punkt tokenizer and stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edf524f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fabia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fabia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee84f40-20bf-47da-b0b4-a0ff28f9b5cd",
   "metadata": {},
   "source": [
    "First, we load the [Lee Background Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf) included with gensim that contains 300 news articles of the Australian Broadcasting Corporation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d72e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "train_file = datapath('lee_background.cor')\n",
    "articles_orig = open(train_file).read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b2e56f",
   "metadata": {},
   "source": [
    "Preprocess the text by lowercasing, removing stopwords, stemming, and removing rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88a870af-9f6b-43ea-940f-558e9a21bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stopword list\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords = stopwords | {'\\\"', '\\'', '\\'\\'', '`', '``', '\\'s'}\n",
    "\n",
    "# initialize stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def preprocess(article):\n",
    "    # tokenize\n",
    "    article = nltk.word_tokenize(article)\n",
    "\n",
    "    # lowercase all words\n",
    "    article = [word.lower() for word in article]\n",
    "\n",
    "    # remove stopwords\n",
    "    article = [word for word in article if word not in stopwords]\n",
    "\n",
    "    # optional: stem\n",
    "    # article = [stemmer.stem(word) for word in article]\n",
    "    return article\n",
    "\n",
    "articles = [preprocess(article) for article in articles_orig]\n",
    "\n",
    "# create the dictionary and corpus objects that gensim uses for topic modeling\n",
    "dictionary = gensim.corpora.Dictionary(articles)\n",
    "\n",
    "# remove words that occur in less than 2 documents, or more than 50% of documents\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.5)\n",
    "temp = dictionary[0]  # load the dictionary by calling it once\n",
    "corpus_bow = [dictionary.doc2bow(article) for article in articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ae61a",
   "metadata": {},
   "source": [
    "\n",
    "Now we create a TF-IDF model and transform the corpus into TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fab13db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 7), (42, 1), (43, 1), (44, 1), (45, 3), (46, 1), (47, 1), (48, 2), (49, 2), (50, 3), (51, 3), (52, 1), (53, 2), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 2), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 8), (73, 1), (74, 1), (75, 1), (76, 2), (77, 1), (78, 1), (79, 2), (80, 1), (81, 1), (82, 3), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 5), (90, 1), (91, 2), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 3), (99, 1), (100, 1), (101, 3), (102, 1), (103, 1), (104, 1), (105, 4), (106, 2), (107, 1), (108, 1), (109, 1), (110, 1)]\n",
      "[(0, 0.045163832296308125), (1, 0.049004990699027966), (2, 0.09398031720792203), (3, 0.06797874731615453), (4, 0.08637534553463992), (5, 0.10158528888120417), (6, 0.058872481173046734), (7, 0.045871696227162966), (8, 0.04660732651093343), (9, 0.03476708703034139), (10, 0.09174339245432593), (11, 0.06379342938648586), (12, 0.08097953226203827), (13, 0.08637534553463992), (14, 0.06576958891547403), (15, 0.05748249959948285), (16, 0.07679421433236962), (17, 0.09398031720792203), (18, 0.04197717742438698), (19, 0.06379342938648586), (20, 0.09398031720792203), (21, 0.07679421433236962), (22, 0.08097953226203827), (23, 0.058872481173046734), (24, 0.05497796237027076), (25, 0.05497796237027076), (26, 0.07337456058875615), (27, 0.05497796237027076), (28, 0.08637534553463992), (29, 0.058872481173046734), (30, 0.062005775644911734), (31, 0.08637534553463992), (32, 0.09398031720792203), (33, 0.04737299069698862), (34, 0.07048328454536662), (35, 0.09398031720792203), (36, 0.09398031720792203), (37, 0.07679421433236962), (38, 0.06379342938648586), (39, 0.09398031720792203), (40, 0.05276880396959025), (41, 0.3161468260741569), (42, 0.06576958891547403), (43, 0.06576958891547403), (44, 0.04197717742438698), (45, 0.1860173269347352), (46, 0.08637534553463992), (47, 0.09398031720792203), (48, 0.17275069106927984), (49, 0.15358842866473923), (50, 0.1973087667464221), (51, 0.19138028815945754), (52, 0.06379342938648586), (53, 0.18796063441584407), (54, 0.07679421433236962), (55, 0.05384087678041912), (56, 0.07679421433236962), (57, 0.07679421433236962), (58, 0.08637534553463992), (59, 0.04318767276731996), (60, 0.13595749463230905), (61, 0.07048328454536662), (62, 0.06797874731615453), (63, 0.04318767276731996), (64, 0.08637534553463992), (65, 0.04448171465359908), (66, 0.049877527926200725), (67, 0.07337456058875615), (68, 0.05175471008582299), (69, 0.029876861457627475), (70, 0.043823535964961836), (71, 0.07337456058875615), (72, 0.1663540992526395), (73, 0.048171245973727274), (74, 0.09398031720792203), (75, 0.062005775644911734), (76, 0.04274284161044218), (77, 0.07337456058875615), (78, 0.06037377564287238), (79, 0.18796063441584407), (80, 0.09398031720792203), (81, 0.06379342938648586), (82, 0.23038264299710884), (83, 0.05618845771320373), (84, 0.08097953226203827), (85, 0.06379342938648586), (86, 0.07048328454536662), (87, 0.05384087678041912), (88, 0.06797874731615453), (89, 0.14342796675805272), (90, 0.07679421433236962), (91, 0.10995592474054151), (92, 0.06379342938648586), (93, 0.03976801902370649), (94, 0.0360042057531442), (95, 0.06797874731615453), (96, 0.07679421433236962), (97, 0.058872481173046734), (98, 0.11930405707111948), (99, 0.07679421433236962), (100, 0.030502124955654616), (101, 0.1860173269347352), (102, 0.05618845771320373), (103, 0.058872481173046734), (104, 0.08097953226203827), (105, 0.17529414385984735), (106, 0.11237691542640746), (107, 0.045871696227162966), (108, 0.08097953226203827), (109, 0.06037377564287238), (110, 0.03398546693692743)]\n"
     ]
    }
   ],
   "source": [
    "model_tfidf = gensim.models.TfidfModel(corpus_bow)\n",
    "corpus_tfidf = model_tfidf[corpus_bow]\n",
    "\n",
    "print(corpus_bow[0])\n",
    "print(corpus_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24df8cb",
   "metadata": {},
   "source": [
    "Now we train an [LDA model](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html) with 10 topics on the TF-IDF corpus. Save it to a variable `model_lda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ded6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lda = gensim.models.LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91845654",
   "metadata": {},
   "source": [
    "Let's inspect the first 5 topics of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca3a357e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2,\n",
       "  '0.002*\"detainees\" + 0.002*\"woomera\" + 0.002*\"arafat\" + 0.002*\"centre\" + 0.002*\"fire\" + 0.002*\"palestinian\" + 0.002*\"night\" + 0.001*\"labor\" + 0.001*\"south\" + 0.001*\"firefighters\"'),\n",
       " (0,\n",
       "  '0.002*\"afghanistan\" + 0.002*\"river\" + 0.002*\"bin\" + 0.002*\"laden\" + 0.002*\"man\" + 0.002*\"metres\" + 0.002*\"mr\" + 0.002*\"world\" + 0.001*\"states\" + 0.001*\"palestinian\"'),\n",
       " (9,\n",
       "  '0.003*\"hih\" + 0.002*\"company\" + 0.002*\"commission\" + 0.002*\"report\" + 0.002*\"palestinian\" + 0.002*\"royal\" + 0.002*\"mr\" + 0.001*\"killed\" + 0.001*\"year\" + 0.001*\"road\"'),\n",
       " (7,\n",
       "  '0.002*\"radio\" + 0.001*\"world\" + 0.001*\"south\" + 0.001*\"two\" + 0.001*\"abloy\" + 0.001*\"assa\" + 0.001*\"river\" + 0.001*\"krishna\" + 0.001*\"warne\" + 0.001*\"week\"'),\n",
       " (6,\n",
       "  '0.002*\"qantas\" + 0.002*\"reid\" + 0.002*\"israeli\" + 0.002*\"palestinian\" + 0.002*\"mr\" + 0.001*\"australian\" + 0.001*\"suicide\" + 0.001*\"hewitt\" + 0.001*\"workers\" + 0.001*\"maintenance\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lda.print_topics(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ce453",
   "metadata": {},
   "source": [
    "We see the 5 topics with the highest importance. For each topic, the 10 most important words are shown, together with their coefficient of \"alignment\" to the topic.\n",
    "\n",
    "## Document Similarity\n",
    "We now use our LDA model to compare the similarity of new documents (*queries*) to documents in our collection.\n",
    "\n",
    "First, create an index of the news articles in our corpus. Use the `MatrixSimilarity` transformation as described in gensim's [similarity queries tutorial](https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eb44cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = gensim.similarities.MatrixSimilarity(model_lda[corpus_tfidf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7b2c1f",
   "metadata": {},
   "source": [
    "Now, write a function that takes a query string as input and returns the LDA representation for it. Make sure to apply the same preprocessing as we did to the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dabf9dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.05001586), (1, 0.050014295), (2, 0.050014537), (3, 0.05001436), (4, 0.050014295), (5, 0.5498681), (6, 0.05001476), (7, 0.050014295), (8, 0.050015204), (9, 0.050014295)]\n"
     ]
    }
   ],
   "source": [
    "query = \"Human computer interaction\"\n",
    "pre_query = preprocess(query)\n",
    "\n",
    "vec_bow = dictionary.doc2bow(pre_query)\n",
    "vec_lsi = model_lda[vec_bow]  # convert the query to LSI space\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77753be",
   "metadata": {},
   "source": [
    "Print the top 5 most similar documents, together with their similarities, using your index created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7696f2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.10241625), (1, 0.11014203), (2, 0.11761003), (3, 0.109698236), (4, 0.11270343), (5, 0.109107696), (6, 0.087748684), (7, 0.114298336), (8, 0.12805778), (9, 0.12096505), (10, 0.97500217), (11, 0.9754217), (12, 0.106940836), (13, 0.11324691), (14, 0.11486876), (15, 0.11321192), (16, 0.11692338), (17, 0.10992997), (18, 0.11799764), (19, 0.114796124), (20, 0.97821873), (21, 0.113405384), (22, 0.11777974), (23, 0.11245378), (24, 0.11595931), (25, 0.12004268), (26, 0.114118755), (27, 0.11547435), (28, 0.110628165), (29, 0.11037162), (30, 0.11573291), (31, 0.1321513), (32, 0.98098457), (33, 0.9265822), (34, 0.110535875), (35, 0.14734425), (36, 0.9750854), (37, 0.08775028), (38, 0.112546995), (39, 0.11048309), (40, 0.109421395), (41, 0.11492355), (42, 0.11039954), (43, 0.11251092), (44, 0.109232135), (45, 0.11113871), (46, 0.114339076), (47, 0.11196843), (48, 0.10619465), (49, 0.10646482), (50, 0.11291571), (51, 0.11684278), (52, 0.10700049), (53, 0.11541241), (54, 0.11445456), (55, 0.108902484), (56, 0.11436459), (57, 0.11372738), (58, 0.14867978), (59, 0.11022006), (60, 0.114111856), (61, 0.109964125), (62, 0.12308249), (63, 0.11473898), (64, 0.109378695), (65, 0.1122869), (66, 0.9743818), (67, 0.11756493), (68, 0.11747382), (69, 0.11021273), (70, 0.11242245), (71, 0.12450985), (72, 0.11613374), (73, 0.9768736), (74, 0.904887), (75, 0.13442144), (76, 0.96472824), (77, 0.110466614), (78, 0.10883491), (79, 0.9730157), (80, 0.11488662), (81, 0.087748796), (82, 0.087751426), (83, 0.2629869), (84, 0.10874164), (85, 0.1153313), (86, 0.115760736), (87, 0.087748684), (88, 0.11928563), (89, 0.10975433), (90, 0.1087959), (91, 0.11537653), (92, 0.11241098), (93, 0.13856447), (94, 0.112339586), (95, 0.11555245), (96, 0.106289856), (97, 0.14562675), (98, 0.087751426), (99, 0.10979268), (100, 0.11600334), (101, 0.11135496), (102, 0.11297586), (103, 0.116724595), (104, 0.087748796), (105, 0.08774911), (106, 0.10881607), (107, 0.087751426), (108, 0.111974955), (109, 0.10921436), (110, 0.11399599), (111, 0.11698811), (112, 0.087748796), (113, 0.11178247), (114, 0.121257395), (115, 0.109854), (116, 0.121450946), (117, 0.8719693), (118, 0.10846131), (119, 0.10985397), (120, 0.8726029), (121, 0.11448321), (122, 0.11292804), (123, 0.11664327), (124, 0.11945403), (125, 0.12738374), (126, 0.111879475), (127, 0.10790043), (128, 0.111841545), (129, 0.10737013), (130, 0.110311285), (131, 0.10453946), (132, 0.13353857), (133, 0.10713995), (134, 0.107377484), (135, 0.11283851), (136, 0.11533938), (137, 0.11573863), (138, 0.10780363), (139, 0.10903787), (140, 0.107420325), (141, 0.9749729), (142, 0.11287135), (143, 0.11639547), (144, 0.10926853), (145, 0.12071516), (146, 0.97539985), (147, 0.087751426), (148, 0.112060316), (149, 0.11539343), (150, 0.10618924), (151, 0.10740781), (152, 0.10016686), (153, 0.89664984), (154, 0.11307845), (155, 0.1187578), (156, 0.106189065), (157, 0.087751426), (158, 0.117641345), (159, 0.11013758), (160, 0.109630145), (161, 0.11462481), (162, 0.11814368), (163, 0.13245644), (164, 0.11686894), (165, 0.11082029), (166, 0.11230819), (167, 0.11895999), (168, 0.14324799), (169, 0.11469154), (170, 0.10960331), (171, 0.11320572), (172, 0.10887792), (173, 0.11308581), (174, 0.12058431), (175, 0.11299137), (176, 0.14097956), (177, 0.95791215), (178, 0.11143336), (179, 0.10750607), (180, 0.115705), (181, 0.11067572), (182, 0.14980431), (183, 0.15339044), (184, 0.1133533), (185, 0.10720131), (186, 0.97669023), (187, 0.112874776), (188, 0.111049704), (189, 0.12330913), (190, 0.11433535), (191, 0.08775028), (192, 0.13718425), (193, 0.11613116), (194, 0.14687079), (195, 0.14878996), (196, 0.9784554), (197, 0.11002173), (198, 0.114332184), (199, 0.93398064), (200, 0.12016229), (201, 0.13512263), (202, 0.9755445), (203, 0.10636351), (204, 0.10915602), (205, 0.116642855), (206, 0.111618936), (207, 0.12429083), (208, 0.11232452), (209, 0.9771517), (210, 0.14650913), (211, 0.10944426), (212, 0.10958805), (213, 0.13459764), (214, 0.12631501), (215, 0.12826799), (216, 0.97670764), (217, 0.13812695), (218, 0.1156149), (219, 0.11561737), (220, 0.121702164), (221, 0.11135482), (222, 0.12240758), (223, 0.110652104), (224, 0.12204187), (225, 0.120838515), (226, 0.11578758), (227, 0.110384636), (228, 0.113468446), (229, 0.11151291), (230, 0.109350346), (231, 0.1125601), (232, 0.10611882), (233, 0.44978684), (234, 0.11035935), (235, 0.1125276), (236, 0.10935033), (237, 0.11509624), (238, 0.9764483), (239, 0.11288257), (240, 0.11392957), (241, 0.087751426), (242, 0.11505935), (243, 0.96472824), (244, 0.109366044), (245, 0.10939142), (246, 0.11517039), (247, 0.11224053), (248, 0.11479362), (249, 0.1006069), (250, 0.14570266), (251, 0.09749834), (252, 0.11170334), (253, 0.116776906), (254, 0.109266594), (255, 0.10879814), (256, 0.11424334), (257, 0.11298452), (258, 0.115920275), (259, 0.087748796), (260, 0.120223075), (261, 0.113704816), (262, 0.11178461), (263, 0.11113014), (264, 0.12960866), (265, 0.11076678), (266, 0.976871), (267, 0.32247627), (268, 0.91048187), (269, 0.114766374), (270, 0.10894929), (271, 0.11113013), (272, 0.087751426), (273, 0.10838034), (274, 0.1086363), (275, 0.08774911), (276, 0.1208794), (277, 0.10997928), (278, 0.11619087), (279, 0.109417364), (280, 0.11956068), (281, 0.109570004), (282, 0.11261515), (283, 0.13562989), (284, 0.14123344), (285, 0.11040566), (286, 0.11510399), (287, 0.14826316), (288, 0.109569974), (289, 0.11297316), (290, 0.1263103), (291, 0.11021358), (292, 0.11297794), (293, 0.9751065), (294, 0.115368724), (295, 0.9758914), (296, 0.11426381), (297, 0.11642313), (298, 0.11709394), (299, 0.107770674)]\n",
      "0.98098457 An earthquake measuring 4.1 on the Richter scale has shaken parts of Western Australia's wheatbelt overnight. Geo-science Australia says the epicentre was in Burakin, 240 kilometres north-east of Perth. A spokesman says the earthquake, which occurred at about 12:30am, follows a larger quake in September which measured over five on the Richter scale. Shane Bradford of Ballidu just west of Burakin, says the quake shook his house and woke him from his sleep. \n",
      "0.9784554 Olympic 400 metres champion Cathy Freeman will return to competition at the Melbourne Track Classic on March 7. Freeman began training six weeks ago after taking a break from the sport following the Sydney Olympics. The Melbourne Track Classic is a lead-up event to the Australian Championships in Brisbane, which double as the Commonwealth Games selection trials. \n",
      "0.97821873 Argentine President Adolfo Rodriguez Saa has asked the country's banks to help re-establish peace by facilitating the payment of pensions and salaries to workers and retirees. He says he issued the appeal at a meeting with leaders of the banking community. \"I'm very concerned about what has happened in Argentina,\" Mr Rodriguez Saa said. He says he has asked banks to remain open from 8:00am to 8:00pm Monday, to be able to cash checks of up to 1,000 pesos or $US1,000 per person. \n",
      "0.9771517 The Federal Government has confirmed there is a blowout in the Defence budget because of the cost of sending troops to Afghanistan. Defence Minister Robert Hill says the Government will have to consider delaying some Defence projects because of the cost of sending troops to the war against terrorism, but says no projects will be scrapped. Senator Hill says the cost of the deployment means some projects outlined in last year's Defence white paper will have to be reprioritised. He says the Government's options are to boost defence spending or to delay some projects. \"Now what I'm saying is you can't enter into major undertakings like contributing to the war against terrorism without knowing it's going to cost extra money,\" he said. \"We have to fund that, there are a range of options to do that.\" The Federal Opposition says Senator Hill must reveal which projects would be affected. Shadow Defence Minister Chris Evans says the Government should also come clean on the cost of using the Navy to intercept asylum seekers. \n",
      "0.9768736 The Prime Minister has thrown his full support behind the Governor-General, Dr Peter Hollingworth. Child rights campaigners have accused Dr Hollingworth of trying to cover-up child abuse allegations at a Toowoomba Anglican school when he was Archbishop of Brisbane. In a statement released earlier this week, the Governor-General said the allegations were unfounded, but there are continuing calls for Dr Hollingworth to resign. But Mr Howard says he has confidence in the Governor-General. \"I don't have any direct knowledge of this [matter but] I've talked to him about it and I've tried to form a judgment,\" Mr Howard said. \"The criticism made is that he's involved in a cover-up, well there's no evidence of that, that's ridiculous.\" \n"
     ]
    }
   ],
   "source": [
    "sims = index[vec_lsi]\n",
    "print(list(enumerate(sims)))\n",
    "\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "for doc_position, doc_score in sims[:5]:\n",
    "    print(doc_score, articles_orig[doc_position])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e05dba",
   "metadata": {},
   "source": [
    "Run your code again, now training an LDA model with 100 topics. Do you see a qualitative difference in the top-5 most similar documents?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
