{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "Epoch 1, Loss: 1.6098, Val Accuracy: 0.1974\n",
      "Epoch 2, Loss: 1.6093, Val Accuracy: 0.2170\n",
      "Epoch 3, Loss: 1.6059, Val Accuracy: 0.2064\n",
      "Epoch 4, Loss: 1.6028, Val Accuracy: 0.2080\n",
      "Epoch 5, Loss: 1.5976, Val Accuracy: 0.2269\n",
      "Epoch 6, Loss: 1.5900, Val Accuracy: 0.2146\n",
      "Epoch 7, Loss: 1.5831, Val Accuracy: 0.2031\n",
      "Epoch 8, Loss: 1.5729, Val Accuracy: 0.2072\n",
      "Epoch 9, Loss: 1.5619, Val Accuracy: 0.2056\n",
      "Epoch 10, Loss: 1.5494, Val Accuracy: 0.2031\n",
      "Training RNN Model...\n",
      "Epoch 1, Loss: 1.6104, Val Accuracy: 0.2088\n",
      "Epoch 2, Loss: 1.6098, Val Accuracy: 0.1925\n",
      "Epoch 3, Loss: 1.6087, Val Accuracy: 0.1966\n",
      "Epoch 4, Loss: 1.6068, Val Accuracy: 0.1998\n",
      "Epoch 5, Loss: 1.6056, Val Accuracy: 0.2064\n",
      "Epoch 6, Loss: 1.6014, Val Accuracy: 0.2138\n",
      "Epoch 7, Loss: 1.5968, Val Accuracy: 0.2015\n",
      "Epoch 8, Loss: 1.5904, Val Accuracy: 0.2129\n",
      "Epoch 9, Loss: 1.5839, Val Accuracy: 0.1982\n",
      "Epoch 10, Loss: 1.5773, Val Accuracy: 0.1892\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "import gensim.downloader as api\n",
    "# import wandb  # For logging\n",
    "\n",
    "# # Initialize Weights & Biases\n",
    "# wandb.init(project=\"commonsense_qa\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load and Preprocess Dataset\n",
    "# -----------------------------\n",
    "dataset = load_dataset(\"commonsense_qa\")\n",
    "train_data, val_data = dataset['train'], dataset['validation']\n",
    "\n",
    "# Preprocess text (Tokenization)\n",
    "def preprocess(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "train_sentences = [preprocess(q) for q in train_data['question']]\n",
    "val_sentences = [preprocess(q) for q in val_data['question']]\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load Pretrained Word2Vec Model\n",
    "# -----------------------------\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Function to get sentence embeddings\n",
    "def get_embedding(sentence, model):\n",
    "    vectors = [model[word] for word in sentence if word in model]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "# Convert dataset into embeddings\n",
    "X_train = np.array([get_embedding(q, wv) for q in train_sentences])\n",
    "X_val = np.array([get_embedding(q, wv) for q in val_sentences])\n",
    "\n",
    "# Convert labels to numerical values\n",
    "label_map = {label: idx for idx, label in enumerate(set(train_data['answerKey']))}\n",
    "y_train = np.array([label_map[label] for label in train_data['answerKey']])\n",
    "y_val = np.array([label_map[label] for label in val_data['answerKey']])\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Create PyTorch Dataset\n",
    "# -----------------------------\n",
    "class CommonsenseDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = CommonsenseDataset(X_train, y_train)\n",
    "val_dataset = CommonsenseDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Model Architectures\n",
    "# -----------------------------\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        x = self.relu(self.fc1(hidden[-1]))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = 300  # Word2Vec embedding size\n",
    "hidden_dim = 128\n",
    "output_dim = len(label_map)\n",
    "\n",
    "# Instantiate models\n",
    "classifier = SimpleClassifier(input_dim, hidden_dim, output_dim)\n",
    "rnn_model = RNNModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Training Function\n",
    "# -----------------------------\n",
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        val_acc = evaluate_model(model, val_loader)\n",
    "        # wandb.log({\"Epoch\": epoch + 1, \"Loss\": total_loss / len(train_loader), \"Val Accuracy\": val_acc})\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Evaluation Function\n",
    "# -----------------------------\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Train and Evaluate Classifier\n",
    "print(\"Training Classifier...\")\n",
    "train_model(classifier, train_loader, val_loader, epochs=10)\n",
    "\n",
    "# Train and Evaluate RNN Model\n",
    "print(\"Training RNN Model...\")\n",
    "train_model(rnn_model, train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where do you put your grapes just before checking out?\n",
      "Predicted Answer: D\n"
     ]
    }
   ],
   "source": [
    "def ask_question(question, model, label_map):\n",
    "    # Preprocess the question\n",
    "    question_tokens = preprocess(question)\n",
    "    question_embedding = get_embedding(question_tokens, wv)\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    question_tensor = torch.tensor(question_embedding, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Predict answer\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(question_tensor)\n",
    "        _, predicted_label = torch.max(output, 1)\n",
    "\n",
    "    # Reverse the label map to get the actual answer choice\n",
    "    reversed_label_map = {v: k for k, v in label_map.items()}\n",
    "    predicted_answer = reversed_label_map[predicted_label.item()]\n",
    "\n",
    "    # Print question and answer\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Predicted Answer: {predicted_answer}\")\n",
    "    return predicted_answer\n",
    "\n",
    "# Example usage\n",
    "question = \"Where do you put your grapes just before checking out?\"\n",
    "predicted_answer = ask_question(question, classifier, label_map)  # Try classifier or rnn_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
