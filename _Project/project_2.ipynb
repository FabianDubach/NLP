{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4357e38e",
   "metadata": {},
   "source": [
    "## Step 1: Set Up the CommonsenseQA Dataset\n",
    "\n",
    "Download and prepare the CommonsenseQA dataset\n",
    "Split the data into train/validation/test sets if not already done\n",
    "Understand the format (questions, multiple-choice answers)\n",
    "\n",
    "## Step 2: Set Up Three Models\n",
    "\n",
    "Randomly Initialized Transformer\n",
    "\n",
    "Build a transformer architecture from scratch\n",
    "Initialize weights randomly\n",
    "This will serve as your baseline\n",
    "\n",
    "\n",
    "Pretrained Transformer\n",
    "\n",
    "Use the same transformer architecture as Model 1\n",
    "Initialize with pretrained weights (e.g., BERT, RoBERTa)\n",
    "Make sure this model wasn't specifically trained on CommonsenseQA\n",
    "\n",
    "\n",
    "Large Language Model (1B+ parameters)\n",
    "\n",
    "Choose an LLM (e.g., GPT-2, LLaMA, OPT, BLOOM)\n",
    "No finetuning for this model - just prompt engineering\n",
    "\n",
    "\n",
    "\n",
    "## Step 3: Training/Finetuning\n",
    "\n",
    "Finetune Models 1 & 2 on CommonsenseQA train set\n",
    "\n",
    "Use the same hyperparameters for both\n",
    "Train for multiple epochs\n",
    "Save checkpoints and track validation performance\n",
    "\n",
    "\n",
    "For Model 3 (LLM), develop effective prompts instead of finetuning\n",
    "\n",
    "## Step 4: Prompt Engineering (for LLM)\n",
    "\n",
    "Design different prompt formats\n",
    "Test various instruction styles\n",
    "Try few-shot examples in prompts\n",
    "Experiment with temperature and other generation parameters\n",
    "\n",
    "## Step 5: Evaluation\n",
    "\n",
    "Evaluate all three models on the test set\n",
    "Calculate accuracy, F1 score, or other relevant metrics\n",
    "Compare performance across models\n",
    "\n",
    "## Step 6: Analysis\n",
    "\n",
    "Analyze which types of questions each model handles well/poorly\n",
    "Look at error patterns\n",
    "Discuss why certain approaches work better\n",
    "\n",
    "## Step 7: Create Presentation\n",
    "\n",
    "Summarize methodology\n",
    "Present results with visualizations\n",
    "Include discussion of findings\n",
    "Provide limitations and potential improvements\n",
    "\n",
    "Technical Requirements:\n",
    "\n",
    "Programming language: Python recommended\n",
    "Libraries: PyTorch/TensorFlow, Transformers (Hugging Face), etc.\n",
    "Computational resources: You'll need GPU access for training\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "TODO: Checkpointing, Early stopping works?, Log to Wandb, sweeps or other auto tool (optional), llm\n",
    "\n",
    "**Delete steps generated by Claude later**\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5cc4f4",
   "metadata": {},
   "source": [
    "# **FS25 NLP Project 1: Word Embeddings/Recurrent Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52707bad",
   "metadata": {},
   "source": [
    "Fabian Dubach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee79ba7",
   "metadata": {},
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df05906a",
   "metadata": {},
   "source": [
    "<style>\n",
    "  .container {\n",
    "    display: flex;\n",
    "    align-items: flex-start;\n",
    "    gap: 20px; /* spacing between text and ASCII art */\n",
    "    font-family: monospace;\n",
    "  }\n",
    "  .text {\n",
    "    flex: 2;\n",
    "  }\n",
    "  .ascii {\n",
    "    white-space: pre;\n",
    "    font-size: 4.5px;\n",
    "    line-height: 1.2;\n",
    "    flex: 1;\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<div class=\"container\">\n",
    "  <div class=\"text\">\n",
    "    <p>The task for my project was to perform common sense question answering using the CommonsenseQA dataset.</p><br>\n",
    "    <p>I evaluated the performance of three different Transformer-based models:</p>\n",
    "    <p>1. A randomly initialized Transformer</p>\n",
    "    <p>2. A pretrained Transformer (with the same architecture as the first Transformer)</p>\n",
    "    <p>3. A large language model (LLM) with over 1 billion parameters</p><br>\n",
    "    <p>While the first two models were finetuned on the dataset using the same hyperparameters for a fair comparison, the LLM was evaluated through prompt engineering without additional training. This setup allowed me to explore how different levels of pretraining and model scale impact common sense reasoning performance.</p>\n",
    "    <p>We had to also track the trainings with Wandb (workspace URL: <a href=\"https://wandb.ai/fabian-dubach-hochschule-luzern/CommonsenseQA/workspace?nw=nwuserfabiandubach\" target=\"_blank\">https://wandb.ai/fabian-dubach-hochschule-luzern/CommonsenseQA/workspace?nw=nwuserfabiandubach</a>).</p>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"ascii\">\n",
    "<pre>\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣶⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⠀⠀⠀⠀⠀⣤⣤⣤⠀⠀⠀⠀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⠀⣠⡶⢿⡇⢿⣿⡏⢳⣦⠀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⡛⣆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣧⣼⣿⣴⣋⡽⠮⠿⢭⣟⣏⣷⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⣧⠘⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡼⣇⣿⡿⠶⣶⣿⣟⡛⣷⣿⢠⠙⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⡈⣏⠇⢹⡀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡟⢹⠁⣿⠋⠉⢹⠉⠙⣿⡇⣾⣀⣾⠀⢀⣤⡀⢀⡀⠀⠀⢀⣠⣴⣾⠛⢻⡛⢻⡄⢀⣳⡀⢀⣠⠄⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⣷⣾⢀⣿⡇⠀⠸⠀⠀⣿⣧⡽⠿⣟⣺⣭⠴⢿⡏⣩⣷⡾⢛⣭⣴⣿⣇⠘⣿⣷⣿⡛⠉⢻⣟⣷⠄⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠿⢿⣟⣿⣿⡦⣶⣪⡭⠿⣚⣫⣭⣽⣶⡄⠀⢸⡇⣿⡙⣿⣿⣿⣿⣿⣿⣆⠹⣿⣿⣷⡀⠀⢿⡉⠁⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⣤⣶⣿⠿⠛⣉⣭⣶⣾⣿⠿⠟⠛⠉⠉⢻⠀⢸⣷⣿⣇⢻⡿⣿⣿⣿⣿⠟⠀⠹⣿⣿⠃⠀⠘⣷⡀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣤⣦⣼⣿⠿⠛⣋⡁⣼⢠⣿⡿⠛⠉⠁⠀⠀⢀⡀⢀⣴⣾⠀⢸⣿⡇⢻⡄⠙⠿⠻⠛⠁⠀⢀⣠⣽⣿⣇⡀⠀⠸⣧⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⠿⣛⣭⣴⡾⠟⠛⣧⣿⢸⡿⠀⠀⠀⠀⣰⣿⣿⣷⣾⣿⣿⠀⢸⡏⣇⢸⣷⡀⠀⢀⣠⣴⣾⠿⠛⣿⢻⣿⣹⡀⠀⢻⣆⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣴⡟⣦⠀⠀⠀⢀⡿⣵⡿⠛⠉⣡⣶⣤⣄⣿⣯⢸⣇⠀⠀⢠⣾⣿⡿⣿⣿⣿⣿⡿⠀⢸⡇⢻⡼⣿⣷⣶⠿⠛⠉⠀⠀⠀⠸⡇⣿⣿⣧⠀⠘⣿⡀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⢹⠀⢀⣠⣼⣿⣿⠀⢀⣼⣿⣿⣿⣿⡇⣿⢸⣿⣀⣀⣿⡿⠿⠶⠚⠛⠉⠉⠀⠀⢸⡇⠀⢻⣾⣝⣿⡆⠀⢀⣠⡴⠖⠛⢻⡾⣿⣿⣆⠀⢹⡇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣇⣼⡾⠟⠋⣿⢻⣇⣤⣌⠻⢿⣿⣿⣿⠃⢿⠀⠉⠉⠁⠀⠀⠀⣀⣤⡤⠶⠶⠒⠚⣻⣷⣄⠈⣿⣿⣿⣿⡞⠉⠀⠀⠀⠀⠀⣿⢿⣿⣾⣋⣽⠇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣹⠏⠀⠀⠀⣿⢿⣿⣿⣯⡴⠾⠛⢋⣡⠶⠛⠛⠋⣉⣉⣉⣙⢻⣿⠀⠀⠀⠀⠀⢠⡟⠀⠈⠻⢦⣈⣿⣿⣧⠀⠀⢀⣠⣴⡾⢿⣿⣿⣿⣿⣿⡀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡟⣿⡟⠀⠀⠀⣿⠈⠋⠉⢀⣠⠴⣛⣩⣤⣶⣞⣭⣿⢿⣿⣿⣻⣼⣿⣆⣀⣤⣤⣴⣿⣄⣠⣶⣦⣀⣙⣿⣿⣿⡶⣿⠟⠋⣁⣶⠟⢻⣽⣿⣿⣿⠇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⢠⣿⣇⠀⠀⠀⢹⣠⡴⠖⢻⣷⢫⣿⣿⣿⣯⣿⣟⣿⣿⣭⣽⣿⡿⣿⣿⣿⠿⠿⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⣿⠋⠉⣿⠀⢸⣿⣿⣿⣿⣷⡀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣼⣿⣿⣤⣴⣾⢿⡅⠀⣀⣾⢿⣿⣿⣿⣿⣿⣿⡿⣿⣷⣿⣿⣿⡇⣿⣿⡇⠀⠀⢸⣿⣿⡟⢿⣿⣿⣿⣿⣿⣣⣿⠁⣿⣀⣤⡿⠀⢀⣿⣿⣿⣿⣿⡇⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⠻⣿⠛⠉⠀⠈⣿⠛⢽⣿⢻⣿⣿⢿⣿⣿⣿⡇⣿⠿⣶⣶⣚⣧⣿⣿⡇⠀⠀⣸⣿⣿⣿⣄⣈⢿⣿⢿⣷⣿⣿⠀⠉⠉⠀⠀⠀⠘⡇⣿⣿⣿⣿⡇⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⡀⣷⡆⠀⠀⠀⠸⣧⣻⣿⢸⣿⣿⡿⢿⣾⣻⡇⣿⣿⣿⣿⣿⣿⣿⠿⠷⠾⠛⠛⠿⢿⣿⣿⣿⣄⣿⠿⠋⢸⣿⠀⠀⠀⠀⠀⠀⠀⡇⣿⣿⣿⣿⣿⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣷⡇⣿⡇⠀⠀⠀⠀⣿⣿⣿⡾⢿⣿⣿⣿⣿⡶⠷⠾⠛⠛⠉⠁⢀⣠⠤⠴⠒⡆⢠⠀⢰⡉⠻⣿⣽⡏⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⠀⡇⣿⡿⣿⣿⣿⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣧⣿⠿⢀⣀⣤⣴⣿⣿⣿⡷⠾⠛⠋⠉⢀⣀⣠⠤⠴⠒⠻⡆⢸⠀⠀⢀⡠⠇⠸⡄⠈⣇⠀⠈⡻⢦⡀⠀⢸⡇⠀⠀⠀⠀⠀⠀⠀⡇⣿⣧⡘⠿⢻⡆\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠻⣆⣿⣿⣿⣿⣿⡿⠛⣉⣀⡀⣠⠴⠒⠋⠉⠁⠀⠀⠀⠀⠀⡇⢸⣠⠴⣫⡄⠀⠀⡇⠀⢹⠀⠀⣿⠦⢿⡀⢸⡇⠀⠀⣀⣤⣤⣿⠀⡇⣿⣿⣿⣆⢸⡇\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⢿⡟⣽⣿⠀⣏⠁⠀⡇⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣇⠀⡖⣻⠋⠀⠀⠈⢻⠀⢈⡇⠀⠸⡄⠘⣧⢸⡇⠀⢸⣷⣾⣿⠏⠀⡇⣿⣿⣿⣿⢸⡇\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣾⠏⠛⠋⢡⣿⠀⠸⣿⣟⡃⣇⠀⠀⠀⠀⠀⣀⣠⡤⠶⠒⠋⠀⠛⠁⠀⣀⣤⣶⣿⣿⣿⣿⣷⣤⡈⠁⢻⡞⣿⠀⠈⠻⣴⠏⠀⠀⠿⢹⣿⣎⢻⣿⡇\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣾⡟⠀⠀⢀⡿⣿⠀⠀⠈⠳⡇⠻⠤⠶⠚⠋⠉⠁⠀⠀⠀⠀⠀⣀⣤⣶⣿⣿⣿⣿⣿⠿⠛⠻⣿⣿⣿⣷⣜⣷⣿⠀⠀⢀⣀⣤⣤⣶⣾⣶⣿⣿⠃⢸⡇\n",
    "⠀⠀⠀⠀⠀⠀⣀⣤⡶⠶⠖⠚⢛⠛⠳⢶⣼⡟⠀⠀⢀⣼⣹⣿⢀⠀⠀⠀⠀⡀⠀⠀⠀⠀⠀⢀⣀⣠⡤⢤⣾⣿⣿⣿⡿⠿⠛⠉⠹⡇⠀⠀⣿⣿⣟⢿⣿⣿⠹⣶⣿⡿⠛⠻⣏⠀⠉⠉⡛⣿⡿⣾⡇\n",
    "⠀⠀⠀⢀⣴⠞⠋⢰⡇⢰⣿⢻⢻⢻⢶⣦⠙⣷⡀⠀⣸⢧⠟⢿⣿⣿⣿⣷⣶⣶⣤⣴⣲⡾⠿⠟⠒⠒⠛⡇⠙⣿⠉⠀⢧⠀⠀⠀⠀⣧⠀⠀⢸⣿⣿⡎⣿⠁⢀⣼⣏⢀⣠⣤⣸⣶⠀⠀⣿⣿⣿⠛⠁\n",
    "⠀⠀⠀⣾⠃⠀⣠⡬⣤⣼⣛⠾⣼⣞⡾⡟⠀⠘⣧⣠⣏⡞⠀⠈⠻⣿⡏⢹⡟⠛⠻⣿⠁⠀⠀⠀⠀⠀⠀⣇⠀⣿⠀⠀⢸⡄⠀⠀⠀⢸⠀⠀⠘⣿⣿⣇⣿⣴⡞⢣⣽⣿⣿⣿⣿⣿⠀⠀⣿⣿⡟⠀⠀\n",
    "⠀⠀⠀⣿⡶⣿⣿⣸⣿⣿⣿⠿⠷⠾⢽⣅⡲⠶⢻⣿⣼⢁⣠⣤⣶⣿⣿⠘⡇⠀⠀⢻⡆⠀⠀⠀⠀⠀⢀⣸⡀⢹⡇⠀⠈⡇⠀⠀⠀⠈⡇⠀⠀⢿⣿⣿⢹⣿⣤⣿⣿⣿⣿⡿⢿⣟⡀⠀⣿⣿⡇⠀⠀\n",
    "⠀⠀⠀⠈⠛⠿⢯⣜⣿⠏⠀⠀⠀⢀⡿⣨⣿⣶⣤⣿⣷⣯⣿⣿⣿⣿⣿⠀⡇⠀⠀⠐⡿⣦⣰⣒⣶⣿⣿⣿⣷⣾⣇⠀⠀⢻⠀⠀⠀⠀⢷⠀⠀⢸⣿⣿⣾⣿⣸⣿⡏⢠⠟⣠⣿⣿⣿⣦⡈⢹⡇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⢸⡟⣾⠄⠀⠀⣸⡇⣿⣿⣿⠟⠋⠛⢿⣿⣿⣿⣿⣿⡄⢻⠀⠀⠀⡇⠈⠙⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⢸⡆⠀⠀⠀⢸⡄⠀⠀⣿⣿⣇⣿⠛⠛⠻⣿⣺⣿⣿⣿⣿⣿⣿⡿⠃⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⣼⢧⡇⠀⠀⠀⣿⢸⣿⣿⡿⢦⣴⣿⣿⣷⡿⣿⡿⣿⡇⢸⡄⠀⠀⢹⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⡆⠀⠀⣇⠀⠀⠀⠀⣇⠀⠀⢸⣿⣟⢿⡀⠀⠀⠈⠉⠀⠉⠉⠉⠁⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⣿⣨⡧⠤⠤⢤⣇⡾⣿⣿⣠⣿⣿⣿⣿⣿⣿⣽⣿⣿⣷⠀⣇⠀⠀⢸⠀⠀⢸⢻⣿⣿⣿⣿⡇⣿⣿⠀⠀⢹⡄⠀⠀⢀⣸⠀⠀⠸⣿⣿⣼⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⢀⡿⣧⣤⠶⠦⣼⣿⣿⣿⡏⠈⣿⣿⢿⣿⣿⣿⣏⠉⢹⣿⡀⢻⠀⠀⠘⡇⠀⠸⡄⠙⢿⣿⣿⠇⣿⣿⡄⠀⠈⠓⠒⠋⠉⠀⠀⠀⠀⢿⠹⣯⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⣸⣿⢃⡏⠀⠀⢻⣿⣿⣽⣿⣦⠘⣿⣿⣿⣿⣿⢻⣿⣾⣿⡇⠘⡇⠀⠀⣇⠀⠀⣇⠀⠀⠙⢿⡇⣿⢸⣧⠀⠀⠀⠀⡴⠒⢶⠀⠀⠀⠘⣆⠀⢻⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⡿⡅⣸⢁⣄⡄⣾⣿⢿⣿⠿⣿⣿⢻⣿⣿⣟⣿⣸⣻⡿⣿⣧⠀⠙⠒⠛⠛⠀⠀⢿⣿⣄⠀⠀⠀⣿⠈⣿⡄⠀⠀⠀⡇⠀⠘⡇⠀⠀⠀⢿⣦⢸⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⢸⣧⡇⣿⣼⣿⠃⣿⣿⣾⣿⣷⣤⡿⠿⢿⣿⣿⣇⣿⡟⠋⠀⣿⡀⠀⣴⠲⡆⠀⠀⠸⣿⣿⣦⠀⠀⢸⡀⢹⣧⠀⠀⠀⣇⠀⠀⢹⠀⠀⠀⠸⣿⡟⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⢽⡿⣷⠏⠛⠿⢠⣿⣿⣿⣿⢿⣯⡇⠀⠀⠈⠁⠀⠀⠀⠀⠀⢸⣇⠀⢻⠀⢳⠀⠀⠀⣿⣿⣿⣷⣾⢸⡇⠈⣿⡀⠀⠀⢸⠀⠀⠈⡇⠀⠀⢀⣿⣿⣷⣀⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠘⣧⡙⣀⣀⣀⣸⣿⣽⣿⣿⠀⠈⠙⣶⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡀⢸⡀⠸⡄⠀⠀⢻⣿⣿⣿⣿⡼⡇⠀⢘⣧⣤⡴⠾⠷⠶⠖⠛⠛⢛⠋⠉⢿⢹⠉⣭⡿⠿⠷⠶⢦⡄⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠹⣟⣁⣸⣿⣿⣧⡿⠿⣿⣀⡀⠀⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣇⣈⣧⣘⣷⣤⣤⣼⠿⠿⣿⣿⣧⣧⡀⣸⢹⡏⠀⠀⠀⠀⠀⠀⠀⠈⡇⠀⢸⢸⡄⡿⠖⠚⠉⡉⠓⢿⡀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⣠⡴⣾⠋⠉⢙⣻⣷⠛⠛⠳⠶⠶⠽⠿⠃⠀⠀⠀⠀⠀⣀⡤⣼⡿⠋⠉⠁⠀⠀⣠⠀⣿⣿⠀⠀⠀⠀⠈⠉⠻⣿⢸⣷⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠸⡏⡇⣿⠀⠀⠀⢻⣷⢸⡇⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⡟⠀⡟⠀⠀⢸⣿⣿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⡥⢺⠏⡆⠀⠀⠀⠀⠀⡏⠀⡟⡇⠀⠀⠀⠀⠀⠀⢀⡇⢸⣿⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⡇⡇⢿⠀⠀⠀⢸⣿⡌⣷⠀⠀⠀⠀\n",
    "⠀⠀⠀⢸⠇⢠⡇⠀⠀⢰⣿⣯⣏⣻⡆⠀⠀⠀⠀⠀⠀⠀⠀⣸⠃⢀⡿⢸⡇⠀⠀⠀⠀⢠⡇⠀⡇⡇⠀⠀⠀⠀⠀⠀⢸⡇⢸⣿⡆⠀⠀⠀⠀⠀⠀⠀⣧⠀⠀⡇⢿⢸⠀⠀⠀⠈⣿⡇⢹⡀⠀⠀⠀\n",
    "⠀⠀⠀⡟⡄⣼⠀⠀⢀⣿⣿⣿⣿⣿⣷⠀⠀⠀⠀⠀⠀⠀⠀⣿⠀⢸⡇⣸⡇⠀⠀⠀⠀⢸⠁⢸⣷⡇⠀⠀⠀⠀⠀⠀⢸⡇⢸⣿⡇⠀⠀⠀⠀⠀⠀⠀⢻⠀⠀⢹⢸⣼⡀⠀⣀⣀⣿⣧⣸⡇⠀⠀⠀\n",
    "⠀⠀⢰⢧⣇⡏⠀⠀⣸⣿⠿⢭⣿⣿⡏⠀⠀⠀⠀⠀⠀⠀⢰⡏⠀⣿⠀⣿⡇⠀⠀⠀⠀⢸⠀⢸⢸⠁⠀⠀⠀⠀⠀⠀⢸⡇⢸⣿⣿⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⢸⢸⣿⡏⢉⣁⣤⣤⣄⢈⡇⠀⠀⠀\n",
    "⠀⠀⣼⢼⣿⠃⠀⠀⣿⣿⠀⢸⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⢸⡇⢠⡿⢰⣿⠃⠀⠀⠀⠀⣼⠀⢸⢸⠀⠀⠀⠀⠀⠀⠀⢸⡇⢸⢹⣸⣦⣤⣤⣤⣶⣶⣶⡿⠀⠀⢸⡄⡇⣧⣽⣿⣿⣿⡽⠟⠁⠀⠀⠀\n",
    "⠀⠀⢿⢻⡏⠀⠀⢰⣿⣿⣟⠛⢿⣿⡇⠀⠀⠀⠀⠀⠀⠀⢸⠗⣻⡇⢸⢹⣆⣀⣀⣀⣤⡏⠀⢸⢸⠀⠀⠀⠀⠀⠀⠀⢸⡇⢸⠈⠉⠉⠉⠉⠉⠉⠀⠀⠀⠀⠀⠈⡇⣿⠘⣿⣿⣿⣇⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⢸⠛⠤⢤⣤⣘⢺⣿⣿⣿⣿⡿⠃⠀⠀⠀⠀⠀⠀⠀⠸⢧⣿⠃⠘⠓⠛⠛⠛⠋⠉⠁⠀⢼⢸⠀⢰⡾⠿⠛⠛⠿⢿⡇⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⢸⠀⠙⣿⣿⣿⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⢘⣶⡶⠚⠿⢿⣿⣩⢿⢿⡏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣸⠀⢸⡇⠀⠀⠀⠀⣿⡇⡏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢷⢸⡀⠀⠈⠁⢸⡇⠀⠀⠀⠀⠀\n",
    "⠀⠀⣼⣹⠃⠀⢰⣷⢻⠁⠈⠛⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡟⣹⠀⢸⠃⠀⠀⠀⠀⣿⠇⠜⠀⣤⠶⠖⠛⠛⠋⠉⠉⢩⣿⡇⠀⢸⠸⡇⠀⠀⠀⠘⡇⠀⠀⠀⠀⠀\n",
    "⠀⢠⡟⠏⠀⠀⣾⣿⣼⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⡇⠀⢀⣴⠶⠞⠛⠛⣻⣷⠀⡏⣿⠀⢸⢀⣴⣷⣦⡀⣿⠇⡇⠀⡟⠀⣀⣀⣀⣀⣀⣀⣸⣿⡇⠀⢸⡆⡇⠀⠀⠀⠀⣷⠀⠀⠀⠀⠀\n",
    "⠀⣸⠇⠀⠀⢸⣿⡇⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⣾⣀⣀⣤⣤⣶⣿⡿⠀⡇⣿⠀⢸⣿⣿⣿⣫⣾⣿⠀⡇⢠⣟⣿⣿⣿⡿⠿⠿⠿⠿⠁⡇⠀⠈⡇⣷⢀⡀⠀⠀⢻⠀⠀⠀⠀⠀\n",
    "⠀⣿⡼⠀⠀⡟⣿⣷⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⣿⠀⢀⡟⡿⠿⠟⠛⠛⣃⡇⠀⡇⣿⠀⢸⣿⣿⣿⣿⣿⣿⡄⡇⢸⡇⠀⠀⠀⠀⠀⠀⠀⢰⣶⡇⠀⠀⣇⢹⣾⣿⠀⣰⢾⡆⠀⠀⠀⠀\n",
    "⢠⣿⡇⠀⢸⣷⣿⣹⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⠀⢸⡇⠀⠀⠀⠀⢰⣿⡇⠀⡇⣿⠀⣾⣿⣿⣿⣿⣿⣿⡃⡇⢸⣧⣤⣤⣴⣶⣶⣶⣶⣾⣿⡇⠀⠀⢿⢸⣿⣿⣾⣿⣸⡇⠀⠀⠀⠀\n",
    "⢸⢭⠥⠦⣬⣽⣧⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣸⣿⠀⢸⢵⣶⣾⣿⣿⣿⡿⡇⠀⡇⣿⠀⣿⣿⣿⣿⣿⣿⣿⡇⡇⢸⡏⠿⠟⠛⠛⠛⠛⠛⠛⣧⣷⠀⠀⢸⠀⣿⣿⣿⣿⠛⣇⠀⠀⠀⠀\n",
    "⢸⣸⠁⢠⣿⣿⣹⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⢸⠉⠉⠉⠁⠀⢠⣾⡇⠀⡇⣿⠀⣿⣿⣿⣿⣿⣿⣿⠇⡇⢸⡇⠀⣀⣀⣀⣀⣀⣀⣰⣿⣿⠀⠀⠸⠀⣿⣿⣿⣵⡇⣿⠀⠀⠀⠀\n",
    "⠘⣧⣰⠞⣞⣷⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⢸⣀⣀⣠⣤⣤⣼⣿⡇⠀⡇⣿⠀⢈⣭⣭⠭⠽⠭⣿⡇⡇⢸⣟⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡀⠀⠀⠀⢻⣟⣾⣿⣿⢻⠀⠀⠀⠀\n",
    "⠀⠈⠛⠛⠛⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⣿⠿⠿⠿⠿⠟⢛⣻⡇⠀⡇⢻⠀⢸⠁⠀⠀⠀⠀⣿⡇⡇⠸⡏⠉⠀⠀⠀⠀⠀⠀⠀⣼⣿⡇⠀⠀⠀⢸⣿⣿⣿⣿⢸⡆⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣇⠀⣿⣀⣤⣤⣤⣤⣼⣿⡇⠀⡇⢸⠀⢸⠀⣠⣶⣄⠀⣿⡇⣇⠀⡇⣴⣶⣶⣾⣿⣿⣿⣿⣿⣿⣇⣀⣂⠀⢸⣿⣿⣿⣿⣿⡇⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⡿⠴⠿⠿⠿⠿⠿⠿⠿⠿⠷⣦⡄⢸⠀⢸⣾⣿⣿⢟⣴⣿⣷⣼⠶⠗⠛⠛⠛⠛⠛⠛⠛⠋⠉⠉⠉⢉⡟⣧⠈⣿⣿⣿⣿⡿⣧⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣴⣿⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⣴⣿⡇⢸⣴⢾⣿⡿⣻⣿⣿⣿⣿⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⣿⣿⡇⢸⣿⢸⣿⣿⣿⣿⣿⣿⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⣿⣿⡀⣿⣿⣿⣿⣿⣿⡄⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣿⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣾⣿⣿⣿⡇⢸⣿⢸⣿⣿⣿⣿⣿⠃⠀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣿⣿⣿⡇⢸⣿⣿⣿⣿⢻⡇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣿⣷⣶⣶⣶⣶⣶⣶⣶⡶⠶⠦⠤⣾⣿⣿⣿⣿⣷⢘⣿⢸⣿⣿⣿⣿⡏⣭⠭⠭⠭⠤⠤⠤⠴⠶⠶⠶⠶⠶⠶⠶⠱⣌⢻⣿⣧⢸⣿⣿⣿⣿⣾⣇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⡾⠟⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⣉⣽⣿⣾⣿⣿⣿⣿⣿⠀⣿⢸⣿⣿⣿⡟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⡞⣿⢻⠈⣿⣿⣿⣿⣿⣿⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣶⠟⠋⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠⣴⣾⣿⣿⣿⣿⣿⣿⣿⠛⢹⠀⣿⣾⣿⣿⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⣿⣿⣿⢻⣿⡀⣿⣿⣿⣿⣿⣿⡄⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣾⣿⣀⣤⣄⣤⣤⣄⣀⣀⣀⣀⣶⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣅⢸⠀⣿⡿⣿⣿⣤⣤⣤⡤⠤⠤⠶⠶⠶⠖⠒⠒⠒⠚⠛⠛⠛⠺⣿⣿⣿⡇⠹⡇⣿⣿⣿⣿⣿⣿⡇⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⣸⣿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⣿⣿⣿⣿⡟⠉⢹⣿⣿⣿⣿⡿⠿⡾⠀⣿⡇⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⣿⠰⠇⣿⣿⣿⣿⡿⣿⡇⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⢿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡟⠛⠉⠁⠀⠀⠀⠙⠛⠉⠁⠀⠀⠁⠀⣛⣁⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⠟⣹⡇⢀⣙⣿⣯⡷⠿⠛⠁⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠉⠉⠉⠉⠉⠉⠹⠷⣦⣤⣤⣤⣤⣤⣤⣤⣤⣤⣶⣶⣶⡶⠶⠶⠶⠶⠾⠿⠛⠛⠋⠉⠉⠁⠀⠀\n",
    "</pre>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48b5c1",
   "metadata": {},
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d849338",
   "metadata": {},
   "source": [
    "Import all libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5a12ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from transformers import BertConfig, BertForMultipleChoice, BertTokenizer, AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup, GenerationConfig\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import gensim\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import wandb\n",
    "\n",
    "\n",
    "import re\n",
    "import string\n",
    "import html\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import words as nltk_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e4e4e4",
   "metadata": {},
   "source": [
    "Download necessary NLTK resources. I just downloaded everything that I might use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa32b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fabia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fabia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fabia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\fabia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\fabia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d266c9",
   "metadata": {},
   "source": [
    "Setup random seed to ensure reproducibility.\n",
    "\n",
    "_Info about the seed value: The field of natural language processing began in the 1940s, after World War II. At this time, people recognized the importance of translation from one language to another and hoped to create a machine that could do this sort of translation automatically._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a7ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1940 # normal: 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a438be37",
   "metadata": {},
   "source": [
    "In the next step I import and split the dataset. For the split I took off the last 1000 entries from the train-split and used it as validation, the rest of this is of course used for the training. Then I used the validation-part as the test. This was done since the real test-split has no answer keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22c7f692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8741 1000 1221\n"
     ]
    }
   ],
   "source": [
    "train = load_dataset(\"tau/commonsense_qa\", split=\"train[:-1000]\")\n",
    "valid = load_dataset(\"tau/commonsense_qa\", split=\"train[-1000:]\")\n",
    "test = load_dataset(\"tau/commonsense_qa\", split=\"validation\")\n",
    "\n",
    "print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff2820",
   "metadata": {},
   "source": [
    "Login for the experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492738f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f8b6b6",
   "metadata": {},
   "source": [
    "# **Data Exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[4m\" + \"Dataset Features\" + \"\\033[0m\")\n",
    "for feature in train.features:\n",
    "    print(feature)\n",
    "print(\"\\n\" + \"\\033[4m\" + \"Example\" + \"\\033[0m\")\n",
    "for feature in train.features:\n",
    "    print(feature + \":\", train[0][str(feature)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec5d2d0",
   "metadata": {},
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3c46e3",
   "metadata": {},
   "source": [
    "For the preprocessing I looked at the following points:\n",
    "\n",
    "1. Tokenization\n",
    "2. Lowercasing, stemming, lemmatizing, stopword/punctuation removal \n",
    "3. Removal of unknown/other words \n",
    "4. Format cleaning (e.g. html-extracted text) \n",
    "5. Truncation \n",
    "6. Feature selection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c0a00e",
   "metadata": {},
   "source": [
    "**Reasoning why used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18589408",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c7ec690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_deepseek = AutoTokenizer.from_pretrained('deepseek-ai/DeepSeek-V2-Lite', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd469731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_commonsenseqa(examples):\n",
    "    \n",
    "    # Extract questions and choices\n",
    "    questions = [q for q in examples['question']]\n",
    "    \n",
    "    # Initialize arrays for input_ids, attention_masks, and token_type_ids\n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_token_type_ids = []\n",
    "    \n",
    "    # Get correct answer indices\n",
    "    answerkeys = examples['answerKey']\n",
    "    labels = []\n",
    "    \n",
    "    # Convert letter answers to indices (A->0, B->1, etc.)\n",
    "    for key in answerkeys:\n",
    "        labels.append(ord(key) - ord('A'))\n",
    "    \n",
    "    # Process each question with its choices\n",
    "    for i, (question, choices) in enumerate(zip(questions, examples['choices'])):\n",
    "        inputs = []\n",
    "        \n",
    "        # Process each choice for the current question\n",
    "        for choice in choices['text']:\n",
    "            # Combine question and choice\n",
    "            text_a = question\n",
    "            text_b = choice\n",
    "            \n",
    "            # Tokenize\n",
    "            encoded = tokenizer_bert(\n",
    "                text_a, text_b,\n",
    "                add_special_tokens=True,\n",
    "                max_length=128,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            inputs.append({\n",
    "                'input_ids': encoded['input_ids'],\n",
    "                'attention_mask': encoded['attention_mask'],\n",
    "                'token_type_ids': encoded['token_type_ids']\n",
    "            })\n",
    "        \n",
    "        # Stack tensors for all choices of this question\n",
    "        input_ids = torch.cat([x['input_ids'] for x in inputs])\n",
    "        attention_mask = torch.cat([x['attention_mask'] for x in inputs])\n",
    "        token_type_ids = torch.cat([x['token_type_ids'] for x in inputs])\n",
    "        \n",
    "        all_input_ids.append(input_ids)\n",
    "        all_attention_mask.append(attention_mask)\n",
    "        all_token_type_ids.append(token_type_ids)\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    return {\n",
    "        'input_ids': all_input_ids,\n",
    "        'attention_mask': all_attention_mask,\n",
    "        'token_type_ids': all_token_type_ids,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49eb21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "train_dataset = preprocess_commonsenseqa(train)\n",
    "validation_dataset = preprocess_commonsenseqa(valid)\n",
    "test_dataset = preprocess_commonsenseqa(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9181ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch datasets\n",
    "train_features = TensorDataset(\n",
    "    torch.stack(train_dataset['input_ids']),\n",
    "    torch.stack(train_dataset['attention_mask']),\n",
    "    torch.stack(train_dataset['token_type_ids']),\n",
    "    torch.tensor(train_dataset['labels'])\n",
    ")\n",
    "\n",
    "val_features = TensorDataset(\n",
    "    torch.stack(validation_dataset['input_ids']),\n",
    "    torch.stack(validation_dataset['attention_mask']),\n",
    "    torch.stack(validation_dataset['token_type_ids']),\n",
    "    torch.tensor(validation_dataset['labels'])\n",
    ")\n",
    "\n",
    "test_features = TensorDataset(\n",
    "    torch.stack(test_dataset['input_ids']),\n",
    "    torch.stack(test_dataset['attention_mask']),\n",
    "    torch.stack(test_dataset['token_type_ids']),\n",
    "    torch.tensor(test_dataset['labels'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcee6ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_commonsense_qa_for_deepseek(dataset, model, tokenizer, num_examples=5, max_new_tokens=100, include_answer=False):\n",
    "    results = {\n",
    "        'questions': [],\n",
    "        'prompts': [],\n",
    "        'responses': [],\n",
    "        'correct_answers': []\n",
    "    }\n",
    "    \n",
    "    # Limit the dataset to the first num_examples\n",
    "    if num_examples is not None:\n",
    "        # Slice the dataset\n",
    "        limited_dataset = dataset.select(range(min(num_examples, len(dataset))))\n",
    "    else:\n",
    "        limited_dataset = dataset\n",
    "    \n",
    "    # Check if question_concept is in the dataset\n",
    "    has_question_concept = 'question_concept' in limited_dataset.column_names\n",
    "    \n",
    "    # Process each example\n",
    "    for i in range(len(limited_dataset)):\n",
    "        question = limited_dataset['question'][i]\n",
    "        \n",
    "        # Get question_concept if available\n",
    "        question_concept = limited_dataset['question_concept'][i] if has_question_concept else ''\n",
    "        \n",
    "        # Format the choices - correctly accessing the nested structure\n",
    "        choices = limited_dataset['choices'][i]  # This is a dictionary with 'label' and 'text' keys\n",
    "        choice_labels = choices['label']  # Already a list\n",
    "        choice_texts = choices['text']   # Already a list\n",
    "        choices_text = \", \".join([f\"{label}. {text}\" for label, text in zip(choice_labels, choice_texts)])\n",
    "        \n",
    "        # Get answer key if available\n",
    "        answer_key = limited_dataset['answerKey'][i] if 'answerKey' in limited_dataset.column_names else None\n",
    "        \n",
    "        # Build the prompt\n",
    "        prompt = f\"Question: {question}\\n\"\n",
    "        if question_concept:\n",
    "            prompt += f\"Question concept: {question_concept}\\n\"\n",
    "        prompt += f\"Choices: {choices_text}\\n\"\n",
    "        \n",
    "        if include_answer and answer_key:\n",
    "            prompt += f\"The correct answer is: {answer_key}\"\n",
    "        else:\n",
    "            prompt += \"The correct answer is:\"\n",
    "        \n",
    "        # Tokenize and generate\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Move inputs to the model's device\n",
    "        \n",
    "        try:\n",
    "            outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "            \n",
    "            # Decode the result\n",
    "            result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response for example {i}: {e}\")\n",
    "            result = f\"Error: {str(e)}\"\n",
    "        \n",
    "        # Store the results\n",
    "        results['questions'].append(question)\n",
    "        results['prompts'].append(prompt)\n",
    "        results['responses'].append(result)\n",
    "        results['correct_answers'].append(answer_key if answer_key else \"N/A\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399fe1e",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eac71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_bert = BertConfig.from_pretrained('bert-base-cased')\n",
    "random_bert_model = BertForMultipleChoice(config_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7d9b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_bert_model = BertForMultipleChoice.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f91a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pretrained_bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "177d6325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f576550181d4374b598994beb1bf99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_deepseek = AutoModelForCausalLM.from_pretrained('deepseek-ai/DeepSeek-V2-Lite', trust_remote_code=True, torch_dtype=torch.bfloat16, attn_implementation=\"eager\")\n",
    "model_deepseek.generation_config = GenerationConfig.from_pretrained('deepseek-ai/DeepSeek-V2-Lite')\n",
    "model_deepseek.generation_config.pad_token_id = model_deepseek.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41073d29",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596c651e",
   "metadata": {},
   "source": [
    "Example for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4add2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff26a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_features, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_features, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_features, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c0e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(model, train_dataloader, val_dataloader, device, \n",
    "                      epochs=10, learning_rate=1e-4, warmup_steps=None,\n",
    "                      log_wandb=True, gradient_clip_val=5.0, save_interval=1,\n",
    "                      gradient_accumulation_steps=4, patience=3,\n",
    "                      weight_decay=0.001, save_path=None):\n",
    "        \n",
    "    # Handle save path for checkpoints\n",
    "    if save_path:\n",
    "        # Make sure parent directory exists\n",
    "        checkpoint_dir = save_path\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        # Define best model path inside the checkpoint directory\n",
    "        best_model_path = os.path.join(checkpoint_dir, \"best_transformer_model.pt\")\n",
    "    else:\n",
    "        print(\"Warning: No save_path provided. Model and checkpoints will not be saved.\")\n",
    "        checkpoint_dir = None\n",
    "        best_model_path = None\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Enable gradient checkpointing for memory efficiency\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    # Initialize wandb logging if enabled\n",
    "    if log_wandb:\n",
    "        import wandb\n",
    "        # Check if wandb is initialized, if not initialize it\n",
    "        if not wandb.run:\n",
    "            wandb.init(\n",
    "                project=\"CommonsenseQA\",\n",
    "                # Change the name to the current transformer model\n",
    "                name=f\"pretrained_transformer-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\",\n",
    "                config={\n",
    "                    \"learning_rate\": learning_rate,\n",
    "                    \"epochs\": epochs,\n",
    "                    \"batch_size\": train_dataloader.batch_size,\n",
    "                    \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "                    \"effective_batch_size\": train_dataloader.batch_size * gradient_accumulation_steps,\n",
    "                    \"weight_decay\": weight_decay,\n",
    "                    \"warmup_steps\": warmup_steps,\n",
    "                    \"gradient_clip_val\": gradient_clip_val,\n",
    "                    \"model_type\": model.__class__.__name__,\n",
    "                      })\n",
    "    \n",
    "    # Initialize CrossEntropy Loss\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Calculate total training steps\n",
    "    total_steps = len(train_dataloader) * epochs // gradient_accumulation_steps\n",
    "    \n",
    "    # Set default warmup steps if not provided\n",
    "    if warmup_steps is None:\n",
    "        warmup_steps = len(train_dataloader)  # One epoch of warmup\n",
    "    \n",
    "    # Initialize scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=warmup_steps, \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Variables to track best model\n",
    "    best_accuracy = 0.0\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_correct = 0\n",
    "        epoch_total = 0\n",
    "        progress_bar = tqdm(train_dataloader, desc=\"Training\")\n",
    "        optimizer.zero_grad()  # Zero gradients once at the beginning of epoch\n",
    "        \n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            # Move batch to device\n",
    "            input_ids, attention_mask, token_type_ids, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            \n",
    "            # Use CrossEntropy Loss\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            epoch_correct += (preds == labels).sum().item()\n",
    "            epoch_total += labels.size(0)\n",
    "            \n",
    "            # Scale loss for gradient accumulation\n",
    "            loss_to_backward = loss / gradient_accumulation_steps\n",
    "            \n",
    "            # Backward pass\n",
    "            loss_to_backward.backward()\n",
    "            \n",
    "            # Update weights every gradient_accumulation_steps batches\n",
    "            if (i + 1) % gradient_accumulation_steps == 0 or (i + 1) == len(train_dataloader):\n",
    "                # Clip gradients using the provided gradient_clip_val parameter\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_val)\n",
    "                \n",
    "                # Update weights\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Log learning rate\n",
    "                if log_wandb:\n",
    "                    wandb.log({\"learning_rate\": scheduler.get_last_lr()[0]})\n",
    "            \n",
    "            # Update progress bar (use the unscaled loss for display)\n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "            \n",
    "            # Accumulate loss (use the unscaled loss for logging)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Calculate average loss and accuracy for the epoch\n",
    "        avg_train_loss = epoch_loss / len(train_dataloader)\n",
    "        train_accuracy = epoch_correct / epoch_total\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Training loss: {avg_train_loss:.4f}, accuracy: {train_accuracy:.4f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        # No gradient computation for validation\n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(val_dataloader, desc=\"Validation\")\n",
    "            \n",
    "            for batch in progress_bar:\n",
    "                # Move batch to device\n",
    "                input_ids, attention_mask, token_type_ids, labels = [b.to(device) for b in batch]\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids\n",
    "                )\n",
    "                logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "                \n",
    "                # Use CrossEntropy Loss\n",
    "                criterion = torch.nn.CrossEntropyLoss()\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Get predictions\n",
    "                _, preds = torch.max(logits, dim=1)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\"acc\": correct/total})\n",
    "            \n",
    "        # Calculate validation metrics\n",
    "        val_accuracy = correct / total\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        print(f\"Validation loss: {avg_val_loss:.4f}, accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Log metrics to wandb\n",
    "        if log_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"train_accuracy\": train_accuracy,\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"val_accuracy\": val_accuracy,\n",
    "                \"learning_rate\": scheduler.get_last_lr()[0]\n",
    "            })\n",
    "        \n",
    "        # Save checkpoint at specified interval\n",
    "        if checkpoint_dir and (epoch + 1) % save_interval == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            \n",
    "            # Create checkpoint with additional information\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model_to_save.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'train_losses': train_losses,\n",
    "                'val_accuracies': val_accuracies\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if best_model_path and val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            # Save model\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            torch.save(model_to_save.state_dict(), best_model_path)\n",
    "            print(f\"Best model saved to {best_model_path}\")\n",
    "            \n",
    "            # Also save as checkpoint with additional metadata\n",
    "            best_checkpoint_path = os.path.join(checkpoint_dir, f\"best_checkpoint_epoch_{epoch+1}.pt\")\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model_to_save.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'train_losses': train_losses,\n",
    "                'val_accuracies': val_accuracies\n",
    "            }\n",
    "            torch.save(checkpoint, best_checkpoint_path)\n",
    "            \n",
    "            # Log best model to wandb\n",
    "            if log_wandb:\n",
    "                wandb.run.summary[\"best_accuracy\"] = best_accuracy\n",
    "                wandb.run.summary[\"best_epoch\"] = epoch + 1\n",
    "            \n",
    "            # Reset early stopping counter\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            # Increment early stopping counter\n",
    "            early_stopping_counter += 1\n",
    "            print(f\"No improvement for {early_stopping_counter} epochs\")\n",
    "            \n",
    "            # Check if we should stop early\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(f\"Early stopping after {epoch+1} epochs\")\n",
    "                if log_wandb:\n",
    "                    wandb.run.summary[\"stopped_epoch\"] = epoch + 1\n",
    "                break\n",
    "    \n",
    "    # Load best model if it was saved\n",
    "    if best_model_path and os.path.exists(best_model_path):\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        print(f\"Loaded best model from {best_model_path}\")\n",
    "    \n",
    "    # Finish wandb run\n",
    "    if log_wandb:\n",
    "        wandb.finish()\n",
    "    \n",
    "    return model, train_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a50a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_model, random_losses, random_accuracies = train_transformer(\n",
    "    random_bert_model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    device,\n",
    "    epochs=20,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=0.1 * len(train_dataloader),\n",
    "    gradient_clip_val=5.0,\n",
    "    save_interval=1,\n",
    "    gradient_accumulation_steps=4,  # Effectively creates a batch size of 128 (batch_size=32 * 4)\n",
    "    patience=3,\n",
    "    weight_decay=0.01,\n",
    "    save_path=f\"./checkpoints/random_transformer-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103eedf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model, pretrained_losses, pretrained_accuracies = train_transformer(\n",
    "    pretrained_bert_model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    device,\n",
    "    epochs=20,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=0.1 * len(train_dataloader),\n",
    "    gradient_clip_val=5.0,\n",
    "    save_interval=1,\n",
    "    gradient_accumulation_steps=4,  # Effectively creates a batch size of 128 (batch_size=32 * 4)\n",
    "    patience=5,\n",
    "    weight_decay=0.01,\n",
    "    save_path=f\"./checkpoints/pretrained_transformer-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716b5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def objective_function_pretrained(config=None):\n",
    "    \"\"\"Objective function for hyperparameter optimization of pretrained model\"\"\"\n",
    "    # Initialize a new wandb run for this trial\n",
    "    with wandb.init(config=config):\n",
    "        # Get the configuration for this run\n",
    "        config = wandb.config\n",
    "        \n",
    "        # Create dataloaders with the batch size from config\n",
    "        train_batch_size = config.batch_size\n",
    "        \n",
    "        train_dataloader = DataLoader(\n",
    "            train_features,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True  # Faster transfers to GPU\n",
    "        )\n",
    "        \n",
    "        val_dataloader = DataLoader(\n",
    "            val_features,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Use the training function with model_pretrained\n",
    "        model, train_losses, val_accuracies = train_transformer(\n",
    "            model=pretrained_bert_model,  # Using the pretrained model\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            device=device,\n",
    "            epochs=config.epochs,\n",
    "            learning_rate=config.learning_rate,\n",
    "            warmup_steps=config.warmup_ratio * len(train_dataloader),\n",
    "            gradient_clip_val=config.gradient_clip_val,\n",
    "            save_interval=1,\n",
    "            gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "            patience=config.patience,\n",
    "            weight_decay=config.weight_decay,\n",
    "            save_path=f\"./checkpoints/pretrained-sweep-{wandb.run.id}-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        )\n",
    "        \n",
    "        # Return the best validation accuracy\n",
    "        return max(val_accuracies)\n",
    "\n",
    "def objective_function_random(config=None):\n",
    "    \"\"\"Objective function for hyperparameter optimization of random initialized model\"\"\"\n",
    "    # Initialize a new wandb run for this trial\n",
    "    with wandb.init(config=config):\n",
    "        # Get the configuration for this run\n",
    "        config = wandb.config\n",
    "        \n",
    "        # Create dataloaders with the batch size from config\n",
    "        train_batch_size = config.batch_size\n",
    "        \n",
    "        train_dataloader = DataLoader(\n",
    "            train_features,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_dataloader = DataLoader(\n",
    "            val_features,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Use the training function with model_random\n",
    "        model, train_losses, val_accuracies = train_transformer(\n",
    "            model=random_bert_model,  # Using the randomly initialized model\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            device=device,\n",
    "            epochs=config.epochs,\n",
    "            learning_rate=config.learning_rate,\n",
    "            warmup_steps=config.warmup_ratio * len(train_dataloader),\n",
    "            gradient_clip_val=config.gradient_clip_val,\n",
    "            save_interval=1,\n",
    "            gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "            patience=config.patience,\n",
    "            weight_decay=config.weight_decay,\n",
    "            save_path=f\"./checkpoints/random-sweep-{wandb.run.id}-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        )\n",
    "        \n",
    "        # Return the best validation accuracy\n",
    "        return max(val_accuracies)\n",
    "\n",
    "def run_sweep_pretrained(count=20):\n",
    "    \"\"\"Run hyperparameter sweep for the pretrained model\"\"\"\n",
    "    # Define the parameter search space\n",
    "    sweep_config = {\n",
    "        'method': 'bayes',\n",
    "        'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
    "        'name': 'pretrained_model_sweep',\n",
    "        'parameters': {\n",
    "            'batch_size': {\n",
    "                'values': [8, 16, 32, 64, 128]\n",
    "            },\n",
    "            'learning_rate': {\n",
    "                'distribution': 'log_uniform',\n",
    "                'min': 1e-6,\n",
    "                'max': 1e-3\n",
    "            },\n",
    "            'weight_decay': {\n",
    "                'distribution': 'log_uniform',\n",
    "                'min': 1e-5,\n",
    "                'max': 1e-1\n",
    "            },\n",
    "            'gradient_clip_val': {\n",
    "                'distribution': 'uniform',\n",
    "                'min': 1.0,\n",
    "                'max': 10.0\n",
    "            },\n",
    "            'gradient_accumulation_steps': {\n",
    "                'values': [1, 2, 4, 8]\n",
    "            },\n",
    "            'warmup_ratio': {\n",
    "                'distribution': 'uniform',\n",
    "                'min': 0.0,\n",
    "                'max': 0.5\n",
    "            },\n",
    "            'epochs': {\n",
    "                'values': [50]\n",
    "            },\n",
    "            'patience': {\n",
    "                'values': [5, 10]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create the sweep\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"CommonsenseQA\")\n",
    "    \n",
    "    # Run the sweep\n",
    "    wandb.agent(sweep_id, function=objective_function_pretrained, count=count)\n",
    "\n",
    "def run_sweep_random(count=20):\n",
    "    \"\"\"Run hyperparameter sweep for the randomly initialized model\"\"\"\n",
    "    # Define the parameter search space\n",
    "    # For random initialization, we might want to explore different learning rates\n",
    "    sweep_config = {\n",
    "        'method': 'bayes',\n",
    "        'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
    "        'name': 'random_model_sweep',\n",
    "        'parameters': {\n",
    "            'batch_size': {\n",
    "                'values': [8, 16, 32, 64, 128]\n",
    "            },\n",
    "            'learning_rate': {\n",
    "                'distribution': 'log_uniform',\n",
    "                'min': 1e-6,\n",
    "                'max': 1e-3\n",
    "            },\n",
    "            'weight_decay': {\n",
    "                'distribution': 'log_uniform',\n",
    "                'min': 1e-5,\n",
    "                'max': 1e-1\n",
    "            },\n",
    "            'gradient_clip_val': {\n",
    "                'distribution': 'uniform',\n",
    "                'min': 1.0,\n",
    "                'max': 10.0\n",
    "            },\n",
    "            'gradient_accumulation_steps': {\n",
    "                'values': [1, 2, 4, 8]\n",
    "            },\n",
    "            'warmup_ratio': {\n",
    "                'distribution': 'uniform',\n",
    "                'min': 0.0,\n",
    "                'max': 0.5\n",
    "            },\n",
    "            'epochs': {\n",
    "                'values': [50]\n",
    "            },\n",
    "            'patience': {\n",
    "                'values': [5, 10]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create the sweep\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"CommonsenseQA\")\n",
    "    \n",
    "    # Run the sweep\n",
    "    wandb.agent(sweep_id, function=objective_function_random, count=count)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run pretrained model sweep first\n",
    "    print(\"Starting sweep for pretrained model...\")\n",
    "    run_sweep_pretrained(count=50)\n",
    "    \n",
    "    # Then run random model sweep\n",
    "    print(\"Starting sweep for randomly initialized model...\")\n",
    "    run_sweep_random(count=50)\n",
    "    \n",
    "    # Alternatively, you can run them separately:\n",
    "    # To run only the pretrained model sweep:\n",
    "    # run_sweep_pretrained(count=20)\n",
    "    \n",
    "    # To run only the random model sweep:\n",
    "    # run_sweep_random(count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d85c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(random_losses, random_accuracies, pretrained_losses, pretrained_accuracies):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(random_losses, label='Random Init')\n",
    "    plt.plot(pretrained_losses, label='Pretrained')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(random_accuracies, label='Random Init')\n",
    "    plt.plot(pretrained_accuracies, label='Pretrained')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c47992",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curves(random_losses, random_accuracies, pretrained_losses, pretrained_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61866a81",
   "metadata": {},
   "source": [
    "# **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8fe1f8",
   "metadata": {},
   "source": [
    "Important: Use test split for eval, not validation (& ofc no train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d4be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    all_logits = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Evaluation\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            input_ids, attention_mask, token_type_ids, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = outputs.logits\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            \n",
    "            # Collect labels and predictions\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(preds.cpu().numpy())\n",
    "            all_logits.append(logits.cpu().numpy())\n",
    "    \n",
    "    # Combine all logits\n",
    "    all_logits = np.vstack(all_logits) if all_logits else np.array([])\n",
    "    \n",
    "    # Compute metrics\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score, \n",
    "        precision_score, \n",
    "        recall_score, \n",
    "        f1_score, \n",
    "        confusion_matrix,\n",
    "        classification_report\n",
    "    )\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    \n",
    "    # Only calculate these metrics if there are predictions for each class\n",
    "    try:\n",
    "        precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "        recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "        f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    except:\n",
    "        print(\"Warning: Some classes may not have predictions. Using only accuracy.\")\n",
    "        precision = recall = f1 = None\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    \n",
    "    # Class-wise report \n",
    "    class_report = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "    \n",
    "    # Create a list to map index to answer choice label (A-E)\n",
    "    idx_to_label = {i: chr(65 + i) for i in range(5)}  # 0->A, 1->B, etc.\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    class_accuracies = {}\n",
    "    for i in range(5):\n",
    "        class_indices = np.where(np.array(true_labels) == i)[0]\n",
    "        if len(class_indices) > 0:\n",
    "            class_correct = sum([predicted_labels[j] == i for j in class_indices])\n",
    "            class_accuracies[idx_to_label[i]] = class_correct / len(class_indices)\n",
    "        else:\n",
    "            class_accuracies[idx_to_label[i]] = 0\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'class_report': class_report,\n",
    "        'per_class_accuracy': class_accuracies,\n",
    "        'logits': all_logits\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Per-class accuracy:\")\n",
    "    for label, acc in class_accuracies.items():\n",
    "        print(f\"  Choice {label}: {acc:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2b1c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_model_results = evaluate(random_model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bcd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_results = evaluate(pretrained_model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948de111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the validation set (1 example)\n",
    "results = process_commonsense_qa_for_deepseek(valid, model_deepseek, tokenizer_deepseek, num_examples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b95e68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is a well known way for couples  of celebrating a marriage?']\n",
      "['Question: What is a well known way for couples  of celebrating a marriage?\\nQuestion concept: celebrating\\nChoices: A. eat cake, B. getting drunk, C. having sex, D. cleaning rooms, E. drink too much\\nThe correct answer is:']\n",
      "['Question: What is a well known way for couples  of celebrating a marriage?\\nQuestion concept: celebrating\\nChoices: A. eat cake, B. getting drunk, C. having sex, D. cleaning rooms, E. drink too much\\nThe correct answer is: C. having sex\\nThe correct answer is: C. having sex\\nThe correct answer is: C. having sex\\nThe correct answer is: C. having sex\\nThe correct answer is: C. having sex\\nThe correct answer is: C. having sex\\nThe correct answer is: C. having sex\\nThe correct answer is: C. having sex\\nThe correct answer is: C. having sex\\nThe correct answer is: C. having sex\\nThe correct answer is:']\n",
      "['C']\n"
     ]
    }
   ],
   "source": [
    "print(results['questions'])\n",
    "print(results['prompts'])\n",
    "print(results['responses'])\n",
    "print(results['correct_answers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83e495",
   "metadata": {},
   "source": [
    "# **Interpretation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363fea4",
   "metadata": {},
   "source": [
    "# **Tools used**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0bf1a1",
   "metadata": {},
   "source": [
    "### **Adjust this section before submitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8479f8",
   "metadata": {},
   "source": [
    "1. **Programming Environment**\n",
    "   - Python 3.12.8\n",
    "   - Jupyter Notebook\n",
    "\n",
    "2. **Machine Learning and Deep Learning**\n",
    "   - PyTorch (neural network development)\n",
    "   - Hugging Face Datasets (data management)\n",
    "   - NLTK (natural language preprocessing)\n",
    "   - FastText (pre-trained word embeddings, 300-dimensional vectors)\n",
    "\n",
    "3. **Data Manipulation and Analysis**\n",
    "   - NumPy (numerical computing)\n",
    "   - Pandas (data structuring and manipulation)\n",
    "   - Scikit-learn (potential additional machine learning utilities)\n",
    "\n",
    "4. **Visualization and Tracking**\n",
    "   - Matplotlib (basic plotting)\n",
    "   - Seaborn (statistical data visualization)\n",
    "   - Weights & Biases (experiment tracking and logging)\n",
    "     * Tracked metrics: training loss, accuracy, learning rates\n",
    "     * Logged hyperparameter configurations\n",
    "     * Enabled comparative analysis across model runs\n",
    "\n",
    "5. **Computational Infrastructure**\n",
    "   - CUDA-enabled GPU acceleration\n",
    "   - GPU-optimized PyTorch operations\n",
    "   - Efficient parallel computing for model training\n",
    "\n",
    "6. **Dataset and Benchmarking**\n",
    "   - CommonsenseQA dataset (Hugging Face)\n",
    "   - Standard benchmark for commonsense reasoning tasks\n",
    "\n",
    "7. **Additional Libraries**\n",
    "   - Gensim (word vector processing)\n",
    "   - tqdm (progress bar visualization)\n",
    "   - datetime (experiment timestamping)\n",
    "\n",
    "8. **AI-Tools**\n",
    "   - Claude 3.5 Sonnet: Utilized as a coding assistant for debugging, optimization and documentation.\n",
    "   - GPT-4-turbo: Assisted in drafting and refining documentation, helping with structure and phrasing.\n",
    "   - Copilot: Used for quick inserts, when recommendation was suitable for what I was planning to do.\n",
    "\n",
    "9. **Sources**\n",
    "   - Transformer architecture: https://medium.com/data-science/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\n",
    "   - Deepseek implementation: https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
