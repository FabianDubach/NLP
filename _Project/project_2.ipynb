{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4357e38e",
   "metadata": {},
   "source": [
    "## Step 1: Set Up the CommonsenseQA Dataset\n",
    "\n",
    "Download and prepare the CommonsenseQA dataset\n",
    "Split the data into train/validation/test sets if not already done\n",
    "Understand the format (questions, multiple-choice answers)\n",
    "\n",
    "## Step 2: Set Up Three Models\n",
    "\n",
    "Randomly Initialized Transformer\n",
    "\n",
    "Build a transformer architecture from scratch\n",
    "Initialize weights randomly\n",
    "This will serve as your baseline\n",
    "\n",
    "\n",
    "Pretrained Transformer\n",
    "\n",
    "Use the same transformer architecture as Model 1\n",
    "Initialize with pretrained weights (e.g., BERT, RoBERTa)\n",
    "Make sure this model wasn't specifically trained on CommonsenseQA\n",
    "\n",
    "\n",
    "Large Language Model (1B+ parameters)\n",
    "\n",
    "Choose an LLM (e.g., GPT-2, LLaMA, OPT, BLOOM)\n",
    "No finetuning for this model - just prompt engineering\n",
    "\n",
    "\n",
    "\n",
    "## Step 3: Training/Finetuning\n",
    "\n",
    "Finetune Models 1 & 2 on CommonsenseQA train set\n",
    "\n",
    "Use the same hyperparameters for both\n",
    "Train for multiple epochs\n",
    "Save checkpoints and track validation performance\n",
    "\n",
    "\n",
    "For Model 3 (LLM), develop effective prompts instead of finetuning\n",
    "\n",
    "## Step 4: Prompt Engineering (for LLM)\n",
    "\n",
    "Design different prompt formats\n",
    "Test various instruction styles\n",
    "Try few-shot examples in prompts\n",
    "Experiment with temperature and other generation parameters\n",
    "\n",
    "## Step 5: Evaluation\n",
    "\n",
    "Evaluate all three models on the test set\n",
    "Calculate accuracy, F1 score, or other relevant metrics\n",
    "Compare performance across models\n",
    "\n",
    "## Step 6: Analysis\n",
    "\n",
    "Analyze which types of questions each model handles well/poorly\n",
    "Look at error patterns\n",
    "Discuss why certain approaches work better\n",
    "\n",
    "## Step 7: Create Presentation\n",
    "\n",
    "Summarize methodology\n",
    "Present results with visualizations\n",
    "Include discussion of findings\n",
    "Provide limitations and potential improvements\n",
    "\n",
    "Technical Requirements:\n",
    "\n",
    "Programming language: Python recommended\n",
    "Libraries: PyTorch/TensorFlow, Transformers (Hugging Face), etc.\n",
    "Computational resources: You'll need GPU access for training\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "**Delete steps generated by Claude later**\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5cc4f4",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d849338",
   "metadata": {},
   "source": [
    "Import all libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5a12ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d266c9",
   "metadata": {},
   "source": [
    "Setup random seed to ensure reproducibility.\n",
    "\n",
    "_Info about the seed value: The field of natural language processing began in the 1940s, after World War II. At this time, people recognized the importance of translation from one language to another and hoped to create a machine that could do this sort of translation automatically._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a7ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1940 # normal: 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a438be37",
   "metadata": {},
   "source": [
    "In the next step I import and split the dataset. For the split I took off the last 1000 entries from the train-split and used it as validation, the rest of this is of course used for the training. Then I used the validation-part as the test. This was done since the real test-split has no answer keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22c7f692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8741 1000 1221\n"
     ]
    }
   ],
   "source": [
    "train = load_dataset(\"tau/commonsense_qa\", split=\"train[:-1000]\")\n",
    "valid = load_dataset(\"tau/commonsense_qa\", split=\"train[-1000:]\")\n",
    "test = load_dataset(\"tau/commonsense_qa\", split=\"validation\")\n",
    "\n",
    "print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff2820",
   "metadata": {},
   "source": [
    "Login for the experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "492738f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfabian-dubach\u001b[0m (\u001b[33mfabian-dubach-hochschule-luzern\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48b5c1",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec5d2d0",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399fe1e",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41073d29",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61866a81",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8fe1f8",
   "metadata": {},
   "source": [
    "Important: Use test split for eval, not validation (& ofc no train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83e495",
   "metadata": {},
   "source": [
    "# Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363fea4",
   "metadata": {},
   "source": [
    "# Tools used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8479f8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
