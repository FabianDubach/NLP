{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4357e38e",
   "metadata": {},
   "source": [
    "## Step 1: Set Up the CommonsenseQA Dataset\n",
    "\n",
    "Download and prepare the CommonsenseQA dataset\n",
    "Split the data into train/validation/test sets if not already done\n",
    "Understand the format (questions, multiple-choice answers)\n",
    "\n",
    "## Step 2: Set Up Three Models\n",
    "\n",
    "Randomly Initialized Transformer\n",
    "\n",
    "Build a transformer architecture from scratch\n",
    "Initialize weights randomly\n",
    "This will serve as your baseline\n",
    "\n",
    "\n",
    "Pretrained Transformer\n",
    "\n",
    "Use the same transformer architecture as Model 1\n",
    "Initialize with pretrained weights (e.g., BERT, RoBERTa)\n",
    "Make sure this model wasn't specifically trained on CommonsenseQA\n",
    "\n",
    "\n",
    "Large Language Model (1B+ parameters)\n",
    "\n",
    "Choose an LLM (e.g., GPT-2, LLaMA, OPT, BLOOM)\n",
    "No finetuning for this model - just prompt engineering\n",
    "\n",
    "\n",
    "\n",
    "## Step 3: Training/Finetuning\n",
    "\n",
    "Finetune Models 1 & 2 on CommonsenseQA train set\n",
    "\n",
    "Use the same hyperparameters for both\n",
    "Train for multiple epochs\n",
    "Save checkpoints and track validation performance\n",
    "\n",
    "\n",
    "For Model 3 (LLM), develop effective prompts instead of finetuning\n",
    "\n",
    "## Step 4: Prompt Engineering (for LLM)\n",
    "\n",
    "Design different prompt formats\n",
    "Test various instruction styles\n",
    "Try few-shot examples in prompts\n",
    "Experiment with temperature and other generation parameters\n",
    "\n",
    "## Step 5: Evaluation\n",
    "\n",
    "Evaluate all three models on the test set\n",
    "Calculate accuracy, F1 score, or other relevant metrics\n",
    "Compare performance across models\n",
    "\n",
    "## Step 6: Analysis\n",
    "\n",
    "Analyze which types of questions each model handles well/poorly\n",
    "Look at error patterns\n",
    "Discuss why certain approaches work better\n",
    "\n",
    "## Step 7: Create Presentation\n",
    "\n",
    "Summarize methodology\n",
    "Present results with visualizations\n",
    "Include discussion of findings\n",
    "Provide limitations and potential improvements\n",
    "\n",
    "Technical Requirements:\n",
    "\n",
    "Programming language: Python recommended\n",
    "Libraries: PyTorch/TensorFlow, Transformers (Hugging Face), etc.\n",
    "Computational resources: You'll need GPU access for training\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "TODO: Checkpointing, Early stopping works?, Log to Wandb, sweeps or other auto tool (optional), llm\n",
    "\n",
    "**Delete steps generated by Claude later**\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5cc4f4",
   "metadata": {},
   "source": [
    "# **FS25 NLP Project 1: Word Embeddings/Recurrent Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52707bad",
   "metadata": {},
   "source": [
    "Fabian Dubach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee79ba7",
   "metadata": {},
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df05906a",
   "metadata": {},
   "source": [
    "<style>\n",
    "  .container {\n",
    "    display: flex;\n",
    "    align-items: flex-start;\n",
    "    gap: 20px; /* spacing between text and ASCII art */\n",
    "    font-family: monospace;\n",
    "  }\n",
    "  .text {\n",
    "    flex: 2;\n",
    "  }\n",
    "  .ascii {\n",
    "    white-space: pre;\n",
    "    font-size: 4.5px;\n",
    "    line-height: 1.2;\n",
    "    flex: 1;\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<div class=\"container\">\n",
    "  <div class=\"text\">\n",
    "    <p>The task for my project was to perform common sense question answering using the CommonsenseQA dataset.</p><br>\n",
    "    <p>I evaluated the performance of three different Transformer-based models:</p>\n",
    "    <p>1. A randomly initialized Transformer</p>\n",
    "    <p>2. A pretrained Transformer (with the same architecture as the first Transformer)</p>\n",
    "    <p>3. A large language model (LLM) with over 1 billion parameters</p><br>\n",
    "    <p>While the first two models were finetuned on the dataset using the same hyperparameters for a fair comparison, the LLM was evaluated through prompt engineering without additional training. This setup allowed me to explore how different levels of pretraining and model scale impact common sense reasoning performance.</p>\n",
    "    <p>We had to also track the trainings with Wandb (workspace URL: <a href=\"https://wandb.ai/fabian-dubach-hochschule-luzern/CommonsenseQA/workspace?nw=nwuserfabiandubach\" target=\"_blank\">https://wandb.ai/fabian-dubach-hochschule-luzern/CommonsenseQA/workspace?nw=nwuserfabiandubach</a>).</p>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"ascii\">\n",
    "<pre>\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣶⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⠀⠀⠀⠀⠀⣤⣤⣤⠀⠀⠀⠀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⠀⣠⡶⢿⡇⢿⣿⡏⢳⣦⠀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⡛⣆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣧⣼⣿⣴⣋⡽⠮⠿⢭⣟⣏⣷⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⣧⠘⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡼⣇⣿⡿⠶⣶⣿⣟⡛⣷⣿⢠⠙⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⡈⣏⠇⢹⡀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡟⢹⠁⣿⠋⠉⢹⠉⠙⣿⡇⣾⣀⣾⠀⢀⣤⡀⢀⡀⠀⠀⢀⣠⣴⣾⠛⢻⡛⢻⡄⢀⣳⡀⢀⣠⠄⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⣷⣾⢀⣿⡇⠀⠸⠀⠀⣿⣧⡽⠿⣟⣺⣭⠴⢿⡏⣩⣷⡾⢛⣭⣴⣿⣇⠘⣿⣷⣿⡛⠉⢻⣟⣷⠄⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠿⢿⣟⣿⣿⡦⣶⣪⡭⠿⣚⣫⣭⣽⣶⡄⠀⢸⡇⣿⡙⣿⣿⣿⣿⣿⣿⣆⠹⣿⣿⣷⡀⠀⢿⡉⠁⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⣤⣶⣿⠿⠛⣉⣭⣶⣾⣿⠿⠟⠛⠉⠉⢻⠀⢸⣷⣿⣇⢻⡿⣿⣿⣿⣿⠟⠀⠹⣿⣿⠃⠀⠘⣷⡀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣤⣦⣼⣿⠿⠛⣋⡁⣼⢠⣿⡿⠛⠉⠁⠀⠀⢀⡀⢀⣴⣾⠀⢸⣿⡇⢻⡄⠙⠿⠻⠛⠁⠀⢀⣠⣽⣿⣇⡀⠀⠸⣧⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⠿⣛⣭⣴⡾⠟⠛⣧⣿⢸⡿⠀⠀⠀⠀⣰⣿⣿⣷⣾⣿⣿⠀⢸⡏⣇⢸⣷⡀⠀⢀⣠⣴⣾⠿⠛⣿⢻⣿⣹⡀⠀⢻⣆⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣴⡟⣦⠀⠀⠀⢀⡿⣵⡿⠛⠉⣡⣶⣤⣄⣿⣯⢸⣇⠀⠀⢠⣾⣿⡿⣿⣿⣿⣿⡿⠀⢸⡇⢻⡼⣿⣷⣶⠿⠛⠉⠀⠀⠀⠸⡇⣿⣿⣧⠀⠘⣿⡀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⢹⠀⢀⣠⣼⣿⣿⠀⢀⣼⣿⣿⣿⣿⡇⣿⢸⣿⣀⣀⣿⡿⠿⠶⠚⠛⠉⠉⠀⠀⢸⡇⠀⢻⣾⣝⣿⡆⠀⢀⣠⡴⠖⠛⢻⡾⣿⣿⣆⠀⢹⡇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣇⣼⡾⠟⠋⣿⢻⣇⣤⣌⠻⢿⣿⣿⣿⠃⢿⠀⠉⠉⠁⠀⠀⠀⣀⣤⡤⠶⠶⠒⠚⣻⣷⣄⠈⣿⣿⣿⣿⡞⠉⠀⠀⠀⠀⠀⣿⢿⣿⣾⣋⣽⠇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣹⠏⠀⠀⠀⣿⢿⣿⣿⣯⡴⠾⠛⢋⣡⠶⠛⠛⠋⣉⣉⣉⣙⢻⣿⠀⠀⠀⠀⠀⢠⡟⠀⠈⠻⢦⣈⣿⣿⣧⠀⠀⢀⣠⣴⡾⢿⣿⣿⣿⣿⣿⡀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡟⣿⡟⠀⠀⠀⣿⠈⠋⠉⢀⣠⠴⣛⣩⣤⣶⣞⣭⣿⢿⣿⣿⣻⣼⣿⣆⣀⣤⣤⣴⣿⣄⣠⣶⣦⣀⣙⣿⣿⣿⡶⣿⠟⠋⣁⣶⠟⢻⣽⣿⣿⣿⠇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⢠⣿⣇⠀⠀⠀⢹⣠⡴⠖⢻⣷⢫⣿⣿⣿⣯⣿⣟⣿⣿⣭⣽⣿⡿⣿⣿⣿⠿⠿⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⣿⠋⠉⣿⠀⢸⣿⣿⣿⣿⣷⡀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣼⣿⣿⣤⣴⣾⢿⡅⠀⣀⣾⢿⣿⣿⣿⣿⣿⣿⡿⣿⣷⣿⣿⣿⡇⣿⣿⡇⠀⠀⢸⣿⣿⡟⢿⣿⣿⣿⣿⣿⣣⣿⠁⣿⣀⣤⡿⠀⢀⣿⣿⣿⣿⣿⡇⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⠻⣿⠛⠉⠀⠈⣿⠛⢽⣿⢻⣿⣿⢿⣿⣿⣿⡇⣿⠿⣶⣶⣚⣧⣿⣿⡇⠀⠀⣸⣿⣿⣿⣄⣈⢿⣿⢿⣷⣿⣿⠀⠉⠉⠀⠀⠀⠘⡇⣿⣿⣿⣿⡇⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⡀⣷⡆⠀⠀⠀⠸⣧⣻⣿⢸⣿⣿⡿⢿⣾⣻⡇⣿⣿⣿⣿⣿⣿⣿⠿⠷⠾⠛⠛⠿⢿⣿⣿⣿⣄⣿⠿⠋⢸⣿⠀⠀⠀⠀⠀⠀⠀⡇⣿⣿⣿⣿⣿⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣷⡇⣿⡇⠀⠀⠀⠀⣿⣿⣿⡾⢿⣿⣿⣿⣿⡶⠷⠾⠛⠛⠉⠁⢀⣠⠤⠴⠒⡆⢠⠀⢰⡉⠻⣿⣽⡏⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⠀⡇⣿⡿⣿⣿⣿⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣧⣿⠿⢀⣀⣤⣴⣿⣿⣿⡷⠾⠛⠋⠉⢀⣀⣠⠤⠴⠒⠻⡆⢸⠀⠀⢀⡠⠇⠸⡄⠈⣇⠀⠈⡻⢦⡀⠀⢸⡇⠀⠀⠀⠀⠀⠀⠀⡇⣿⣧⡘⠿⢻⡆\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠻⣆⣿⣿⣿⣿⣿⡿⠛⣉⣀⡀⣠⠴⠒⠋⠉⠁⠀⠀⠀⠀⠀⡇⢸⣠⠴⣫⡄⠀⠀⡇⠀⢹⠀⠀⣿⠦⢿⡀⢸⡇⠀⠀⣀⣤⣤⣿⠀⡇⣿⣿⣿⣆⢸⡇\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⢿⡟⣽⣿⠀⣏⠁⠀⡇⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣇⠀⡖⣻⠋⠀⠀⠈⢻⠀⢈⡇⠀⠸⡄⠘⣧⢸⡇⠀⢸⣷⣾⣿⠏⠀⡇⣿⣿⣿⣿⢸⡇\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣾⠏⠛⠋⢡⣿⠀⠸⣿⣟⡃⣇⠀⠀⠀⠀⠀⣀⣠⡤⠶⠒⠋⠀⠛⠁⠀⣀⣤⣶⣿⣿⣿⣿⣷⣤⡈⠁⢻⡞⣿⠀⠈⠻⣴⠏⠀⠀⠿⢹⣿⣎⢻⣿⡇\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣾⡟⠀⠀⢀⡿⣿⠀⠀⠈⠳⡇⠻⠤⠶⠚⠋⠉⠁⠀⠀⠀⠀⠀⣀⣤⣶⣿⣿⣿⣿⣿⠿⠛⠻⣿⣿⣿⣷⣜⣷⣿⠀⠀⢀⣀⣤⣤⣶⣾⣶⣿⣿⠃⢸⡇\n",
    "⠀⠀⠀⠀⠀⠀⣀⣤⡶⠶⠖⠚⢛⠛⠳⢶⣼⡟⠀⠀⢀⣼⣹⣿⢀⠀⠀⠀⠀⡀⠀⠀⠀⠀⠀⢀⣀⣠⡤⢤⣾⣿⣿⣿⡿⠿⠛⠉⠹⡇⠀⠀⣿⣿⣟⢿⣿⣿⠹⣶⣿⡿⠛⠻⣏⠀⠉⠉⡛⣿⡿⣾⡇\n",
    "⠀⠀⠀⢀⣴⠞⠋⢰⡇⢰⣿⢻⢻⢻⢶⣦⠙⣷⡀⠀⣸⢧⠟⢿⣿⣿⣿⣷⣶⣶⣤⣴⣲⡾⠿⠟⠒⠒⠛⡇⠙⣿⠉⠀⢧⠀⠀⠀⠀⣧⠀⠀⢸⣿⣿⡎⣿⠁⢀⣼⣏⢀⣠⣤⣸⣶⠀⠀⣿⣿⣿⠛⠁\n",
    "⠀⠀⠀⣾⠃⠀⣠⡬⣤⣼⣛⠾⣼⣞⡾⡟⠀⠘⣧⣠⣏⡞⠀⠈⠻⣿⡏⢹⡟⠛⠻⣿⠁⠀⠀⠀⠀⠀⠀⣇⠀⣿⠀⠀⢸⡄⠀⠀⠀⢸⠀⠀⠘⣿⣿⣇⣿⣴⡞⢣⣽⣿⣿⣿⣿⣿⠀⠀⣿⣿⡟⠀⠀\n",
    "⠀⠀⠀⣿⡶⣿⣿⣸⣿⣿⣿⠿⠷⠾⢽⣅⡲⠶⢻⣿⣼⢁⣠⣤⣶⣿⣿⠘⡇⠀⠀⢻⡆⠀⠀⠀⠀⠀⢀⣸⡀⢹⡇⠀⠈⡇⠀⠀⠀⠈⡇⠀⠀⢿⣿⣿⢹⣿⣤⣿⣿⣿⣿⡿⢿⣟⡀⠀⣿⣿⡇⠀⠀\n",
    "⠀⠀⠀⠈⠛⠿⢯⣜⣿⠏⠀⠀⠀⢀⡿⣨⣿⣶⣤⣿⣷⣯⣿⣿⣿⣿⣿⠀⡇⠀⠀⠐⡿⣦⣰⣒⣶⣿⣿⣿⣷⣾⣇⠀⠀⢻⠀⠀⠀⠀⢷⠀⠀⢸⣿⣿⣾⣿⣸⣿⡏⢠⠟⣠⣿⣿⣿⣦⡈⢹⡇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⢸⡟⣾⠄⠀⠀⣸⡇⣿⣿⣿⠟⠋⠛⢿⣿⣿⣿⣿⣿⡄⢻⠀⠀⠀⡇⠈⠙⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⢸⡆⠀⠀⠀⢸⡄⠀⠀⣿⣿⣇⣿⠛⠛⠻⣿⣺⣿⣿⣿⣿⣿⣿⡿⠃⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⣼⢧⡇⠀⠀⠀⣿⢸⣿⣿⡿⢦⣴⣿⣿⣷⡿⣿⡿⣿⡇⢸⡄⠀⠀⢹⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⡆⠀⠀⣇⠀⠀⠀⠀⣇⠀⠀⢸⣿⣟⢿⡀⠀⠀⠈⠉⠀⠉⠉⠉⠁⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⣿⣨⡧⠤⠤⢤⣇⡾⣿⣿⣠⣿⣿⣿⣿⣿⣿⣽⣿⣿⣷⠀⣇⠀⠀⢸⠀⠀⢸⢻⣿⣿⣿⣿⡇⣿⣿⠀⠀⢹⡄⠀⠀⢀⣸⠀⠀⠸⣿⣿⣼⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⢀⡿⣧⣤⠶⠦⣼⣿⣿⣿⡏⠈⣿⣿⢿⣿⣿⣿⣏⠉⢹⣿⡀⢻⠀⠀⠘⡇⠀⠸⡄⠙⢿⣿⣿⠇⣿⣿⡄⠀⠈⠓⠒⠋⠉⠀⠀⠀⠀⢿⠹⣯⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⣸⣿⢃⡏⠀⠀⢻⣿⣿⣽⣿⣦⠘⣿⣿⣿⣿⣿⢻⣿⣾⣿⡇⠘⡇⠀⠀⣇⠀⠀⣇⠀⠀⠙⢿⡇⣿⢸⣧⠀⠀⠀⠀⡴⠒⢶⠀⠀⠀⠘⣆⠀⢻⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⡿⡅⣸⢁⣄⡄⣾⣿⢿⣿⠿⣿⣿⢻⣿⣿⣟⣿⣸⣻⡿⣿⣧⠀⠙⠒⠛⠛⠀⠀⢿⣿⣄⠀⠀⠀⣿⠈⣿⡄⠀⠀⠀⡇⠀⠘⡇⠀⠀⠀⢿⣦⢸⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⢸⣧⡇⣿⣼⣿⠃⣿⣿⣾⣿⣷⣤⡿⠿⢿⣿⣿⣇⣿⡟⠋⠀⣿⡀⠀⣴⠲⡆⠀⠀⠸⣿⣿⣦⠀⠀⢸⡀⢹⣧⠀⠀⠀⣇⠀⠀⢹⠀⠀⠀⠸⣿⡟⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⢽⡿⣷⠏⠛⠿⢠⣿⣿⣿⣿⢿⣯⡇⠀⠀⠈⠁⠀⠀⠀⠀⠀⢸⣇⠀⢻⠀⢳⠀⠀⠀⣿⣿⣿⣷⣾⢸⡇⠈⣿⡀⠀⠀⢸⠀⠀⠈⡇⠀⠀⢀⣿⣿⣷⣀⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠘⣧⡙⣀⣀⣀⣸⣿⣽⣿⣿⠀⠈⠙⣶⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡀⢸⡀⠸⡄⠀⠀⢻⣿⣿⣿⣿⡼⡇⠀⢘⣧⣤⡴⠾⠷⠶⠖⠛⠛⢛⠋⠉⢿⢹⠉⣭⡿⠿⠷⠶⢦⡄⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠹⣟⣁⣸⣿⣿⣧⡿⠿⣿⣀⡀⠀⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣇⣈⣧⣘⣷⣤⣤⣼⠿⠿⣿⣿⣧⣧⡀⣸⢹⡏⠀⠀⠀⠀⠀⠀⠀⠈⡇⠀⢸⢸⡄⡿⠖⠚⠉⡉⠓⢿⡀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⣠⡴⣾⠋⠉⢙⣻⣷⠛⠛⠳⠶⠶⠽⠿⠃⠀⠀⠀⠀⠀⣀⡤⣼⡿⠋⠉⠁⠀⠀⣠⠀⣿⣿⠀⠀⠀⠀⠈⠉⠻⣿⢸⣷⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠸⡏⡇⣿⠀⠀⠀⢻⣷⢸⡇⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⡟⠀⡟⠀⠀⢸⣿⣿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⡥⢺⠏⡆⠀⠀⠀⠀⠀⡏⠀⡟⡇⠀⠀⠀⠀⠀⠀⢀⡇⢸⣿⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⡇⡇⢿⠀⠀⠀⢸⣿⡌⣷⠀⠀⠀⠀\n",
    "⠀⠀⠀⢸⠇⢠⡇⠀⠀⢰⣿⣯⣏⣻⡆⠀⠀⠀⠀⠀⠀⠀⠀⣸⠃⢀⡿⢸⡇⠀⠀⠀⠀⢠⡇⠀⡇⡇⠀⠀⠀⠀⠀⠀⢸⡇⢸⣿⡆⠀⠀⠀⠀⠀⠀⠀⣧⠀⠀⡇⢿⢸⠀⠀⠀⠈⣿⡇⢹⡀⠀⠀⠀\n",
    "⠀⠀⠀⡟⡄⣼⠀⠀⢀⣿⣿⣿⣿⣿⣷⠀⠀⠀⠀⠀⠀⠀⠀⣿⠀⢸⡇⣸⡇⠀⠀⠀⠀⢸⠁⢸⣷⡇⠀⠀⠀⠀⠀⠀⢸⡇⢸⣿⡇⠀⠀⠀⠀⠀⠀⠀⢻⠀⠀⢹⢸⣼⡀⠀⣀⣀⣿⣧⣸⡇⠀⠀⠀\n",
    "⠀⠀⢰⢧⣇⡏⠀⠀⣸⣿⠿⢭⣿⣿⡏⠀⠀⠀⠀⠀⠀⠀⢰⡏⠀⣿⠀⣿⡇⠀⠀⠀⠀⢸⠀⢸⢸⠁⠀⠀⠀⠀⠀⠀⢸⡇⢸⣿⣿⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⢸⢸⣿⡏⢉⣁⣤⣤⣄⢈⡇⠀⠀⠀\n",
    "⠀⠀⣼⢼⣿⠃⠀⠀⣿⣿⠀⢸⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⢸⡇⢠⡿⢰⣿⠃⠀⠀⠀⠀⣼⠀⢸⢸⠀⠀⠀⠀⠀⠀⠀⢸⡇⢸⢹⣸⣦⣤⣤⣤⣶⣶⣶⡿⠀⠀⢸⡄⡇⣧⣽⣿⣿⣿⡽⠟⠁⠀⠀⠀\n",
    "⠀⠀⢿⢻⡏⠀⠀⢰⣿⣿⣟⠛⢿⣿⡇⠀⠀⠀⠀⠀⠀⠀⢸⠗⣻⡇⢸⢹⣆⣀⣀⣀⣤⡏⠀⢸⢸⠀⠀⠀⠀⠀⠀⠀⢸⡇⢸⠈⠉⠉⠉⠉⠉⠉⠀⠀⠀⠀⠀⠈⡇⣿⠘⣿⣿⣿⣇⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⢸⠛⠤⢤⣤⣘⢺⣿⣿⣿⣿⡿⠃⠀⠀⠀⠀⠀⠀⠀⠸⢧⣿⠃⠘⠓⠛⠛⠛⠋⠉⠁⠀⢼⢸⠀⢰⡾⠿⠛⠛⠿⢿⡇⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⢸⠀⠙⣿⣿⣿⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⢘⣶⡶⠚⠿⢿⣿⣩⢿⢿⡏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣸⠀⢸⡇⠀⠀⠀⠀⣿⡇⡏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢷⢸⡀⠀⠈⠁⢸⡇⠀⠀⠀⠀⠀\n",
    "⠀⠀⣼⣹⠃⠀⢰⣷⢻⠁⠈⠛⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡟⣹⠀⢸⠃⠀⠀⠀⠀⣿⠇⠜⠀⣤⠶⠖⠛⠛⠋⠉⠉⢩⣿⡇⠀⢸⠸⡇⠀⠀⠀⠘⡇⠀⠀⠀⠀⠀\n",
    "⠀⢠⡟⠏⠀⠀⣾⣿⣼⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⡇⠀⢀⣴⠶⠞⠛⠛⣻⣷⠀⡏⣿⠀⢸⢀⣴⣷⣦⡀⣿⠇⡇⠀⡟⠀⣀⣀⣀⣀⣀⣀⣸⣿⡇⠀⢸⡆⡇⠀⠀⠀⠀⣷⠀⠀⠀⠀⠀\n",
    "⠀⣸⠇⠀⠀⢸⣿⡇⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⣾⣀⣀⣤⣤⣶⣿⡿⠀⡇⣿⠀⢸⣿⣿⣿⣫⣾⣿⠀⡇⢠⣟⣿⣿⣿⡿⠿⠿⠿⠿⠁⡇⠀⠈⡇⣷⢀⡀⠀⠀⢻⠀⠀⠀⠀⠀\n",
    "⠀⣿⡼⠀⠀⡟⣿⣷⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⣿⠀⢀⡟⡿⠿⠟⠛⠛⣃⡇⠀⡇⣿⠀⢸⣿⣿⣿⣿⣿⣿⡄⡇⢸⡇⠀⠀⠀⠀⠀⠀⠀⢰⣶⡇⠀⠀⣇⢹⣾⣿⠀⣰⢾⡆⠀⠀⠀⠀\n",
    "⢠⣿⡇⠀⢸⣷⣿⣹⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⠀⢸⡇⠀⠀⠀⠀⢰⣿⡇⠀⡇⣿⠀⣾⣿⣿⣿⣿⣿⣿⡃⡇⢸⣧⣤⣤⣴⣶⣶⣶⣶⣾⣿⡇⠀⠀⢿⢸⣿⣿⣾⣿⣸⡇⠀⠀⠀⠀\n",
    "⢸⢭⠥⠦⣬⣽⣧⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣸⣿⠀⢸⢵⣶⣾⣿⣿⣿⡿⡇⠀⡇⣿⠀⣿⣿⣿⣿⣿⣿⣿⡇⡇⢸⡏⠿⠟⠛⠛⠛⠛⠛⠛⣧⣷⠀⠀⢸⠀⣿⣿⣿⣿⠛⣇⠀⠀⠀⠀\n",
    "⢸⣸⠁⢠⣿⣿⣹⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⢸⠉⠉⠉⠁⠀⢠⣾⡇⠀⡇⣿⠀⣿⣿⣿⣿⣿⣿⣿⠇⡇⢸⡇⠀⣀⣀⣀⣀⣀⣀⣰⣿⣿⠀⠀⠸⠀⣿⣿⣿⣵⡇⣿⠀⠀⠀⠀\n",
    "⠘⣧⣰⠞⣞⣷⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⢸⣀⣀⣠⣤⣤⣼⣿⡇⠀⡇⣿⠀⢈⣭⣭⠭⠽⠭⣿⡇⡇⢸⣟⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡀⠀⠀⠀⢻⣟⣾⣿⣿⢻⠀⠀⠀⠀\n",
    "⠀⠈⠛⠛⠛⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⣿⠿⠿⠿⠿⠟⢛⣻⡇⠀⡇⢻⠀⢸⠁⠀⠀⠀⠀⣿⡇⡇⠸⡏⠉⠀⠀⠀⠀⠀⠀⠀⣼⣿⡇⠀⠀⠀⢸⣿⣿⣿⣿⢸⡆⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣇⠀⣿⣀⣤⣤⣤⣤⣼⣿⡇⠀⡇⢸⠀⢸⠀⣠⣶⣄⠀⣿⡇⣇⠀⡇⣴⣶⣶⣾⣿⣿⣿⣿⣿⣿⣇⣀⣂⠀⢸⣿⣿⣿⣿⣿⡇⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⡿⠴⠿⠿⠿⠿⠿⠿⠿⠿⠷⣦⡄⢸⠀⢸⣾⣿⣿⢟⣴⣿⣷⣼⠶⠗⠛⠛⠛⠛⠛⠛⠛⠋⠉⠉⠉⢉⡟⣧⠈⣿⣿⣿⣿⡿⣧⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣴⣿⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⣴⣿⡇⢸⣴⢾⣿⡿⣻⣿⣿⣿⣿⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⣿⣿⡇⢸⣿⢸⣿⣿⣿⣿⣿⣿⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⣿⣿⡀⣿⣿⣿⣿⣿⣿⡄⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣿⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣾⣿⣿⣿⡇⢸⣿⢸⣿⣿⣿⣿⣿⠃⠀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣿⣿⣿⡇⢸⣿⣿⣿⣿⢻⡇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣿⣷⣶⣶⣶⣶⣶⣶⣶⡶⠶⠦⠤⣾⣿⣿⣿⣿⣷⢘⣿⢸⣿⣿⣿⣿⡏⣭⠭⠭⠭⠤⠤⠤⠴⠶⠶⠶⠶⠶⠶⠶⠱⣌⢻⣿⣧⢸⣿⣿⣿⣿⣾⣇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⡾⠟⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⣉⣽⣿⣾⣿⣿⣿⣿⣿⠀⣿⢸⣿⣿⣿⡟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⡞⣿⢻⠈⣿⣿⣿⣿⣿⣿⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣶⠟⠋⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠⣴⣾⣿⣿⣿⣿⣿⣿⣿⠛⢹⠀⣿⣾⣿⣿⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⣿⣿⣿⢻⣿⡀⣿⣿⣿⣿⣿⣿⡄⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣾⣿⣀⣤⣄⣤⣤⣄⣀⣀⣀⣀⣶⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣅⢸⠀⣿⡿⣿⣿⣤⣤⣤⡤⠤⠤⠶⠶⠶⠖⠒⠒⠒⠚⠛⠛⠛⠺⣿⣿⣿⡇⠹⡇⣿⣿⣿⣿⣿⣿⡇⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⣸⣿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⣿⣿⣿⣿⡟⠉⢹⣿⣿⣿⣿⡿⠿⡾⠀⣿⡇⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⣿⠰⠇⣿⣿⣿⣿⡿⣿⡇⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⢿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡟⠛⠉⠁⠀⠀⠀⠙⠛⠉⠁⠀⠀⠁⠀⣛⣁⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⠟⣹⡇⢀⣙⣿⣯⡷⠿⠛⠁⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠉⠉⠉⠉⠉⠉⠹⠷⣦⣤⣤⣤⣤⣤⣤⣤⣤⣤⣶⣶⣶⡶⠶⠶⠶⠶⠾⠿⠛⠛⠋⠉⠉⠁⠀⠀\n",
    "</pre>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48b5c1",
   "metadata": {},
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d849338",
   "metadata": {},
   "source": [
    "Import all libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a12ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMultipleChoice,\n",
    "    BertTokenizer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    GenerationConfig,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d266c9",
   "metadata": {},
   "source": [
    "Setup random seed to ensure reproducibility.\n",
    "\n",
    "_Info about the seed value: The field of natural language processing began in the 1940s, after World War II. At this time, people recognized the importance of translation from one language to another and hoped to create a machine that could do this sort of translation automatically._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a7ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1940 # normal: 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a438be37",
   "metadata": {},
   "source": [
    "In the next step I import and split the dataset. For the split I take off the last 1000 entries from the train-split and use it as validation, the rest of this is of course used for the training. Then I use the validation-part as the test, since the real test-split has no answer keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c7f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_dataset(\"tau/commonsense_qa\", split=\"train[:-1000]\")\n",
    "valid = load_dataset(\"tau/commonsense_qa\", split=\"train[-1000:]\")\n",
    "test = load_dataset(\"tau/commonsense_qa\", split=\"validation\")\n",
    "\n",
    "print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff2820",
   "metadata": {},
   "source": [
    "Login for the experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492738f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f8b6b6",
   "metadata": {},
   "source": [
    "# **Data Exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[4m\" + \"Dataset Features\" + \"\\033[0m\")\n",
    "for feature in train.features:\n",
    "    print(feature)\n",
    "print(\"\\n\" + \"\\033[4m\" + \"Example\" + \"\\033[0m\")\n",
    "for feature in train.features:\n",
    "    print(feature + \":\", train[0][str(feature)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec5d2d0",
   "metadata": {},
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d03e9f7",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c9643",
   "metadata": {},
   "source": [
    "During the preprocessing phase of my NLP project, I carefully considered several common text-cleaning and preparation techniques. Below is a breakdown of each step, whether I used it, and the reasoning behind my decision.\n",
    "\n",
    "1. **Tokenization**  \n",
    "   ✅ *Used*  \n",
    "   I used the `BertTokenizer` from Hugging Face to tokenize all text inputs. This tokenizer breaks text into subword units and adds special tokens, ensuring compatibility with the BERT model architecture.\n",
    "\n",
    "2. **Lowercasing, Stemming, Lemmatizing, Stopword/Punctuation Removal**  \n",
    "   ❌ *Not used*  \n",
    "   These steps are common in traditional NLP pipelines but not necessary when using a pre-trained transformer like BERT. I specifically used the `'bert-base-cased'` model, which is sensitive to letter casing. Applying lowercasing or stripping punctuation could disrupt the model's understanding of context. Similarly, stemming or lemmatizing would interfere with subword tokenization, which already handles morphological variations effectively.\n",
    "\n",
    "3. **Removal of Unknown/Other Words**  \n",
    "   ❌ *Not explicitly used*  \n",
    "   Instead of manually removing unknown words, I relied on the tokenizer to handle them. Words not in the vocabulary are broken into subword tokens or mapped to the `[UNK]` token if completely unrecognized. BERT is designed to handle such cases gracefully.\n",
    "\n",
    "4. **Format Cleaning (e.g., HTML-extracted text)**  \n",
    "   ✅ *Used when necessary*  \n",
    "   While my dataset (CommonsenseQA) was fairly clean, I included basic text normalization steps to remove potential noise (e.g., HTML entities) as a precaution in other stages of the pipeline.\n",
    "\n",
    "5. **Truncation**  \n",
    "   ✅ *Used*  \n",
    "   To fit input sequences into BERT's maximum input size constraint, I applied truncation during tokenization. This ensures that long question-choice pairs are trimmed to 128 tokens, which balances performance and memory usage.\n",
    "\n",
    "6. **Feature Selection**  \n",
    "   ✅ *Used implicitly*  \n",
    "   Rather than traditional feature engineering, I relied on the tokenized outputs (`input_ids`, `attention_mask`, `token_type_ids`) generated by the tokenizer. These features are optimized for transformer models and encapsulate the essential linguistic structure needed for training.\n",
    "\n",
    "By tailoring preprocessing to suit BERT’s architecture, I avoided redundant or harmful steps while retaining the ones critical for accurate and efficient model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bed1da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76239e3c",
   "metadata": {},
   "source": [
    "The BertTokenizer is a class from the Hugging Face `transformers` library that handles the conversion of raw text into tokens that BERT can understand. Specifically, it tokenizes the input text into subword tokens (e.g., \"playing\" becomes [\"play\", \"##ing\"]). This subword tokenization allows the model to process both common and out-of-vocabulary words more effectively. The `from_pretrained('bert-base-cased')` method loads a pre-trained tokenizer that corresponds to the BERT model. The `'bert-base-cased'` model refers to a base-sized BERT model (with 12 layers and 768 hidden units) that has been trained on cased text, meaning it differentiates between uppercase and lowercase letters which is important for distinguishing meaning in proper nouns or acronyms (e.g., “US” vs “us”).\n",
    "\n",
    "**Why I use BertTokenizer:** I use the BertTokenizer to ensure that the text is processed in the exact way BERT was originally trained. \n",
    "\n",
    "**The tokenizer will:** \n",
    "- Split the text into subword tokens. \n",
    "- Add special tokens such as `[CLS]` and `[SEP]` that BERT requires. \n",
    "- Handle padding and truncation to ensure the input is the correct length for the model.\n",
    "\n",
    "I use the following line of code to initialize the BERT tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18589408",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0449f7",
   "metadata": {},
   "source": [
    "The `preprocess_commonsenseqa` function is designed to preprocess the CommonsenseQA dataset for input into a BERT-based model. The goal is to tokenize the questions and their corresponding multiple-choice answers into a format compatible with BERT, and then convert the correct answer's label into a numerical value.\n",
    "\n",
    "1. **Extracting Questions and Choices:** The function starts by extracting the questions and their multiple-choice options from the `examples` object.\n",
    "\n",
    "2. **Initialize Data Structures:** It then initializes empty lists to hold the tokenized inputs, attention masks, and token type IDs (used for differentiating between sentence pairs in models like BERT).\n",
    "\n",
    "3. **Convert Answer Labels to Indices:** The answer choices are labeled with letters (A, B, C, D, E), but the model requires numerical labels. This step converts the letters into indices (A → 0, B → 1, etc.).\n",
    "\n",
    "4. **Processing Each Question-Choice Pair:** For each question and its corresponding choices: Each choice is paired with the question. Both the question and the choice are tokenized using the BERT tokenizer (`tokenizer_bert`), which converts the text into `input_ids`, `attention_mask` and `token_type_ids` tensors that BERT can understand.\n",
    "\n",
    "5. **Stacking Tokens for Each Choice:** After tokenizing each choice for a question, the function stacks the resulting tensors (for all choices) into single tensors for input to the model.\n",
    "\n",
    "6. **Returning Tokenized Data:** Finally, the function returns a dictionary containing the tokenized inputs (`input_ids`, `attention_mask` and `token_type_ids`) along with the numerical labels corresponding to the correct answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd469731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_commonsenseqa(examples):\n",
    "\n",
    "    questions = [q for q in examples['question']]\n",
    "    \n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_token_type_ids = []\n",
    "    \n",
    "    answerkeys = examples['answerKey']\n",
    "    labels = []\n",
    "    \n",
    "    for key in answerkeys:\n",
    "        labels.append(ord(key) - ord('A'))\n",
    "    \n",
    "    for i, (question, choices) in enumerate(zip(questions, examples['choices'])):\n",
    "        inputs = []\n",
    "\n",
    "        for choice in choices['text']:\n",
    "            text_a = question\n",
    "            text_b = choice\n",
    "            \n",
    "            encoded = tokenizer_bert(\n",
    "                text_a, text_b,\n",
    "                add_special_tokens=True,\n",
    "                max_length=128,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            inputs.append({\n",
    "                'input_ids': encoded['input_ids'],\n",
    "                'attention_mask': encoded['attention_mask'],\n",
    "                'token_type_ids': encoded['token_type_ids']\n",
    "            })\n",
    "        \n",
    "        input_ids = torch.cat([x['input_ids'] for x in inputs])\n",
    "        attention_mask = torch.cat([x['attention_mask'] for x in inputs])\n",
    "        token_type_ids = torch.cat([x['token_type_ids'] for x in inputs])\n",
    "        \n",
    "        all_input_ids.append(input_ids)\n",
    "        all_attention_mask.append(attention_mask)\n",
    "        all_token_type_ids.append(token_type_ids)\n",
    "\n",
    "    return {\n",
    "        'input_ids': all_input_ids,\n",
    "        'attention_mask': all_attention_mask,\n",
    "        'token_type_ids': all_token_type_ids,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01af1037",
   "metadata": {},
   "source": [
    "Next, I apply the preprocessing to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49eb21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = preprocess_commonsenseqa(train)\n",
    "validation_dataset = preprocess_commonsenseqa(valid)\n",
    "test_dataset = preprocess_commonsenseqa(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b832460a",
   "metadata": {},
   "source": [
    "After preprocessing the CommonsenseQA dataset into tokenized inputs and labels, I convert the data into PyTorch `TensorDataset` objects. I do this because this groups all of the input tensors, so they can be iterated over together. It seamlessly integrates with PyTorch’s `DataLoader` for tasks like batching, shuffling and parallel data loading, ensuring that the data pipeline runs efficiently. Additionally, it provides synchronized indexing, ensuring that each input tensor corresponds correctly to its label, making the dataset ready for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9181ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = TensorDataset(\n",
    "    torch.stack(train_dataset['input_ids']),\n",
    "    torch.stack(train_dataset['attention_mask']),\n",
    "    torch.stack(train_dataset['token_type_ids']),\n",
    "    torch.tensor(train_dataset['labels'])\n",
    ")\n",
    "\n",
    "val_features = TensorDataset(\n",
    "    torch.stack(validation_dataset['input_ids']),\n",
    "    torch.stack(validation_dataset['attention_mask']),\n",
    "    torch.stack(validation_dataset['token_type_ids']),\n",
    "    torch.tensor(validation_dataset['labels'])\n",
    ")\n",
    "\n",
    "test_features = TensorDataset(\n",
    "    torch.stack(test_dataset['input_ids']),\n",
    "    torch.stack(test_dataset['attention_mask']),\n",
    "    torch.stack(test_dataset['token_type_ids']),\n",
    "    torch.tensor(test_dataset['labels'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20912123",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2f857d",
   "metadata": {},
   "source": [
    "For tokenizing my LLM I used the AutoTokenizer from Hugging Face to load the tokenizer for the DeepSeek-V2-Lite model. This tokenizer is specifically designed to be compatible with the DeepSeek model architecture. Since DeepSeek is a large language model (LLM), it expects input in a specific tokenized format, including proper handling of special tokens, padding and prompt formatting. By using the `AutoTokenizer` and loading the tokenizer directly from the model’s Hugging Face repository, I ensure that the text is processed exactly as the model was trained on.\n",
    "\n",
    "The `trust_remote_code=True` argument is necessary because DeepSeek uses custom model/tokenizer code not yet fully integrated into the standard Transformers library. This option allows the tokenizer to load correctly and function as intended.\n",
    "\n",
    "In short, I use this tokenizer to guarantee consistency between my input prompts and the expectations of the DeepSeek model, which is crucial for generating accurate and meaningful responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ffc6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_deepseek = AutoTokenizer.from_pretrained('deepseek-ai/DeepSeek-V2-Lite', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399fe1e",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04cdf8d",
   "metadata": {},
   "source": [
    "To see the number of parameters for my models a bit better, I first implemented a function which adds an apostrophe after every three digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29092cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_number(num):\n",
    "    return f\"{num:,}\".replace(\",\", \"'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bfa7cb",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f5dbfa",
   "metadata": {},
   "source": [
    "For the random initialized and the pretrained transformer architechture, I used BERT (Bidirectional Encoder Representations from Transformers) with a classification head specifically designed for multiple-choice inputs called `BertForMultipleChoice`. I used the Hugging Face checkpoint `bert-base-cased`, which is a pretrained transformer model developed by Google with **108'311'041 parameters**. This variant is trained on large English corpora (BooksCorpus and English Wikipedia) and maintains case sensitivity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b8d948",
   "metadata": {},
   "source": [
    "BERT is well-suited for classification tasks like CommonsenseQA due to its deep bidirectional attention, which helps capture the nuanced relationships between the question and each answer option. The pretrained bert-base-cased weights provide strong language understanding out of the box, significantly improving performance over training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f0c030",
   "metadata": {},
   "source": [
    "The BERT model is composed of an embedding layer, encoder, pooling layer, dropout layer and of course a classifier:\n",
    "\n",
    "1. **Embedding Layer (`BertEmbeddings`):**\n",
    "- Word Embeddings: `Embedding(28996, 768)` → Maps each token to a 768-dimensional vector. The vocabulary size is 28,996 tokens.\n",
    "- Position Embeddings: `Embedding(512, 768)` → Adds position information to each token, allowing the model to distinguish word order up to 512 tokens.\n",
    "- Token Type Embeddings: `Embedding(2, 768)` → Distinguishes between sentence pairs (e.g., question vs. answer).\n",
    "- Layer Normalization + Dropout: Normalizes embeddings and applies dropout (`p=0.1`) for regularization.\n",
    "\n",
    "2. **Encoder (`BertEncoder`):**\n",
    "- 12 Transformer Layers (stacked) → Each layer includes:\n",
    "    - Multi-Head Self-Attention (BertSelfAttention)\n",
    "        - Projects inputs into queries, keys and values using linear layers.\n",
    "        - Attention mechanism allows each token to attend to all others.\n",
    "        - Output passed through a linear layer, then dropout + layer norm.\n",
    "    - Feed-Forward Network\n",
    "        - First Linear: `768 → 3072`\n",
    "        - GELU activation\n",
    "        - Second Linear: `3072 → 768`\n",
    "        - Followed by LayerNorm and Dropout.\n",
    "- Each of these layers processes the tokenized question-choice pair, allowing the model to capture deep contextual relationships.\n",
    "\n",
    "3. **Pooling Layer (`BertPooler`):**\n",
    "- Extracts the `[CLS]` token output from the final encoder layer.\n",
    "- Applies a linear layer + `tanh` activation to produce a fixed-size sentence representation.\n",
    "\n",
    "4. **Dropout Layer:**\n",
    "- Applied before classification to reduce overfitting (`p=0.1`).\n",
    "\n",
    "5. **Classifier (`Linear(768 → 1)`):**\n",
    "- For each choice, outputs a single logit.\n",
    "- During training/evaluation, logits for all choices are grouped and passed through softmax to compute the predicted answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6b1756",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361fb529",
   "metadata": {},
   "source": [
    "In the following code block I load the configuration of the `'bert-base-cased'` model, but without the model weights. I just load the architecture details like number of layers, hidden size and so on. After I create a `BertForMultipleChoice` model using that configuration. This model is randomly initialized, meaining it hasn't learned anything yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eac71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_bert = BertConfig.from_pretrained('bert-base-cased')\n",
    "random_bert_model = BertForMultipleChoice(config_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2012b6e5",
   "metadata": {},
   "source": [
    "Next, I calculate and print the total number of parameters of the `random_bert_model` and then print its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7d9b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of parameters: {format_number(sum(p.numel() for p in random_bert_model.parameters()))}\\n\")\n",
    "print(random_bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec07ade",
   "metadata": {},
   "source": [
    "In the next cell I create a `BertForMultipleChoice` model with already pretrained weights. These pretrained weights allow my model to already understand some word meanings, grammar and general language patterns. This helps the model perform better and converge faster when fine-tuned on my specific downstream task, such as multiple-choice question answering with CommonsenseQA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_bert_model = BertForMultipleChoice.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c4b83",
   "metadata": {},
   "source": [
    "I calculate and print the total number of parameters of the `pretrained_bert_model` and then print its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f91a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of parameters: {format_number(sum(p.numel() for p in pretrained_bert_model.parameters()))}\\n\")\n",
    "print(pretrained_bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac8c42a",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febbc8ca",
   "metadata": {},
   "source": [
    "The DeepSeek-V2-Lite is a cutting-edge decoder-only transformer model optimized for causal language modeling, so for generating or completing text. With **15'706'484'224 parameters**, it is orders of magnitude larger than BERT and is specifically designed for autoregressive generation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a1b9b",
   "metadata": {},
   "source": [
    "For the DeepSeek-V2-Lite model, the architecture is composed of an embedding layer, a stack of decoder layers (transformer blocks), normalization layers and a final language modeling head for token prediction:\n",
    "\n",
    "1. **Token Embedding Layer:**\n",
    "- `Embedding(102400, 2048)` → Maps tokens from a very large vocabulary (102,400 tokens) to 2048-dimensional embeddings.\n",
    "\n",
    "2. **Stack of 27 Decoder Layers (`DeepseekV2DecoderLayer`):**\n",
    "- Each decoder layer includes:\n",
    "    - Self-Attention Mechanism (`DeepseekV2Attention`)\n",
    "        - Query projection: `Linear(2048 → 3072)`\n",
    "        - KV projections:\n",
    "            - `kv_a_proj_with_mqa`: `Linear(2048 → 576)` — Multi-query attention (MQA), a memory-efficient variant.\n",
    "            - `kv_b_proj`: `Linear(512 → 4096)` — Advanced attention processing.\n",
    "        - RMSNorm on KV inputs: Normalizes activations to improve stability.\n",
    "        - Rotary Embeddings (`DeepseekV2YarnRotaryEmbedding`) → Positional encoding mechanism that enables extrapolation to longer sequences.\n",
    "        - Output projection: `Linear(2048 → 2048)`\n",
    "    - Feed-Forward Layer\n",
    "        - Layer 0 uses:\n",
    "            - Standard MLP (`DeepseekV2MLP`)\n",
    "                - `gate_proj`, `up_proj`: `2048 → 10944`\n",
    "                - `down_proj`: `10944 → 2048`\n",
    "                - Activation: SiLU (a smooth, non-monotonic function similar to Swish)\n",
    "        - Layers 1–26 use:\n",
    "            - Mixture-of-Experts (MoE) Layer (`DeepseekV2MoE`)\n",
    "                - 64 expert MLPs (`DeepseekV2MLP`) with `2048 → 1408 → 2048`\n",
    "                - Gating mechanism: `MoEGate()` dynamically selects top-k experts per token.\n",
    "                - Shared expert also included: `2048 → 2816 → 2048`\n",
    "                - This makes computation sparse but increases capacity massively.\n",
    "    - Normalization\n",
    "        - RMSNorm instead of LayerNorm, used both before attention and before MLP. RMSNorm scales activations based on root mean square, which is more numerically stable in large-scale training.\n",
    "\n",
    "3. **Final LayerNorm + Output Head:**\n",
    "- Final RMSNorm applied to the output of the last decoder layer.\n",
    "- LM Head: `Linear(2048 → 102400)`\n",
    "    - Maps model outputs back into the token vocabulary for prediction.\n",
    "    - Weight sharing is likely applied with the token embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d01066",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82674b6c",
   "metadata": {},
   "source": [
    "With the following code cell I load the `DeepSeek-V2-Lite` large language model (https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite). This model is designed for causal language modeling tasks such as text generation. The loading configuration includes:\n",
    "- `AutoModelForCausalLM.from_pretrained(...)`: Loads the pretrained DeepSeek model.\n",
    "- `trust_remote_code=True`: Allows loading custom model code from the model's repository.\n",
    "- `torch_dtype=torch.bfloat16`: Uses the more efficient `bfloat16` precision for faster inference.\n",
    "- `device_map=\"cpu\"`: Loads the model onto the CPU.\n",
    "- `GenerationConfig.from_pretrained(...)`: Loads the model's default generation configuration (e.g., max tokens, sampling strategy).\n",
    "- `pad_token_id = eos_token_id`: Sets the padding token to be the same as the end-of-sequence token for compatibility during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d6325",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_model = AutoModelForCausalLM.from_pretrained('deepseek-ai/DeepSeek-V2-Lite', trust_remote_code=True, torch_dtype=torch.bfloat16, device_map=\"cpu\", low_cpu_mem_usage=True)\n",
    "deepseek_model.generation_config = GenerationConfig.from_pretrained('deepseek-ai/DeepSeek-V2-Lite')\n",
    "deepseek_model.generation_config.pad_token_id = deepseek_model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b70bd",
   "metadata": {},
   "source": [
    "I calculate and print the total number of parameters of the `deepseek_model` and then print its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96093558",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of parameters: {format_number(sum(p.numel() for p in deepseek_model.parameters()))}\\n\")\n",
    "print(deepseek_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaf3928",
   "metadata": {},
   "source": [
    "The function `process_commonsense_qa_for_deepseek(...)` in the next code cell is designed to process and evaluate the performance from the LLM on the CommonsenseQA dataset. Rather than using the dataset in a traditional classification setup (with logits over classes), this function reformulates each question into a text prompt that simulates a real-world use case: prompting a language model to directly generate the correct answer letter (e.g., \"A\", \"B\", \"C\"). This approach enables zero-shot evaluation of decoder-based language models (like DeepSeek, GPT-style models, etc.) by leveraging natural language prompts.\n",
    "\n",
    "Key features of the function include:\n",
    "- Prompt Construction: Each question is converted into a formatted prompt including the question, concept (if available) and multiple choice options labeled A–E.\n",
    "- Text Generation: The model is prompted to generate a single token representing the answer choice, using `max_new_tokens=1` to constrain output.\n",
    "- Answer Decoding: The raw model output is post-processed to extract the predicted answer letter.\n",
    "- Result Logging: The function collects questions, prompts, generated answers, and ground truth labels for easy evaluation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05105fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_commonsense_qa_for_deepseek(dataset, model, tokenizer, num_examples=5, max_new_tokens=1):\n",
    "    results = {\n",
    "        'questions': [],\n",
    "        'prompts': [],\n",
    "        'responses': [],\n",
    "        'correct_answers': []\n",
    "    }\n",
    "    \n",
    "    if num_examples is not None:\n",
    "        limited_dataset = dataset.select(range(min(num_examples, len(dataset))))\n",
    "    else:\n",
    "        limited_dataset = dataset\n",
    "    \n",
    "    has_question_concept = 'question_concept' in limited_dataset.column_names\n",
    "    \n",
    "    for i in range(len(limited_dataset)):\n",
    "        question = limited_dataset['question'][i]\n",
    "        \n",
    "        question_concept = limited_dataset['question_concept'][i] if has_question_concept else ''\n",
    "        \n",
    "        choices = limited_dataset['choices'][i]\n",
    "        choice_labels = choices['label']\n",
    "        choice_texts = choices['text']\n",
    "        choices_text = \", \".join([f\"{label}. {text}\" for label, text in zip(choice_labels, choice_texts)])\n",
    "        \n",
    "        answer_key = limited_dataset['answerKey'][i] if 'answerKey' in limited_dataset.column_names else None\n",
    "        \n",
    "        prompt = (\n",
    "            f\"Answer the following multiple choice question with only the correct letter (A, B, C, D, or E). Do not explain your answer.\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            f\"Concept: {question_concept}\\n\"\n",
    "            f\"Choices: {choices_text}\\n\"\n",
    "            f\"Answer:\"\n",
    "        )\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Move inputs to the model's device\n",
    "        \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "            \n",
    "            output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            generated_text = output_text[len(prompt):].strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response for example {i}: {e}\")\n",
    "            generated_text = f\"Error: {str(e)}\"\n",
    "\n",
    "        results['questions'].append(question)\n",
    "        results['prompts'].append(prompt)\n",
    "        results['responses'].append(generated_text)\n",
    "        results['correct_answers'].append(answer_key if answer_key else \"N/A\")\n",
    "\n",
    "        del inputs, outputs\n",
    "        gc.collect()\n",
    "\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41073d29",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596c651e",
   "metadata": {},
   "source": [
    "First, I define my GPU as the device to increase the speed of my training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4add2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183cd80",
   "metadata": {},
   "source": [
    "Next, I define all the needed hyperparameters for my <u>manual</u> training. Heres a description for all of the hyperparameters used:\n",
    "\n",
    "**epochs:** Number of times the model goes through the entire training dataset.<br>\n",
    "**learning_rate:** Controls how much the model updates its weights with each step.<br>\n",
    "**batch_size:** Number of samples processed together before updating model weights.<br>\n",
    "**warmup_steps:** Gradually increases learning rate over the first training steps to avoid instability.<br>\n",
    "**gradient_clip_val:** Limits the maximum gradient norm to prevent exploding gradients.<br>\n",
    "**save_interval:** Saves the model after every epoch.<br>\n",
    "**gradient_accumulation_steps:** Accumulates gradients over multiple steps before updating weights, effectively increasing the batch size.<br>\n",
    "**patience:** Stops training early if validation performance doesn’t improve for the set amount of epochs.<br>\n",
    "**weight_decay:** Applies L2 regularization to discourage overly large weights and reduce overfitting.<br>\n",
    "\n",
    "For the warmup steps I found, that the value is often set to 5-10% of the total training steps (https://medium.com/better-ml/the-art-of-setting-learning-rate-eff11ac0a737). To get for example the 10%, I would use this code:  `warmup_steps = 0.1 * len(train_dataloader)`. Due to the fact, that I wanted to have all my hyperparameters in one block (including the batch size, which is needed for the dataloader after) I had to find a different way to calculate those 10%. A different way to calculate this would be: `warmup_steps = int(0.1 * len(train_features) / batch_size)`. With this approach I could create the dataloaders later, while having all necessary hyperparameters in one block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b06ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "learning_rate = 1e-5\n",
    "batch_size = 16\n",
    "warmup_steps = int(0.1 * len(train_features) / batch_size) # 0.1 = 10% of training data\n",
    "gradient_clip_val = 5.0\n",
    "save_interval = 1\n",
    "gradient_accumulation_steps = 4 # Effectively creates a batch size of 128 (batch_size * gradient_accumulation_steps)\n",
    "patience = 3\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351cbc22",
   "metadata": {},
   "source": [
    "In the next code cell, I define the data loaders for training, validation and testing. A DataLoader is responsible for efficiently loading batches of data during training or evaluation. Here's a breakdown of the configuration used for each dataset:\n",
    "- `train_features` / `val_features` / `test_features`: These are the preprocessed datasets for training, validation, and testing, respectively.\n",
    "- `batch_size`: Controls how many samples are passed through the model at once; defined earlier to ensure consistency.\n",
    "- `shuffle`:\n",
    "    - Set to `True` for the training set to ensure that the model sees a different order of examples each epoch (helps generalization).\n",
    "    - Set to `False` for validation and test sets to maintain deterministic behavior (important for consistent evaluation).\n",
    "- `num_workers=4`: Enables parallel data loading using 4 subprocesses. This speeds up data fetching, especially when I/O or preprocessing is involved.\n",
    "- `pin_memory=True`: Allows faster transfer of data from CPU to GPU by allocating the data in page-locked (pinned) memory — useful when training on a CUDA-enabled device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff26a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_features, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_features, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_features, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1dee63",
   "metadata": {},
   "source": [
    "As stated in the markdown for the hyperparameters, I used a bit of a different calculation to declare the warmup steps. In the following code block, I look at the output value of both calculations to make sure that they're the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef952a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(warmup_steps)\n",
    "print(int(0.1 * len(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c0e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(model, train_dataloader, val_dataloader, device, \n",
    "                      epochs=10, learning_rate=1e-4, warmup_steps=None,\n",
    "                      log_wandb=True, gradient_clip_val=5.0, save_interval=1,\n",
    "                      gradient_accumulation_steps=4, patience=3,\n",
    "                      weight_decay=0.001, save_path=None):\n",
    "        \n",
    "    # Handle save path for checkpoints\n",
    "    if save_path:\n",
    "        # Make sure parent directory exists\n",
    "        checkpoint_dir = save_path\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        # Define best model path inside the checkpoint directory\n",
    "        best_model_path = os.path.join(checkpoint_dir, \"best_transformer_model.pt\")\n",
    "    else:\n",
    "        print(\"Warning: No save_path provided. Model and checkpoints will not be saved.\")\n",
    "        checkpoint_dir = None\n",
    "        best_model_path = None\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Enable gradient checkpointing for memory efficiency\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    # Initialize wandb logging if enabled\n",
    "    if log_wandb:\n",
    "        import wandb\n",
    "        # Check if wandb is initialized, if not initialize it\n",
    "        if not wandb.run:\n",
    "            wandb.init(\n",
    "                project=\"CommonsenseQA\",\n",
    "                name=f\"pretrained_transformer-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\", # Change the name to the current transformer model\n",
    "                config={\n",
    "                    \"learning_rate\": learning_rate,\n",
    "                    \"epochs\": epochs,\n",
    "                    \"batch_size\": train_dataloader.batch_size,\n",
    "                    \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "                    \"effective_batch_size\": train_dataloader.batch_size * gradient_accumulation_steps,\n",
    "                    \"weight_decay\": weight_decay,\n",
    "                    \"warmup_steps\": warmup_steps,\n",
    "                    \"gradient_clip_val\": gradient_clip_val,\n",
    "                    \"model_type\": model.__class__.__name__,\n",
    "                      })\n",
    "    \n",
    "    # Initialize CrossEntropy Loss\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Calculate total training steps\n",
    "    total_steps = len(train_dataloader) * epochs // gradient_accumulation_steps\n",
    "    \n",
    "    # Set default warmup steps if not provided\n",
    "    if warmup_steps is None:\n",
    "        warmup_steps = len(train_dataloader)  # One epoch of warmup\n",
    "    \n",
    "    # Initialize scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=warmup_steps, \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Variables to track best model\n",
    "    best_accuracy = 0.0\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_correct = 0\n",
    "        epoch_total = 0\n",
    "        progress_bar = tqdm(train_dataloader, desc=\"Training\")\n",
    "        optimizer.zero_grad()  # Zero gradients once at the beginning of epoch\n",
    "        \n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            # Move batch to device\n",
    "            input_ids, attention_mask, token_type_ids, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            \n",
    "            # Use CrossEntropy Loss\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            epoch_correct += (preds == labels).sum().item()\n",
    "            epoch_total += labels.size(0)\n",
    "            \n",
    "            # Scale loss for gradient accumulation\n",
    "            loss_to_backward = loss / gradient_accumulation_steps\n",
    "            \n",
    "            # Backward pass\n",
    "            loss_to_backward.backward()\n",
    "            \n",
    "            # Update weights every gradient_accumulation_steps batches\n",
    "            if (i + 1) % gradient_accumulation_steps == 0 or (i + 1) == len(train_dataloader):\n",
    "                # Clip gradients using the provided gradient_clip_val parameter\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_val)\n",
    "                \n",
    "                # Update weights\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Log learning rate\n",
    "                if log_wandb:\n",
    "                    wandb.log({\"learning_rate\": scheduler.get_last_lr()[0]})\n",
    "            \n",
    "            # Update progress bar (use the unscaled loss for display)\n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "            \n",
    "            # Accumulate loss (use the unscaled loss for logging)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Calculate average loss and accuracy for the epoch\n",
    "        avg_train_loss = epoch_loss / len(train_dataloader)\n",
    "        train_accuracy = epoch_correct / epoch_total\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Training loss: {avg_train_loss:.4f}, accuracy: {train_accuracy:.4f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        # No gradient computation for validation\n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(val_dataloader, desc=\"Validation\")\n",
    "            \n",
    "            for batch in progress_bar:\n",
    "                # Move batch to device\n",
    "                input_ids, attention_mask, token_type_ids, labels = [b.to(device) for b in batch]\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids\n",
    "                )\n",
    "                logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "                \n",
    "                # Use CrossEntropy Loss\n",
    "                criterion = torch.nn.CrossEntropyLoss()\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Get predictions\n",
    "                _, preds = torch.max(logits, dim=1)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\"acc\": correct/total})\n",
    "            \n",
    "        # Calculate validation metrics\n",
    "        val_accuracy = correct / total\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        print(f\"Validation loss: {avg_val_loss:.4f}, accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Log metrics to wandb\n",
    "        if log_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"train_accuracy\": train_accuracy,\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"val_accuracy\": val_accuracy,\n",
    "                \"learning_rate\": scheduler.get_last_lr()[0]\n",
    "            })\n",
    "        \n",
    "        # Save checkpoint at specified interval\n",
    "        if checkpoint_dir and (epoch + 1) % save_interval == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            \n",
    "            # Create checkpoint with additional information\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model_to_save.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'train_losses': train_losses,\n",
    "                'val_accuracies': val_accuracies\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if best_model_path and val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            # Save model\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            torch.save(model_to_save.state_dict(), best_model_path)\n",
    "            print(f\"Best model saved to {best_model_path}\")\n",
    "            \n",
    "            # Also save as checkpoint with additional metadata\n",
    "            best_checkpoint_path = os.path.join(checkpoint_dir, f\"best_checkpoint_epoch_{epoch+1}.pt\")\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model_to_save.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'train_losses': train_losses,\n",
    "                'val_accuracies': val_accuracies\n",
    "            }\n",
    "            torch.save(checkpoint, best_checkpoint_path)\n",
    "            \n",
    "            # Log best model to wandb\n",
    "            if log_wandb:\n",
    "                wandb.run.summary[\"best_accuracy\"] = best_accuracy\n",
    "                wandb.run.summary[\"best_epoch\"] = epoch + 1\n",
    "            \n",
    "            # Reset early stopping counter\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            # Increment early stopping counter\n",
    "            early_stopping_counter += 1\n",
    "            print(f\"No improvement for {early_stopping_counter} epochs\")\n",
    "            \n",
    "            # Check if we should stop early\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(f\"Early stopping after {epoch+1} epochs\")\n",
    "                if log_wandb:\n",
    "                    wandb.run.summary[\"stopped_epoch\"] = epoch + 1\n",
    "                break\n",
    "    \n",
    "    # Load best model if it was saved\n",
    "    if best_model_path and os.path.exists(best_model_path):\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        print(f\"Loaded best model from {best_model_path}\")\n",
    "    \n",
    "    # Finish wandb run\n",
    "    if log_wandb:\n",
    "        wandb.finish()\n",
    "    \n",
    "    return model, train_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a50a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_pretrained_bert_model, trained_pretrained_bert_train_losses, trained_pretrained_bert_val_accuracies = train_transformer(\n",
    "    random_bert_model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    device,\n",
    "    epochs=epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_steps=warmup_steps,\n",
    "    gradient_clip_val=gradient_clip_val,\n",
    "    save_interval=save_interval,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    patience=patience,\n",
    "    weight_decay=weight_decay,\n",
    "    save_path=f\"./checkpoints/random_transformer-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103eedf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_random_bert_model, trained_random_bert_train_losses, trained_random_bert_val_accuracies = train_transformer(\n",
    "    pretrained_bert_model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    device,\n",
    "    epochs=epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_steps=warmup_steps,\n",
    "    gradient_clip_val=gradient_clip_val,\n",
    "    save_interval=save_interval,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    patience=patience,\n",
    "    weight_decay=weight_decay,\n",
    "    save_path=f\"./checkpoints/pretrained_transformer-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158629d",
   "metadata": {},
   "source": [
    "### Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716b5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def objective_function_pretrained(config=None):\n",
    "    \"\"\"Objective function for hyperparameter optimization of pretrained model\"\"\"\n",
    "    model_type = \"pretrained_transformer\"\n",
    "    run_name = f\"{model_type}-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "    \n",
    "    with wandb.init(project=\"CommonsenseQA\", name=run_name) as run:\n",
    "        config = wandb.config\n",
    "        \n",
    "        # Create dataloaders with the batch size from config\n",
    "        train_batch_size = config.batch_size\n",
    "        \n",
    "        train_dataloader = DataLoader(train_features,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_dataloader = DataLoader(\n",
    "            val_features,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Use the training function with model_pretrained\n",
    "        trained_pretrained_bert_model, trained_pretrained_bert_train_losses, trained_pretrained_bert_val_accuracies = train_transformer(\n",
    "            model=pretrained_bert_model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            device=device,\n",
    "            epochs=config.epochs,\n",
    "            learning_rate=config.learning_rate,\n",
    "            warmup_steps=config.warmup_ratio * len(train_dataloader),\n",
    "            gradient_clip_val=config.gradient_clip_val,\n",
    "            save_interval=1,\n",
    "            gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "            patience=config.patience,\n",
    "            weight_decay=config.weight_decay,\n",
    "            save_path=f\"./checkpoints/pretrained-sweep-{wandb.run.id}-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        )\n",
    "        \n",
    "        # Return the best validation accuracy\n",
    "        return max(trained_pretrained_bert_val_accuracies)\n",
    "\n",
    "def objective_function_random(config=None):\n",
    "    model_type = \"random_transformer\"\n",
    "    run_name = f\"{model_type}-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "    \n",
    "    with wandb.init(project=\"CommonsenseQA\", name=run_name) as run:\n",
    "        config = wandb.config\n",
    "        \n",
    "        # Create dataloaders with the batch size from config\n",
    "        train_batch_size = config.batch_size\n",
    "        \n",
    "        train_dataloader = DataLoader(train_features,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_dataloader = DataLoader(\n",
    "            val_features,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Use the training function with model_random\n",
    "        trained_random_bert_model, trained_random_bert_train_losses, trained_random_bert_val_accuracies = train_transformer(\n",
    "            model=random_bert_model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            device=device,\n",
    "            epochs=config.epochs,\n",
    "            learning_rate=config.learning_rate,\n",
    "            warmup_steps=config.warmup_ratio * len(train_dataloader),\n",
    "            gradient_clip_val=config.gradient_clip_val,\n",
    "            save_interval=1,\n",
    "            gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "            patience=config.patience,\n",
    "            weight_decay=config.weight_decay,\n",
    "            save_path=f\"./checkpoints/random-sweep-{wandb.run.id}-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        )\n",
    "        \n",
    "        # Return the best validation accuracy\n",
    "        return max(trained_random_bert_val_accuracies)\n",
    "\n",
    "def run_sweep_pretrained(count):\n",
    "    \"\"\"Run hyperparameter sweep for the pretrained model\"\"\"\n",
    "    # Define the parameter search space\n",
    "    sweep_config = {\n",
    "        'method': 'bayes',\n",
    "        'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
    "        'name': 'pretrained_model_sweep',\n",
    "        'parameters': {\n",
    "            'batch_size': {\n",
    "                'values': [8, 16, 32, 64, 128]\n",
    "            },\n",
    "            'learning_rate': {\n",
    "                'values': [1e-6, 1e-5, 1e-4, 1e-3]\n",
    "            },\n",
    "            'weight_decay': {\n",
    "                'values': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "            },\n",
    "            'gradient_clip_val': {\n",
    "                'values': [0.5, 1.0, 5.0, 10.0]\n",
    "            },\n",
    "            'gradient_accumulation_steps': {\n",
    "                'values': [1, 2, 4, 8]\n",
    "            },\n",
    "            'warmup_ratio': {\n",
    "                'values': [0.0, 0.05, 0.1]\n",
    "            },\n",
    "            'epochs': {\n",
    "                'values': [50]\n",
    "            },\n",
    "            'patience': {\n",
    "                'values': [5, 10]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create the sweep\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"CommonsenseQA\")\n",
    "    \n",
    "    # Run the sweep\n",
    "    wandb.agent(sweep_id, function=objective_function_pretrained, count=count)\n",
    "\n",
    "def run_sweep_random(count):\n",
    "    \"\"\"Run hyperparameter sweep for the randomly initialized model\"\"\"\n",
    "    # Define the parameter search space\n",
    "    # For random initialization, we might want to explore different learning rates\n",
    "    sweep_config = {\n",
    "        'method': 'bayes',\n",
    "        'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
    "        'name': 'random_model_sweep',\n",
    "        'parameters': {\n",
    "            'batch_size': {\n",
    "                'values': [8, 16, 32, 64, 128]\n",
    "            },\n",
    "            'learning_rate': {\n",
    "                'values': [1e-6, 1e-5, 1e-4, 1e-3]\n",
    "            },\n",
    "            'weight_decay': {\n",
    "                'values': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "            },\n",
    "            'gradient_clip_val': {\n",
    "                'values': [0.5, 1.0, 5.0, 10.0]\n",
    "            },\n",
    "            'gradient_accumulation_steps': {\n",
    "                'values': [1, 2, 4, 8]\n",
    "            },\n",
    "            'warmup_ratio': {\n",
    "                'values': [0.0, 0.05, 0.1]\n",
    "            },\n",
    "            'epochs': {\n",
    "                'values': [50]\n",
    "            },\n",
    "            'patience': {\n",
    "                'values': [5, 10]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create the sweep\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"CommonsenseQA\")\n",
    "    \n",
    "    # Run the sweep\n",
    "    wandb.agent(sweep_id, function=objective_function_random, count=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting sweep for pretrained model...\")\n",
    "run_sweep_pretrained(count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b90ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting sweep for randomly initialized model...\")\n",
    "run_sweep_random(count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d85c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(random_losses, random_accuracies, pretrained_losses, pretrained_accuracies):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(random_losses, label='Random Init')\n",
    "    plt.plot(pretrained_losses, label='Pretrained')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(random_accuracies, label='Random Init')\n",
    "    plt.plot(pretrained_accuracies, label='Pretrained')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c47992",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curves(trained_random_bert_train_losses, trained_random_bert_val_accuracies, trained_pretrained_bert_train_losses, trained_pretrained_bert_val_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61866a81",
   "metadata": {},
   "source": [
    "# **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8fe1f8",
   "metadata": {},
   "source": [
    "Important: Use test split for eval, not validation (& ofc no train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d4be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    all_logits = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Evaluation\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            input_ids, attention_mask, token_type_ids, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = outputs.logits\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            \n",
    "            # Collect labels and predictions\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(preds.cpu().numpy())\n",
    "            all_logits.append(logits.cpu().numpy())\n",
    "    \n",
    "    # Combine all logits\n",
    "    all_logits = np.vstack(all_logits) if all_logits else np.array([])\n",
    "    \n",
    "    # Compute metrics\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score, \n",
    "        precision_score, \n",
    "        recall_score, \n",
    "        f1_score, \n",
    "        confusion_matrix,\n",
    "        classification_report\n",
    "    )\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    \n",
    "    # Only calculate these metrics if there are predictions for each class\n",
    "    try:\n",
    "        precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "        recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "        f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    except:\n",
    "        print(\"Warning: Some classes may not have predictions. Using only accuracy.\")\n",
    "        precision = recall = f1 = None\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    \n",
    "    # Class-wise report \n",
    "    class_report = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "    \n",
    "    # Create a list to map index to answer choice label (A-E)\n",
    "    idx_to_label = {i: chr(65 + i) for i in range(5)}  # 0->A, 1->B, etc.\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    class_accuracies = {}\n",
    "    for i in range(5):\n",
    "        class_indices = np.where(np.array(true_labels) == i)[0]\n",
    "        if len(class_indices) > 0:\n",
    "            class_correct = sum([predicted_labels[j] == i for j in class_indices])\n",
    "            class_accuracies[idx_to_label[i]] = class_correct / len(class_indices)\n",
    "        else:\n",
    "            class_accuracies[idx_to_label[i]] = 0\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'class_report': class_report,\n",
    "        'per_class_accuracy': class_accuracies,\n",
    "        'logits': all_logits\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Per-class accuracy:\")\n",
    "    for label, acc in class_accuracies.items():\n",
    "        print(f\"  Choice {label}: {acc:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2b1c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_model_results = evaluate(random_bert_model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bcd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_results = evaluate(pretrained_model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948de111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the validation set (1 example)\n",
    "results = process_commonsense_qa_for_deepseek(valid, deepseek_model, tokenizer_deepseek, num_examples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b95e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results['questions'])\n",
    "print(results['prompts'])\n",
    "print(results['responses'])\n",
    "print(results['correct_answers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83e495",
   "metadata": {},
   "source": [
    "# **Interpretation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363fea4",
   "metadata": {},
   "source": [
    "# **Tools used**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0bf1a1",
   "metadata": {},
   "source": [
    "### **Adjust this section before submitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8479f8",
   "metadata": {},
   "source": [
    "1. **Programming Environment**\n",
    "   - Python 3.12.8\n",
    "   - Jupyter Notebook\n",
    "\n",
    "2. **Machine Learning and Deep Learning**\n",
    "   - PyTorch (neural network development)\n",
    "   - Hugging Face Datasets (data management)\n",
    "   - NLTK (natural language preprocessing)\n",
    "   - FastText (pre-trained word embeddings, 300-dimensional vectors)\n",
    "\n",
    "3. **Data Manipulation and Analysis**\n",
    "   - NumPy (numerical computing)\n",
    "   - Pandas (data structuring and manipulation)\n",
    "   - Scikit-learn (potential additional machine learning utilities)\n",
    "\n",
    "4. **Visualization and Tracking**\n",
    "   - Matplotlib (basic plotting)\n",
    "   - Seaborn (statistical data visualization)\n",
    "   - Weights & Biases (experiment tracking and logging)\n",
    "     * Tracked metrics: training loss, accuracy, learning rates\n",
    "     * Logged hyperparameter configurations\n",
    "     * Enabled comparative analysis across model runs\n",
    "\n",
    "5. **Computational Infrastructure**\n",
    "   - CUDA-enabled GPU acceleration\n",
    "   - GPU-optimized PyTorch operations\n",
    "   - Efficient parallel computing for model training\n",
    "\n",
    "6. **Dataset and Benchmarking**\n",
    "   - CommonsenseQA dataset (Hugging Face)\n",
    "   - Standard benchmark for commonsense reasoning tasks\n",
    "\n",
    "7. **Additional Libraries**\n",
    "   - Gensim (word vector processing)\n",
    "   - tqdm (progress bar visualization)\n",
    "   - datetime (experiment timestamping)\n",
    "\n",
    "8. **AI-Tools**\n",
    "   - Claude 3.5 Sonnet: Utilized as a coding assistant for debugging, optimization and documentation.\n",
    "   - GPT-4-turbo: Assisted in drafting and refining documentation, helping with structure and phrasing.\n",
    "   - Copilot: Used for quick inserts, when recommendation was suitable for what I was planning to do.\n",
    "\n",
    "9. **Sources**\n",
    "   - Transformer architecture: https://medium.com/data-science/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\n",
    "   - Deepseek implementation: https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite\n",
    "   - Medium blog for warmup steps: https://medium.com/better-ml/the-art-of-setting-learning-rate-eff11ac0a737"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
