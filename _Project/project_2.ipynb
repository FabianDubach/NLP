{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea5cc4f4",
   "metadata": {},
   "source": [
    "# **FS25 NLP Project 2: Transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52707bad",
   "metadata": {},
   "source": [
    "Fabian Dubach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee79ba7",
   "metadata": {},
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df05906a",
   "metadata": {},
   "source": [
    "<style>\n",
    "  .container {\n",
    "    display: flex;\n",
    "    align-items: flex-start;\n",
    "    gap: 20px; /* spacing between text and ASCII art */\n",
    "    font-family: monospace;\n",
    "  }\n",
    "  .text {\n",
    "    flex: 2;\n",
    "  }\n",
    "  .ascii {\n",
    "    white-space: pre;\n",
    "    font-size: 4.5px;\n",
    "    line-height: 1.2;\n",
    "    flex: 1;\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<div class=\"container\">\n",
    "  <div class=\"text\">\n",
    "    <p>The task for my project was to perform common sense question answering using the CommonsenseQA dataset, which is a multiple-choice question answering dataset that contains 12'247 different questions and was developed to benchmark machine understanding of everyday knowledge. For each questions there are 5 given answer choices, where only one of them is correct. To be able to answer these questions, \"commonsense\" is needed. The dataset is available on HuggingFace: <a href=\"https://huggingface.co/datasets/tau/commonsense_qa\" target=\"_blank\">https://huggingface.co/datasets/tau/commonsense_qa</a>.</p><br>\n",
    "    <p>I evaluated the performance of three different Transformer-based models:</p>\n",
    "    <p>1. A randomly initialized Transformer</p>\n",
    "    <p>2. A pretrained Transformer (with the same architecture as the first Transformer)</p>\n",
    "    <p>3. A large language model (LLM) with over 1 billion parameters</p><br>\n",
    "    <p>While the first two models were finetuned on the dataset using the same hyperparameters for a fair comparison, the LLM was evaluated through prompt engineering without additional training. This setup allowed me to explore how different levels of pretraining and model scale impact common sense reasoning performance.</p>\n",
    "    <p>We had to also track the trainings with Wandb (workspace URL: <a href=\"https://wandb.ai/fabian-dubach-hochschule-luzern/CommonsenseQA/workspace?nw=nwuserfabiandubach\" target=\"_blank\">https://wandb.ai/fabian-dubach-hochschule-luzern/CommonsenseQA/workspace?nw=nwuserfabiandubach</a>).</p>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"ascii\">\n",
    "<pre>\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣶⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⠀⠀⠀⠀⠀⣤⣤⣤⠀⠀⠀⠀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⠀⣠⡶⢿⡇⢿⣿⡏⢳⣦⠀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⡛⣆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣧⣼⣿⣴⣋⡽⠮⠿⢭⣟⣏⣷⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⣧⠘⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡼⣇⣿⡿⠶⣶⣿⣟⡛⣷⣿⢠⠙⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⡈⣏⠇⢹⡀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡟⢹⠁⣿⠋⠉⢹⠉⠙⣿⡇⣾⣀⣾⠀⢀⣤⡀⢀⡀⠀⠀⢀⣠⣴⣾⠛⢻⡛⢻⡄⢀⣳⡀⢀⣠⠄⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⣷⣾⢀⣿⡇⠀⠸⠀⠀⣿⣧⡽⠿⣟⣺⣭⠴⢿⡏⣩⣷⡾⢛⣭⣴⣿⣇⠘⣿⣷⣿⡛⠉⢻⣟⣷⠄⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠿⢿⣟⣿⣿⡦⣶⣪⡭⠿⣚⣫⣭⣽⣶⡄⠀⢸⡇⣿⡙⣿⣿⣿⣿⣿⣿⣆⠹⣿⣿⣷⡀⠀⢿⡉⠁⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⣤⣶⣿⠿⠛⣉⣭⣶⣾⣿⠿⠟⠛⠉⠉⢻⠀⢸⣷⣿⣇⢻⡿⣿⣿⣿⣿⠟⠀⠹⣿⣿⠃⠀⠘⣷⡀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣤⣦⣼⣿⠿⠛⣋⡁⣼⢠⣿⡿⠛⠉⠁⠀⠀⢀⡀⢀⣴⣾⠀⢸⣿⡇⢻⡄⠙⠿⠻⠛⠁⠀⢀⣠⣽⣿⣇⡀⠀⠸⣧⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⠿⣛⣭⣴⡾⠟⠛⣧⣿⢸⡿⠀⠀⠀⠀⣰⣿⣿⣷⣾⣿⣿⠀⢸⡏⣇⢸⣷⡀⠀⢀⣠⣴⣾⠿⠛⣿⢻⣿⣹⡀⠀⢻⣆⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣴⡟⣦⠀⠀⠀⢀⡿⣵⡿⠛⠉⣡⣶⣤⣄⣿⣯⢸⣇⠀⠀⢠⣾⣿⡿⣿⣿⣿⣿⡿⠀⢸⡇⢻⡼⣿⣷⣶⠿⠛⠉⠀⠀⠀⠸⡇⣿⣿⣧⠀⠘⣿⡀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⢹⠀⢀⣠⣼⣿⣿⠀⢀⣼⣿⣿⣿⣿⡇⣿⢸⣿⣀⣀⣿⡿⠿⠶⠚⠛⠉⠉⠀⠀⢸⡇⠀⢻⣾⣝⣿⡆⠀⢀⣠⡴⠖⠛⢻⡾⣿⣿⣆⠀⢹⡇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣇⣼⡾⠟⠋⣿⢻⣇⣤⣌⠻⢿⣿⣿⣿⠃⢿⠀⠉⠉⠁⠀⠀⠀⣀⣤⡤⠶⠶⠒⠚⣻⣷⣄⠈⣿⣿⣿⣿⡞⠉⠀⠀⠀⠀⠀⣿⢿⣿⣾⣋⣽⠇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣹⠏⠀⠀⠀⣿⢿⣿⣿⣯⡴⠾⠛⢋⣡⠶⠛⠛⠋⣉⣉⣉⣙⢻⣿⠀⠀⠀⠀⠀⢠⡟⠀⠈⠻⢦⣈⣿⣿⣧⠀⠀⢀⣠⣴⡾⢿⣿⣿⣿⣿⣿⡀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡟⣿⡟⠀⠀⠀⣿⠈⠋⠉⢀⣠⠴⣛⣩⣤⣶⣞⣭⣿⢿⣿⣿⣻⣼⣿⣆⣀⣤⣤⣴⣿⣄⣠⣶⣦⣀⣙⣿⣿⣿⡶⣿⠟⠋⣁⣶⠟⢻⣽⣿⣿⣿⠇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⢠⣿⣇⠀⠀⠀⢹⣠⡴⠖⢻⣷⢫⣿⣿⣿⣯⣿⣟⣿⣿⣭⣽⣿⡿⣿⣿⣿⠿⠿⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⣿⠋⠉⣿⠀⢸⣿⣿⣿⣿⣷⡀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣼⣿⣿⣤⣴⣾⢿⡅⠀⣀⣾⢿⣿⣿⣿⣿⣿⣿⡿⣿⣷⣿⣿⣿⡇⣿⣿⡇⠀⠀⢸⣿⣿⡟⢿⣿⣿⣿⣿⣿⣣⣿⠁⣿⣀⣤⡿⠀⢀⣿⣿⣿⣿⣿⡇⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⠻⣿⠛⠉⠀⠈⣿⠛⢽⣿⢻⣿⣿⢿⣿⣿⣿⡇⣿⠿⣶⣶⣚⣧⣿⣿⡇⠀⠀⣸⣿⣿⣿⣄⣈⢿⣿⢿⣷⣿⣿⠀⠉⠉⠀⠀⠀⠘⡇⣿⣿⣿⣿⡇⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⡀⣷⡆⠀⠀⠀⠸⣧⣻⣿⢸⣿⣿⡿⢿⣾⣻⡇⣿⣿⣿⣿⣿⣿⣿⠿⠷⠾⠛⠛⠿⢿⣿⣿⣿⣄⣿⠿⠋⢸⣿⠀⠀⠀⠀⠀⠀⠀⡇⣿⣿⣿⣿⣿⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣷⡇⣿⡇⠀⠀⠀⠀⣿⣿⣿⡾⢿⣿⣿⣿⣿⡶⠷⠾⠛⠛⠉⠁⢀⣠⠤⠴⠒⡆⢠⠀⢰⡉⠻⣿⣽⡏⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⠀⡇⣿⡿⣿⣿⣿⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣧⣿⠿⢀⣀⣤⣴⣿⣿⣿⡷⠾⠛⠋⠉⢀⣀⣠⠤⠴⠒⠻⡆⢸⠀⠀⢀⡠⠇⠸⡄⠈⣇⠀⠈⡻⢦⡀⠀⢸⡇⠀⠀⠀⠀⠀⠀⠀⡇⣿⣧⡘⠿⢻⡆\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠻⣆⣿⣿⣿⣿⣿⡿⠛⣉⣀⡀⣠⠴⠒⠋⠉⠁⠀⠀⠀⠀⠀⡇⢸⣠⠴⣫⡄⠀⠀⡇⠀⢹⠀⠀⣿⠦⢿⡀⢸⡇⠀⠀⣀⣤⣤⣿⠀⡇⣿⣿⣿⣆⢸⡇\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⢿⡟⣽⣿⠀⣏⠁⠀⡇⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣇⠀⡖⣻⠋⠀⠀⠈⢻⠀⢈⡇⠀⠸⡄⠘⣧⢸⡇⠀⢸⣷⣾⣿⠏⠀⡇⣿⣿⣿⣿⢸⡇\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣾⠏⠛⠋⢡⣿⠀⠸⣿⣟⡃⣇⠀⠀⠀⠀⠀⣀⣠⡤⠶⠒⠋⠀⠛⠁⠀⣀⣤⣶⣿⣿⣿⣿⣷⣤⡈⠁⢻⡞⣿⠀⠈⠻⣴⠏⠀⠀⠿⢹⣿⣎⢻⣿⡇\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣾⡟⠀⠀⢀⡿⣿⠀⠀⠈⠳⡇⠻⠤⠶⠚⠋⠉⠁⠀⠀⠀⠀⠀⣀⣤⣶⣿⣿⣿⣿⣿⠿⠛⠻⣿⣿⣿⣷⣜⣷⣿⠀⠀⢀⣀⣤⣤⣶⣾⣶⣿⣿⠃⢸⡇\n",
    "⠀⠀⠀⠀⠀⠀⣀⣤⡶⠶⠖⠚⢛⠛⠳⢶⣼⡟⠀⠀⢀⣼⣹⣿⢀⠀⠀⠀⠀⡀⠀⠀⠀⠀⠀⢀⣀⣠⡤⢤⣾⣿⣿⣿⡿⠿⠛⠉⠹⡇⠀⠀⣿⣿⣟⢿⣿⣿⠹⣶⣿⡿⠛⠻⣏⠀⠉⠉⡛⣿⡿⣾⡇\n",
    "⠀⠀⠀⢀⣴⠞⠋⢰⡇⢰⣿⢻⢻⢻⢶⣦⠙⣷⡀⠀⣸⢧⠟⢿⣿⣿⣿⣷⣶⣶⣤⣴⣲⡾⠿⠟⠒⠒⠛⡇⠙⣿⠉⠀⢧⠀⠀⠀⠀⣧⠀⠀⢸⣿⣿⡎⣿⠁⢀⣼⣏⢀⣠⣤⣸⣶⠀⠀⣿⣿⣿⠛⠁\n",
    "⠀⠀⠀⣾⠃⠀⣠⡬⣤⣼⣛⠾⣼⣞⡾⡟⠀⠘⣧⣠⣏⡞⠀⠈⠻⣿⡏⢹⡟⠛⠻⣿⠁⠀⠀⠀⠀⠀⠀⣇⠀⣿⠀⠀⢸⡄⠀⠀⠀⢸⠀⠀⠘⣿⣿⣇⣿⣴⡞⢣⣽⣿⣿⣿⣿⣿⠀⠀⣿⣿⡟⠀⠀\n",
    "⠀⠀⠀⣿⡶⣿⣿⣸⣿⣿⣿⠿⠷⠾⢽⣅⡲⠶⢻⣿⣼⢁⣠⣤⣶⣿⣿⠘⡇⠀⠀⢻⡆⠀⠀⠀⠀⠀⢀⣸⡀⢹⡇⠀⠈⡇⠀⠀⠀⠈⡇⠀⠀⢿⣿⣿⢹⣿⣤⣿⣿⣿⣿⡿⢿⣟⡀⠀⣿⣿⡇⠀⠀\n",
    "⠀⠀⠀⠈⠛⠿⢯⣜⣿⠏⠀⠀⠀⢀⡿⣨⣿⣶⣤⣿⣷⣯⣿⣿⣿⣿⣿⠀⡇⠀⠀⠐⡿⣦⣰⣒⣶⣿⣿⣿⣷⣾⣇⠀⠀⢻⠀⠀⠀⠀⢷⠀⠀⢸⣿⣿⣾⣿⣸⣿⡏⢠⠟⣠⣿⣿⣿⣦⡈⢹⡇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⢸⡟⣾⠄⠀⠀⣸⡇⣿⣿⣿⠟⠋⠛⢿⣿⣿⣿⣿⣿⡄⢻⠀⠀⠀⡇⠈⠙⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⢸⡆⠀⠀⠀⢸⡄⠀⠀⣿⣿⣇⣿⠛⠛⠻⣿⣺⣿⣿⣿⣿⣿⣿⡿⠃⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⣼⢧⡇⠀⠀⠀⣿⢸⣿⣿⡿⢦⣴⣿⣿⣷⡿⣿⡿⣿⡇⢸⡄⠀⠀⢹⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⡆⠀⠀⣇⠀⠀⠀⠀⣇⠀⠀⢸⣿⣟⢿⡀⠀⠀⠈⠉⠀⠉⠉⠉⠁⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⣿⣨⡧⠤⠤⢤⣇⡾⣿⣿⣠⣿⣿⣿⣿⣿⣿⣽⣿⣿⣷⠀⣇⠀⠀⢸⠀⠀⢸⢻⣿⣿⣿⣿⡇⣿⣿⠀⠀⢹⡄⠀⠀⢀⣸⠀⠀⠸⣿⣿⣼⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⢀⡿⣧⣤⠶⠦⣼⣿⣿⣿⡏⠈⣿⣿⢿⣿⣿⣿⣏⠉⢹⣿⡀⢻⠀⠀⠘⡇⠀⠸⡄⠙⢿⣿⣿⠇⣿⣿⡄⠀⠈⠓⠒⠋⠉⠀⠀⠀⠀⢿⠹⣯⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⣸⣿⢃⡏⠀⠀⢻⣿⣿⣽⣿⣦⠘⣿⣿⣿⣿⣿⢻⣿⣾⣿⡇⠘⡇⠀⠀⣇⠀⠀⣇⠀⠀⠙⢿⡇⣿⢸⣧⠀⠀⠀⠀⡴⠒⢶⠀⠀⠀⠘⣆⠀⢻⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⡿⡅⣸⢁⣄⡄⣾⣿⢿⣿⠿⣿⣿⢻⣿⣿⣟⣿⣸⣻⡿⣿⣧⠀⠙⠒⠛⠛⠀⠀⢿⣿⣄⠀⠀⠀⣿⠈⣿⡄⠀⠀⠀⡇⠀⠘⡇⠀⠀⠀⢿⣦⢸⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⢸⣧⡇⣿⣼⣿⠃⣿⣿⣾⣿⣷⣤⡿⠿⢿⣿⣿⣇⣿⡟⠋⠀⣿⡀⠀⣴⠲⡆⠀⠀⠸⣿⣿⣦⠀⠀⢸⡀⢹⣧⠀⠀⠀⣇⠀⠀⢹⠀⠀⠀⠸⣿⡟⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⢽⡿⣷⠏⠛⠿⢠⣿⣿⣿⣿⢿⣯⡇⠀⠀⠈⠁⠀⠀⠀⠀⠀⢸⣇⠀⢻⠀⢳⠀⠀⠀⣿⣿⣿⣷⣾⢸⡇⠈⣿⡀⠀⠀⢸⠀⠀⠈⡇⠀⠀⢀⣿⣿⣷⣀⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠘⣧⡙⣀⣀⣀⣸⣿⣽⣿⣿⠀⠈⠙⣶⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡀⢸⡀⠸⡄⠀⠀⢻⣿⣿⣿⣿⡼⡇⠀⢘⣧⣤⡴⠾⠷⠶⠖⠛⠛⢛⠋⠉⢿⢹⠉⣭⡿⠿⠷⠶⢦⡄⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠹⣟⣁⣸⣿⣿⣧⡿⠿⣿⣀⡀⠀⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣇⣈⣧⣘⣷⣤⣤⣼⠿⠿⣿⣿⣧⣧⡀⣸⢹⡏⠀⠀⠀⠀⠀⠀⠀⠈⡇⠀⢸⢸⡄⡿⠖⠚⠉⡉⠓⢿⡀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⣠⡴⣾⠋⠉⢙⣻⣷⠛⠛⠳⠶⠶⠽⠿⠃⠀⠀⠀⠀⠀⣀⡤⣼⡿⠋⠉⠁⠀⠀⣠⠀⣿⣿⠀⠀⠀⠀⠈⠉⠻⣿⢸⣷⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠸⡏⡇⣿⠀⠀⠀⢻⣷⢸⡇⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⡟⠀⡟⠀⠀⢸⣿⣿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⡥⢺⠏⡆⠀⠀⠀⠀⠀⡏⠀⡟⡇⠀⠀⠀⠀⠀⠀⢀⡇⢸⣿⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⡇⡇⢿⠀⠀⠀⢸⣿⡌⣷⠀⠀⠀⠀\n",
    "⠀⠀⠀⢸⠇⢠⡇⠀⠀⢰⣿⣯⣏⣻⡆⠀⠀⠀⠀⠀⠀⠀⠀⣸⠃⢀⡿⢸⡇⠀⠀⠀⠀⢠⡇⠀⡇⡇⠀⠀⠀⠀⠀⠀⢸⡇⢸⣿⡆⠀⠀⠀⠀⠀⠀⠀⣧⠀⠀⡇⢿⢸⠀⠀⠀⠈⣿⡇⢹⡀⠀⠀⠀\n",
    "⠀⠀⠀⡟⡄⣼⠀⠀⢀⣿⣿⣿⣿⣿⣷⠀⠀⠀⠀⠀⠀⠀⠀⣿⠀⢸⡇⣸⡇⠀⠀⠀⠀⢸⠁⢸⣷⡇⠀⠀⠀⠀⠀⠀⢸⡇⢸⣿⡇⠀⠀⠀⠀⠀⠀⠀⢻⠀⠀⢹⢸⣼⡀⠀⣀⣀⣿⣧⣸⡇⠀⠀⠀\n",
    "⠀⠀⢰⢧⣇⡏⠀⠀⣸⣿⠿⢭⣿⣿⡏⠀⠀⠀⠀⠀⠀⠀⢰⡏⠀⣿⠀⣿⡇⠀⠀⠀⠀⢸⠀⢸⢸⠁⠀⠀⠀⠀⠀⠀⢸⡇⢸⣿⣿⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⢸⢸⣿⡏⢉⣁⣤⣤⣄⢈⡇⠀⠀⠀\n",
    "⠀⠀⣼⢼⣿⠃⠀⠀⣿⣿⠀⢸⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⢸⡇⢠⡿⢰⣿⠃⠀⠀⠀⠀⣼⠀⢸⢸⠀⠀⠀⠀⠀⠀⠀⢸⡇⢸⢹⣸⣦⣤⣤⣤⣶⣶⣶⡿⠀⠀⢸⡄⡇⣧⣽⣿⣿⣿⡽⠟⠁⠀⠀⠀\n",
    "⠀⠀⢿⢻⡏⠀⠀⢰⣿⣿⣟⠛⢿⣿⡇⠀⠀⠀⠀⠀⠀⠀⢸⠗⣻⡇⢸⢹⣆⣀⣀⣀⣤⡏⠀⢸⢸⠀⠀⠀⠀⠀⠀⠀⢸⡇⢸⠈⠉⠉⠉⠉⠉⠉⠀⠀⠀⠀⠀⠈⡇⣿⠘⣿⣿⣿⣇⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⢸⠛⠤⢤⣤⣘⢺⣿⣿⣿⣿⡿⠃⠀⠀⠀⠀⠀⠀⠀⠸⢧⣿⠃⠘⠓⠛⠛⠛⠋⠉⠁⠀⢼⢸⠀⢰⡾⠿⠛⠛⠿⢿⡇⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⢸⠀⠙⣿⣿⣿⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⢘⣶⡶⠚⠿⢿⣿⣩⢿⢿⡏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣸⠀⢸⡇⠀⠀⠀⠀⣿⡇⡏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢷⢸⡀⠀⠈⠁⢸⡇⠀⠀⠀⠀⠀\n",
    "⠀⠀⣼⣹⠃⠀⢰⣷⢻⠁⠈⠛⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡟⣹⠀⢸⠃⠀⠀⠀⠀⣿⠇⠜⠀⣤⠶⠖⠛⠛⠋⠉⠉⢩⣿⡇⠀⢸⠸⡇⠀⠀⠀⠘⡇⠀⠀⠀⠀⠀\n",
    "⠀⢠⡟⠏⠀⠀⣾⣿⣼⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⡇⠀⢀⣴⠶⠞⠛⠛⣻⣷⠀⡏⣿⠀⢸⢀⣴⣷⣦⡀⣿⠇⡇⠀⡟⠀⣀⣀⣀⣀⣀⣀⣸⣿⡇⠀⢸⡆⡇⠀⠀⠀⠀⣷⠀⠀⠀⠀⠀\n",
    "⠀⣸⠇⠀⠀⢸⣿⡇⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⣾⣀⣀⣤⣤⣶⣿⡿⠀⡇⣿⠀⢸⣿⣿⣿⣫⣾⣿⠀⡇⢠⣟⣿⣿⣿⡿⠿⠿⠿⠿⠁⡇⠀⠈⡇⣷⢀⡀⠀⠀⢻⠀⠀⠀⠀⠀\n",
    "⠀⣿⡼⠀⠀⡟⣿⣷⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⣿⠀⢀⡟⡿⠿⠟⠛⠛⣃⡇⠀⡇⣿⠀⢸⣿⣿⣿⣿⣿⣿⡄⡇⢸⡇⠀⠀⠀⠀⠀⠀⠀⢰⣶⡇⠀⠀⣇⢹⣾⣿⠀⣰⢾⡆⠀⠀⠀⠀\n",
    "⢠⣿⡇⠀⢸⣷⣿⣹⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⠀⢸⡇⠀⠀⠀⠀⢰⣿⡇⠀⡇⣿⠀⣾⣿⣿⣿⣿⣿⣿⡃⡇⢸⣧⣤⣤⣴⣶⣶⣶⣶⣾⣿⡇⠀⠀⢿⢸⣿⣿⣾⣿⣸⡇⠀⠀⠀⠀\n",
    "⢸⢭⠥⠦⣬⣽⣧⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣸⣿⠀⢸⢵⣶⣾⣿⣿⣿⡿⡇⠀⡇⣿⠀⣿⣿⣿⣿⣿⣿⣿⡇⡇⢸⡏⠿⠟⠛⠛⠛⠛⠛⠛⣧⣷⠀⠀⢸⠀⣿⣿⣿⣿⠛⣇⠀⠀⠀⠀\n",
    "⢸⣸⠁⢠⣿⣿⣹⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⢸⠉⠉⠉⠁⠀⢠⣾⡇⠀⡇⣿⠀⣿⣿⣿⣿⣿⣿⣿⠇⡇⢸⡇⠀⣀⣀⣀⣀⣀⣀⣰⣿⣿⠀⠀⠸⠀⣿⣿⣿⣵⡇⣿⠀⠀⠀⠀\n",
    "⠘⣧⣰⠞⣞⣷⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⢸⣀⣀⣠⣤⣤⣼⣿⡇⠀⡇⣿⠀⢈⣭⣭⠭⠽⠭⣿⡇⡇⢸⣟⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡀⠀⠀⠀⢻⣟⣾⣿⣿⢻⠀⠀⠀⠀\n",
    "⠀⠈⠛⠛⠛⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⣿⠿⠿⠿⠿⠟⢛⣻⡇⠀⡇⢻⠀⢸⠁⠀⠀⠀⠀⣿⡇⡇⠸⡏⠉⠀⠀⠀⠀⠀⠀⠀⣼⣿⡇⠀⠀⠀⢸⣿⣿⣿⣿⢸⡆⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣇⠀⣿⣀⣤⣤⣤⣤⣼⣿⡇⠀⡇⢸⠀⢸⠀⣠⣶⣄⠀⣿⡇⣇⠀⡇⣴⣶⣶⣾⣿⣿⣿⣿⣿⣿⣇⣀⣂⠀⢸⣿⣿⣿⣿⣿⡇⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⡿⠴⠿⠿⠿⠿⠿⠿⠿⠿⠷⣦⡄⢸⠀⢸⣾⣿⣿⢟⣴⣿⣷⣼⠶⠗⠛⠛⠛⠛⠛⠛⠛⠋⠉⠉⠉⢉⡟⣧⠈⣿⣿⣿⣿⡿⣧⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣴⣿⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⣴⣿⡇⢸⣴⢾⣿⡿⣻⣿⣿⣿⣿⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⣿⣿⡇⢸⣿⢸⣿⣿⣿⣿⣿⣿⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⣿⣿⡀⣿⣿⣿⣿⣿⣿⡄⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣿⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣾⣿⣿⣿⡇⢸⣿⢸⣿⣿⣿⣿⣿⠃⠀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣿⣿⣿⡇⢸⣿⣿⣿⣿⢻⡇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣿⣷⣶⣶⣶⣶⣶⣶⣶⡶⠶⠦⠤⣾⣿⣿⣿⣿⣷⢘⣿⢸⣿⣿⣿⣿⡏⣭⠭⠭⠭⠤⠤⠤⠴⠶⠶⠶⠶⠶⠶⠶⠱⣌⢻⣿⣧⢸⣿⣿⣿⣿⣾⣇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⡾⠟⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⣉⣽⣿⣾⣿⣿⣿⣿⣿⠀⣿⢸⣿⣿⣿⡟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⡞⣿⢻⠈⣿⣿⣿⣿⣿⣿⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣶⠟⠋⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠⣴⣾⣿⣿⣿⣿⣿⣿⣿⠛⢹⠀⣿⣾⣿⣿⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⣿⣿⣿⢻⣿⡀⣿⣿⣿⣿⣿⣿⡄⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣾⣿⣀⣤⣄⣤⣤⣄⣀⣀⣀⣀⣶⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣅⢸⠀⣿⡿⣿⣿⣤⣤⣤⡤⠤⠤⠶⠶⠶⠖⠒⠒⠒⠚⠛⠛⠛⠺⣿⣿⣿⡇⠹⡇⣿⣿⣿⣿⣿⣿⡇⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⣸⣿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⣿⣿⣿⣿⡟⠉⢹⣿⣿⣿⣿⡿⠿⡾⠀⣿⡇⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⣿⠰⠇⣿⣿⣿⣿⡿⣿⡇⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⢿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡟⠛⠉⠁⠀⠀⠀⠙⠛⠉⠁⠀⠀⠁⠀⣛⣁⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⠟⣹⡇⢀⣙⣿⣯⡷⠿⠛⠁⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠉⠉⠉⠉⠉⠉⠹⠷⣦⣤⣤⣤⣤⣤⣤⣤⣤⣤⣶⣶⣶⡶⠶⠶⠶⠶⠾⠿⠛⠛⠋⠉⠉⠁⠀⠀\n",
    "</pre>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48b5c1",
   "metadata": {},
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e1252a",
   "metadata": {},
   "source": [
    "Install all dependencies needed for Windows/Linux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341e5109",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    %pip install pandas==2.2.3\n",
    "    %pip install numpy==1.26.4\n",
    "    %pip install matplotlib==3.10.1\n",
    "    %pip install seaborn==0.13.2\n",
    "    %pip install torch==2.6.0+cu118 torchvision==0.17.0+cu118 torchaudio==2.6.0+cu118 -f https://download.pytorch.org/whl/cu118/torch_stable.html\n",
    "    %pip install transformers==4.37.0\n",
    "    %pip install datasets==3.3.2\n",
    "    %pip install tqdm==4.67.1\n",
    "    %pip install wandb==0.19.8\n",
    "    %pip install bitsandbytes==0.41.1\n",
    "    %pip install accelerate==0.25.0\n",
    "    %pip install scikit-learn==1.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de399020",
   "metadata": {},
   "source": [
    "Install all dependencies needed for MacOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    %pip install pandas==2.2.3\n",
    "    %pip install numpy==1.26.4\n",
    "    %pip install matplotlib==3.10.1\n",
    "    %pip install seaborn==0.13.2\n",
    "    %pip install torch==2.6.0 torchvision==0.17.0 torchaudio==2.6.0\n",
    "    %pip install transformers==4.37.0\n",
    "    %pip install datasets==3.3.2\n",
    "    %pip install tqdm==4.67.1\n",
    "    %pip install wandb==0.19.8\n",
    "    %pip install coremltools\n",
    "    %pip install bitsandbytes==0.41.1\n",
    "    %pip install accelerate==0.25.0\n",
    "    %pip install scikit-learn==1.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d849338",
   "metadata": {},
   "source": [
    "Import all libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a12ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMultipleChoice,\n",
    "    BertTokenizer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    GenerationConfig,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    confusion_matrix,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d266c9",
   "metadata": {},
   "source": [
    "Setup random seed function to ensure reproducibility.\n",
    "\n",
    "_Info about the seed value: The field of natural language processing began in the 1940s, after World War II. At this time, people recognized the importance of translation from one language to another and hoped to create a machine that could do this sort of translation automatically. → Seed value is mostly set to 42_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a7ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1940\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a438be37",
   "metadata": {},
   "source": [
    "In the next step I import and split the dataset. For the split I take off the last 1000 entries from the train-split and use it as validation, the rest of this is of course used for the training. Then I use the validation-part as the test, since the real test-split has no answer keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c7f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_dataset(\"tau/commonsense_qa\", split=\"train[:-1000]\")\n",
    "valid = load_dataset(\"tau/commonsense_qa\", split=\"train[-1000:]\")\n",
    "test = load_dataset(\"tau/commonsense_qa\", split=\"validation\")\n",
    "\n",
    "print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe055504",
   "metadata": {},
   "source": [
    "I already define the GPU as my device, that I can just simply refer to this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5661bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff2820",
   "metadata": {},
   "source": [
    "Login for the experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492738f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eb2506",
   "metadata": {},
   "source": [
    "Next, I set some variables to be able to run the whole notebook without starting a training or a sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8002a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "activate_pretrained_bert_training = False\n",
    "activate_random_bert_training = False\n",
    "activate_pretrained_bert_sweep = False\n",
    "activate_random_bert_sweep = False\n",
    "activate_pretrained_bert_evaluation = False\n",
    "activate_random_bert_evaluation = False\n",
    "activate_deepseek_evaluation = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f8b6b6",
   "metadata": {},
   "source": [
    "# **Data Exploration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678cf5de",
   "metadata": {},
   "source": [
    "In this section I tried to get some insight to understand its structure and patterns. Due to the fact that we use the same data again for the second project, I took some code from my first projects data exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ed63b",
   "metadata": {},
   "source": [
    "### 1. Explore dataset structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[4m\" + \"Dataset Features\" + \"\\033[0m\")\n",
    "for feature in train.features:\n",
    "    print(feature)\n",
    "print(\"\\n\" + \"\\033[4m\" + \"Example\" + \"\\033[0m\")\n",
    "for feature in train.features:\n",
    "    print(feature + \":\", train[2][str(feature)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6fecb2",
   "metadata": {},
   "source": [
    "### 2. Get a general info about each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c20379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_df(dataset):\n",
    "    return pd.DataFrame(dataset)\n",
    "\n",
    "train_df = dataset_to_df(train)\n",
    "valid_df = dataset_to_df(valid)\n",
    "test_df = dataset_to_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4a076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[4m\" + \"Train Info\" + \"\\033[0m\")\n",
    "print(train_df.info())\n",
    "print(\"\\n\")\n",
    "print(\"\\033[4m\" + \"Validation Info\" + \"\\033[0m\")\n",
    "print(valid_df.info())\n",
    "print(\"\\n\")\n",
    "print(\"\\033[4m\" + \"Test Info\" + \"\\033[0m\")\n",
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae206d",
   "metadata": {},
   "source": [
    "### 3. Analyze question lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923ae116",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([train_df, valid_df, test_df], ignore_index=True)\n",
    "\n",
    "combined_df['question_length'] = combined_df['question'].apply(len)\n",
    "combined_df['question_word_count'] = combined_df['question'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"\\033[4m\" + \"Question length (characters)\" + \"\\033[0m\")\n",
    "print(f\"Min: {combined_df['question_length'].min()}\")\n",
    "print(f\"Max: {combined_df['question_length'].max()}\")\n",
    "print(f\"Mean: {combined_df['question_length'].mean():.2f}\")\n",
    "print(f\"Median: {combined_df['question_length'].median()}\")\n",
    "\n",
    "print(\"\\n\\033[4m\" + \"Question Word Count\" + \"\\033[0m\")\n",
    "print(f\"Min: {combined_df['question_word_count'].min()}\")\n",
    "print(f\"Max: {combined_df['question_word_count'].max()}\")\n",
    "print(f\"Mean: {combined_df['question_word_count'].mean():.2f}\")\n",
    "print(f\"Median: {combined_df['question_word_count'].median()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2892cd",
   "metadata": {},
   "source": [
    "### 4. Analyze option lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e8d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_option_lengths(choices):\n",
    "    return [len(text) for text in choices['text']]\n",
    "\n",
    "def get_option_word_counts(choices):\n",
    "    return [len(text.split()) for text in choices['text']]\n",
    "\n",
    "combined_df['option_lengths'] = combined_df['choices'].apply(get_option_lengths)\n",
    "combined_df['option_word_counts'] = combined_df['choices'].apply(get_option_word_counts)\n",
    "\n",
    "all_option_lengths = [length for lengths in combined_df['option_lengths'] for length in lengths]\n",
    "all_option_word_counts = [count for counts in combined_df['option_word_counts'] for count in counts]\n",
    "\n",
    "print(\"\\033[4m\" + \"Option length (characters)\" + \"\\033[0m\")\n",
    "print(f\"Min: {min(all_option_lengths)}\")\n",
    "print(f\"Max: {max(all_option_lengths)}\")\n",
    "print(f\"Mean: {np.mean(all_option_lengths):.2f}\")\n",
    "print(f\"Median: {np.median(all_option_lengths)}\")\n",
    "\n",
    "print(\"\\033[4m\" + \"\\nOption word count\" + \"\\033[0m\")\n",
    "print(f\"Min: {min(all_option_word_counts)}\")\n",
    "print(f\"Max: {max(all_option_word_counts)}\")\n",
    "print(f\"Mean: {np.mean(all_option_word_counts):.2f}\")\n",
    "print(f\"Median: {np.median(all_option_word_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e46b157",
   "metadata": {},
   "source": [
    "### 5. Analyze answer distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c76f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer_letter(example):\n",
    "    return example['answerKey']\n",
    "\n",
    "combined_df['answer_letter'] = combined_df.apply(extract_answer_letter, axis=1)\n",
    "\n",
    "print(\"\\033[4m\" + \"Answer Distribution\" + \"\\033[0m\")\n",
    "print(combined_df['answer_letter'].value_counts(), \"\\n\")\n",
    "print(combined_df['answer_letter'].value_counts(normalize=True).mul(100).round(2).astype(str) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a691ac06",
   "metadata": {},
   "source": [
    "### 6. Visualize question length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421af736",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(combined_df['question_word_count'], bins=20, kde=True)\n",
    "plt.title('Distribution of Question Word Count')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec5d2d0",
   "metadata": {},
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d03e9f7",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c9643",
   "metadata": {},
   "source": [
    "During the preprocessing phase of my NLP project, I carefully considered several common text-cleaning and preparation techniques. Below is a breakdown of each step, whether I used it, and the reasoning behind my decision.\n",
    "\n",
    "1. **Tokenization**  \n",
    "   ✅ *Used*  \n",
    "   I used the `BertTokenizer` from Hugging Face to tokenize all text inputs. This tokenizer breaks text into subword units and adds special tokens, ensuring compatibility with the BERT model architecture.\n",
    "\n",
    "2. **Lowercasing, Stemming, Lemmatizing, Stopword/Punctuation Removal**  \n",
    "   ❌ *Not used*  \n",
    "   These steps are common in traditional NLP pipelines but not necessary when using a pre-trained transformer like BERT. I specifically used the `'bert-base-cased'` model, which is sensitive to letter casing. Applying lowercasing or stripping punctuation could disrupt the model's understanding of context. Similarly, stemming or lemmatizing would interfere with subword tokenization, which already handles morphological variations effectively.\n",
    "\n",
    "3. **Removal of Unknown/Other Words**  \n",
    "   ❌ *Not explicitly used*  \n",
    "   Instead of manually removing unknown words, I relied on the tokenizer to handle them. Words not in the vocabulary are broken into subword tokens or mapped to the `[UNK]` token if completely unrecognized. BERT is designed to handle such cases gracefully.\n",
    "\n",
    "4. **Format Cleaning (e.g., HTML-extracted text)**  \n",
    "   ✅ *Used when necessary*  \n",
    "   While my dataset (CommonsenseQA) was fairly clean, I included basic text normalization steps to remove potential noise (e.g., HTML entities) as a precaution in other stages of the pipeline.\n",
    "\n",
    "5. **Truncation**  \n",
    "   ✅ *Used*  \n",
    "   To fit input sequences into BERT's maximum input size constraint, I applied truncation during tokenization. This ensures that long question-choice pairs are trimmed to 128 tokens, which balances performance and memory usage.\n",
    "\n",
    "6. **Feature Selection**  \n",
    "   ✅ *Used implicitly*  \n",
    "   Rather than traditional feature engineering, I relied on the tokenized outputs (`input_ids`, `attention_mask`, `token_type_ids`) generated by the tokenizer. These features are optimized for transformer models and encapsulate the essential linguistic structure needed for training.\n",
    "\n",
    "By tailoring preprocessing to suit BERT’s architecture, I avoided redundant or harmful steps while retaining the ones critical for accurate and efficient model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bed1da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76239e3c",
   "metadata": {},
   "source": [
    "The BertTokenizer is a class from the Hugging Face `transformers` library that handles the conversion of raw text into tokens that BERT can understand. Specifically, it tokenizes the input text into subword tokens (e.g., \"playing\" becomes [\"play\", \"##ing\"]). This subword tokenization allows the model to process both common and out-of-vocabulary words more effectively. The `from_pretrained('bert-base-cased')` method loads a pre-trained tokenizer that corresponds to the BERT model. The `'bert-base-cased'` model refers to a base-sized BERT model (with 12 layers and 768 hidden units) that has been trained on cased text, meaning it differentiates between uppercase and lowercase letters which is important for distinguishing meaning in proper nouns or acronyms (e.g., “US” vs “us”).\n",
    "\n",
    "**Why I use BertTokenizer:** I use the BertTokenizer to ensure that the text is processed in the exact way BERT was originally trained. \n",
    "\n",
    "**The tokenizer will:** \n",
    "- Split the text into subword tokens. \n",
    "- Add special tokens such as `[CLS]` and `[SEP]` that BERT requires. \n",
    "- Handle padding and truncation to ensure the input is the correct length for the model.\n",
    "\n",
    "I use the following line of code to initialize the BERT tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18589408",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0449f7",
   "metadata": {},
   "source": [
    "The `preprocess_commonsenseqa` function is designed to preprocess the CommonsenseQA dataset for input into a BERT-based model. The goal is to tokenize the questions and their corresponding multiple-choice answers into a format compatible with BERT, and then convert the correct answer's label into a numerical value.\n",
    "\n",
    "1. **Extracting Questions and Choices:** The function starts by extracting the questions and their multiple-choice options from the `examples` object.\n",
    "\n",
    "2. **Initialize Data Structures:** It then initializes empty lists to hold the tokenized inputs, attention masks, and token type IDs (used for differentiating between sentence pairs in models like BERT).\n",
    "\n",
    "3. **Convert Answer Labels to Indices:** The answer choices are labeled with letters (A, B, C, D, E), but the model requires numerical labels. This step converts the letters into indices (A → 0, B → 1, etc.).\n",
    "\n",
    "4. **Processing Each Question-Choice Pair:** For each question and its corresponding choices: Each choice is paired with the question. Both the question and the choice are tokenized using the BERT tokenizer (`tokenizer_bert`), which converts the text into `input_ids`, `attention_mask` and `token_type_ids` tensors that BERT can understand.\n",
    "\n",
    "5. **Stacking Tokens for Each Choice:** After tokenizing each choice for a question, the function stacks the resulting tensors (for all choices) into single tensors for input to the model.\n",
    "\n",
    "6. **Returning Tokenized Data:** Finally, the function returns a dictionary containing the tokenized inputs (`input_ids`, `attention_mask` and `token_type_ids`) along with the numerical labels corresponding to the correct answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd469731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_commonsenseqa(examples):\n",
    "\n",
    "    questions = [q for q in examples['question']]\n",
    "    \n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_token_type_ids = []\n",
    "    \n",
    "    answerkeys = examples['answerKey']\n",
    "    labels = []\n",
    "    \n",
    "    for key in answerkeys:\n",
    "        labels.append(ord(key) - ord('A'))\n",
    "    \n",
    "    for i, (question, choices) in enumerate(zip(questions, examples['choices'])):\n",
    "        inputs = []\n",
    "\n",
    "        for choice in choices['text']:\n",
    "            text_a = question\n",
    "            text_b = choice\n",
    "            \n",
    "            encoded = tokenizer_bert(\n",
    "                text_a, text_b,\n",
    "                add_special_tokens=True,\n",
    "                max_length=128,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            inputs.append({\n",
    "                'input_ids': encoded['input_ids'],\n",
    "                'attention_mask': encoded['attention_mask'],\n",
    "                'token_type_ids': encoded['token_type_ids']\n",
    "            })\n",
    "        \n",
    "        input_ids = torch.cat([x['input_ids'] for x in inputs])\n",
    "        attention_mask = torch.cat([x['attention_mask'] for x in inputs])\n",
    "        token_type_ids = torch.cat([x['token_type_ids'] for x in inputs])\n",
    "        \n",
    "        all_input_ids.append(input_ids)\n",
    "        all_attention_mask.append(attention_mask)\n",
    "        all_token_type_ids.append(token_type_ids)\n",
    "\n",
    "    return {\n",
    "        'input_ids': all_input_ids,\n",
    "        'attention_mask': all_attention_mask,\n",
    "        'token_type_ids': all_token_type_ids,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01af1037",
   "metadata": {},
   "source": [
    "Next, I apply the preprocessing to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49eb21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = preprocess_commonsenseqa(train)\n",
    "validation_dataset = preprocess_commonsenseqa(valid)\n",
    "test_dataset = preprocess_commonsenseqa(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b832460a",
   "metadata": {},
   "source": [
    "After preprocessing the CommonsenseQA dataset into tokenized inputs and labels, I convert the data into PyTorch `TensorDataset` objects. I do this because this groups all of the input tensors, so they can be iterated over together. It seamlessly integrates with PyTorch’s `DataLoader` for tasks like batching, shuffling and parallel data loading, ensuring that the data pipeline runs efficiently. Additionally, it provides synchronized indexing, ensuring that each input tensor corresponds correctly to its label, making the dataset ready for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9181ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = TensorDataset(\n",
    "    torch.stack(train_dataset['input_ids']),\n",
    "    torch.stack(train_dataset['attention_mask']),\n",
    "    torch.stack(train_dataset['token_type_ids']),\n",
    "    torch.tensor(train_dataset['labels'])\n",
    ")\n",
    "\n",
    "val_features = TensorDataset(\n",
    "    torch.stack(validation_dataset['input_ids']),\n",
    "    torch.stack(validation_dataset['attention_mask']),\n",
    "    torch.stack(validation_dataset['token_type_ids']),\n",
    "    torch.tensor(validation_dataset['labels'])\n",
    ")\n",
    "\n",
    "test_features = TensorDataset(\n",
    "    torch.stack(test_dataset['input_ids']),\n",
    "    torch.stack(test_dataset['attention_mask']),\n",
    "    torch.stack(test_dataset['token_type_ids']),\n",
    "    torch.tensor(test_dataset['labels'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20912123",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2f857d",
   "metadata": {},
   "source": [
    "For tokenizing my LLM I used the AutoTokenizer from Hugging Face to load the tokenizer for the DeepSeek-V2-Lite model. This tokenizer is specifically designed to be compatible with the DeepSeek model architecture. Since DeepSeek is a large language model (LLM), it expects input in a specific tokenized format, including proper handling of special tokens, padding and prompt formatting. By using the `AutoTokenizer` and loading the tokenizer directly from the model’s Hugging Face repository, I ensure that the text is processed exactly as the model was trained on.\n",
    "\n",
    "The `trust_remote_code=True` argument is necessary because DeepSeek uses custom model/tokenizer code not yet fully integrated into the standard Transformers library. This option allows the tokenizer to load correctly and function as intended.\n",
    "\n",
    "In short, I use this tokenizer to guarantee consistency between my input prompts and the expectations of the DeepSeek model, which is crucial for generating accurate and meaningful responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ffc6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_deepseek = AutoTokenizer.from_pretrained('deepseek-ai/DeepSeek-V2-Lite', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399fe1e",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04cdf8d",
   "metadata": {},
   "source": [
    "To see the number of parameters for my models a bit better, I first implemented a function which adds an apostrophe after every three digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29092cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_number(num):\n",
    "    return f\"{num:,}\".replace(\",\", \"'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bfa7cb",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f5dbfa",
   "metadata": {},
   "source": [
    "For the random initialized and the pretrained transformer architechture, I used BERT (Bidirectional Encoder Representations from Transformers) with a classification head specifically designed for multiple-choice inputs called `BertForMultipleChoice`. I used the Hugging Face checkpoint `bert-base-cased`, which is a pretrained transformer model developed by Google with **108'311'041 parameters**. This variant is trained on large English corpora (BooksCorpus and English Wikipedia) and maintains case sensitivity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b8d948",
   "metadata": {},
   "source": [
    "BERT is well-suited for classification tasks like CommonsenseQA due to its deep bidirectional attention, which helps capture the nuanced relationships between the question and each answer option. The pretrained bert-base-cased weights provide strong language understanding out of the box, significantly improving performance over training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f0c030",
   "metadata": {},
   "source": [
    "The BERT model is composed of an embedding layer, encoder, pooling layer, dropout layer and of course a classifier:\n",
    "\n",
    "1. **Embedding Layer (`BertEmbeddings`):**\n",
    "- Word Embeddings: `Embedding(28996, 768)` → Maps each token to a 768-dimensional vector. The vocabulary size is 28,996 tokens.\n",
    "- Position Embeddings: `Embedding(512, 768)` → Adds position information to each token, allowing the model to distinguish word order up to 512 tokens.\n",
    "- Token Type Embeddings: `Embedding(2, 768)` → Distinguishes between sentence pairs (e.g., question vs. answer).\n",
    "- Layer Normalization + Dropout: Normalizes embeddings and applies dropout (`p=0.1`) for regularization.\n",
    "\n",
    "2. **Encoder (`BertEncoder`):**\n",
    "- 12 Transformer Layers (stacked) → Each layer includes:\n",
    "    - Multi-Head Self-Attention (BertSelfAttention)\n",
    "        - Projects inputs into queries, keys and values using linear layers.\n",
    "        - Attention mechanism allows each token to attend to all others.\n",
    "        - Output passed through a linear layer, then dropout + layer norm.\n",
    "    - Feed-Forward Network\n",
    "        - First Linear: `768 → 3072`\n",
    "        - GELU activation\n",
    "        - Second Linear: `3072 → 768`\n",
    "        - Followed by LayerNorm and Dropout.\n",
    "- Each of these layers processes the tokenized question-choice pair, allowing the model to capture deep contextual relationships.\n",
    "\n",
    "3. **Pooling Layer (`BertPooler`):**\n",
    "- Extracts the `[CLS]` token output from the final encoder layer.\n",
    "- Applies a linear layer + `tanh` activation to produce a fixed-size sentence representation.\n",
    "\n",
    "4. **Dropout Layer:**\n",
    "- Applied before classification to reduce overfitting (`p=0.1`).\n",
    "\n",
    "5. **Classifier (`Linear(768 → 1)`):**\n",
    "- For each choice, outputs a single logit.\n",
    "- During training/evaluation, logits for all choices are grouped and passed through softmax to compute the predicted answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6b1756",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361fb529",
   "metadata": {},
   "source": [
    "In the following code block I load the configuration of the `'bert-base-cased'` model, but without the model weights. I just load the architecture details like number of layers, hidden size and so on. After I create a `BertForMultipleChoice` model using that configuration. This model is randomly initialized, meaining it hasn't learned anything yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eac71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_bert = BertConfig.from_pretrained('bert-base-cased')\n",
    "random_bert_model = BertForMultipleChoice(config_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2012b6e5",
   "metadata": {},
   "source": [
    "Next, I calculate and print the total number of parameters of the `random_bert_model` and then print its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7d9b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of parameters: {format_number(sum(p.numel() for p in random_bert_model.parameters()))}\\n\")\n",
    "print(random_bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec07ade",
   "metadata": {},
   "source": [
    "In the next cell I create a `BertForMultipleChoice` model with already pretrained weights. These pretrained weights allow my model to already understand some word meanings, grammar and general language patterns. This helps the model perform better and converge faster when fine-tuned on my specific downstream task, such as multiple-choice question answering with CommonsenseQA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_bert_model = BertForMultipleChoice.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c4b83",
   "metadata": {},
   "source": [
    "I calculate and print the total number of parameters of the `pretrained_bert_model` and then print its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f91a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of parameters: {format_number(sum(p.numel() for p in pretrained_bert_model.parameters()))}\\n\")\n",
    "print(pretrained_bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac8c42a",
   "metadata": {},
   "source": [
    "### DeepSeek-V2-Lite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febbc8ca",
   "metadata": {},
   "source": [
    "The `DeepSeek-V2-Lite` is a cutting-edge decoder-only transformer model optimized for causal language modeling, so for generating or completing text. With **15'706'484'224 parameters**, it is orders of magnitude larger than BERT and is specifically designed for autoregressive generation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a1b9b",
   "metadata": {},
   "source": [
    "For the DeepSeek-V2-Lite model, the architecture is composed of an embedding layer, a stack of decoder layers (transformer blocks), normalization layers and a final language modeling head for token prediction:\n",
    "\n",
    "1. **Token Embedding Layer:**\n",
    "- `Embedding(102400, 2048)` → Maps tokens from a very large vocabulary (102,400 tokens) to 2048-dimensional embeddings.\n",
    "\n",
    "2. **Stack of 27 Decoder Layers (`DeepseekV2DecoderLayer`):**\n",
    "- Each decoder layer includes:\n",
    "    - Self-Attention Mechanism (`DeepseekV2Attention`)\n",
    "        - Query projection: `Linear(2048 → 3072)`\n",
    "        - KV projections:\n",
    "            - `kv_a_proj_with_mqa`: `Linear(2048 → 576)` — Multi-query attention (MQA), a memory-efficient variant.\n",
    "            - `kv_b_proj`: `Linear(512 → 4096)` — Advanced attention processing.\n",
    "        - RMSNorm on KV inputs: Normalizes activations to improve stability.\n",
    "        - Rotary Embeddings (`DeepseekV2YarnRotaryEmbedding`) → Positional encoding mechanism that enables extrapolation to longer sequences.\n",
    "        - Output projection: `Linear(2048 → 2048)`\n",
    "    - Feed-Forward Layer\n",
    "        - Layer 0 uses:\n",
    "            - Standard MLP (`DeepseekV2MLP`)\n",
    "                - `gate_proj`, `up_proj`: `2048 → 10944`\n",
    "                - `down_proj`: `10944 → 2048`\n",
    "                - Activation: SiLU (a smooth, non-monotonic function similar to Swish)\n",
    "        - Layers 1–26 use:\n",
    "            - Mixture-of-Experts (MoE) Layer (`DeepseekV2MoE`)\n",
    "                - 64 expert MLPs (`DeepseekV2MLP`) with `2048 → 1408 → 2048`\n",
    "                - Gating mechanism: `MoEGate()` dynamically selects top-k experts per token.\n",
    "                - Shared expert also included: `2048 → 2816 → 2048`\n",
    "                - This makes computation sparse but increases capacity massively.\n",
    "    - Normalization\n",
    "        - RMSNorm instead of LayerNorm, used both before attention and before MLP. RMSNorm scales activations based on root mean square, which is more numerically stable in large-scale training.\n",
    "\n",
    "3. **Final LayerNorm + Output Head:**\n",
    "- Final RMSNorm applied to the output of the last decoder layer.\n",
    "- LM Head: `Linear(2048 → 102400)`\n",
    "    - Maps model outputs back into the token vocabulary for prediction.\n",
    "    - Weight sharing is likely applied with the token embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d01066",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82674b6c",
   "metadata": {},
   "source": [
    "With the following code cell I load the `DeepSeek-V2-Lite` large language model (https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite). This model is designed for causal language modeling tasks such as text generation. The loading configuration includes:\n",
    "- `BitsAndBytesConfig`: Configures 8-bit quantization for more efficient memory usage.\n",
    "  - `load_in_8bit=True`: Enables 8-bit quantization.\n",
    "  - `llm_int8_threshold=6.0`: Sets the threshold for the outlier detection.\n",
    "  - Other parameters to control the quantization behavior.\n",
    "- `AutoModelForCausalLM.from_pretrained(...)`: Loads the pretrained DeepSeek model.\n",
    "  - `quantization_config=bnb_config`: Applies the 8-bit quantization configuration.\n",
    "  - `device_map={\"\": 0}`: Maps the model to GPU device 0.\n",
    "  - `trust_remote_code=True`: Allows loading custom model code from the model's repository.\n",
    "- `GenerationConfig.from_pretrained(...)`: Loads the model's default generation configuration (e.g., max tokens, sampling strategy).\n",
    "- `pad_token_id = eos_token_id`: Sets the padding token to be the same as the end-of-sequence token for compatibility during generation.\n",
    "- A print statement at the end confirms which device the model is loaded on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d6325",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_skip_modules=None,\n",
    "    llm_int8_enable_fp32_cpu_offload=False\n",
    ")\n",
    "\n",
    "deepseek_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-V2-Lite\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    trust_remote_code=True\n",
    ")\n",
    "deepseek_model.generation_config = GenerationConfig.from_pretrained('deepseek-ai/DeepSeek-V2-Lite')\n",
    "deepseek_model.generation_config.pad_token_id = deepseek_model.generation_config.eos_token_id\n",
    "print(next(deepseek_model.parameters()).device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b70bd",
   "metadata": {},
   "source": [
    "I calculate and print the total number of parameters of the `deepseek_model` and then print its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96093558",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of parameters: {format_number(sum(p.numel() for p in deepseek_model.parameters()))}\\n\")\n",
    "print(deepseek_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaf3928",
   "metadata": {},
   "source": [
    "The function `process_commonsense_qa_for_deepseek(...)` in the next code cell is designed to process and evaluate the performance from the LLM on the CommonsenseQA dataset. Rather than using the dataset in a traditional classification setup (with logits over classes), this function reformulates each question into a text prompt that uses one \"few shot example\" and chain-of-thought reasoning to guide the model toward selecting the correct answer choice.\n",
    "\n",
    "Key features of the function include:\n",
    "- **Few-shot Learning**: Includes 5 detailed examples with step-by-step reasoning to help the model understand the task format and reasoning process.\n",
    "- **Prompt Construction**: Each question is converted into a formatted prompt including the question and multiple choice options labeled A–E, followed by \"Let's think step by step.\"\n",
    "- **Batch Processing**: Processes examples in batches for more efficient evaluation, with configurable batch size.\n",
    "- **Text Generation**: The model generates longer responses (default `max_new_tokens=30`) to allow for step-by-step reasoning. → I tried different values and 30 had the best accuracy-to-performance trade-off.\n",
    "- **Answer Extraction**: Uses regex pattern matching to extract the final answer letter from the chain-of-thought response, looking for patterns like \"So the answer is: X\".\n",
    "- **Fallback Mechanism**: If the expected pattern isn't found, it searches for any standalone letters (A-E). If also not found it chooses randomly.\n",
    "- **Memory Management**: Includes garbage collection to free memory between batches.\n",
    "- **Result Logging**: Collects questions, prompts, full generated responses, ground truth labels and accuracy metrics for comprehensive evaluation.\n",
    "- **Progress Tracking**: Uses tqdm to display a progress bar during evaluation (Updates per batch).\n",
    "\n",
    "The function returns a dictionary with all results and prints the final accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05105fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_commonsense_qa_for_deepseek(dataset, model, tokenizer, num_examples, max_new_tokens=100, batch_size=32):\n",
    "    results = {\n",
    "        'questions': [],\n",
    "        'prompts': [],\n",
    "        'responses': [],\n",
    "        'raw_responses': [],\n",
    "        'correct_answers': [],\n",
    "        'is_correct': [],\n",
    "        'is_from_fallback': [],\n",
    "        'pattern_matched': [],\n",
    "        'index_in_dataset': []\n",
    "    }\n",
    "\n",
    "    if num_examples is not None:\n",
    "        limited_dataset = dataset.select(range(min(num_examples, len(dataset))))\n",
    "    else:\n",
    "        limited_dataset = dataset\n",
    "\n",
    "    has_question_concept = 'question_concept' in limited_dataset.column_names\n",
    "    total = len(limited_dataset)\n",
    "\n",
    "    pbar = tqdm(total=total, desc=\"Generating answers\")\n",
    "    count_fallbacks = 0\n",
    "    count_direct_matches = 0\n",
    "    correct_from_fallback = 0\n",
    "    correct_from_direct = 0\n",
    "\n",
    "    for batch_start in range(0, total, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, total)\n",
    "\n",
    "        batch_questions = []\n",
    "        batch_prompts = []\n",
    "        batch_answer_keys = []\n",
    "        batch_indices = []\n",
    "\n",
    "        for i in range(batch_start, batch_end):\n",
    "            question = limited_dataset['question'][i]\n",
    "            question_concept = limited_dataset['question_concept'][i] if has_question_concept else ''\n",
    "            choices = limited_dataset['choices'][i]\n",
    "            choice_labels = choices['label']\n",
    "            choice_texts = choices['text']\n",
    "            answer_key = limited_dataset['answerKey'][i] if 'answerKey' in limited_dataset.column_names else \"N/A\"\n",
    "\n",
    "            formatted_choices = \"\\n\".join([f\"{label}. {text}\" for label, text in zip(choice_labels, choice_texts)])\n",
    "\n",
    "            few_shot_example = (\n",
    "                \"Q: Where would you be most likely to see a stop sign?\\n\"\n",
    "                \"Choices:\\n\"\n",
    "                \"A. In a library\\n\"\n",
    "                \"B. On a highway\\n\"\n",
    "                \"C. In a bedroom\\n\"\n",
    "                \"D. On a boat\\n\"\n",
    "                \"E. In a refrigerator\\n\"\n",
    "                \"Let's think step by step. A stop sign is used to control traffic. You don't see it indoors. It's commonly found where vehicles are present. So the answer is: B\\n\\n\"\n",
    "                \n",
    "                \"Q: What is needed to make a cake?\\n\"\n",
    "                \"Choices:\\n\"\n",
    "                \"A. Gravel\\n\"\n",
    "                \"B. Cement\\n\"\n",
    "                \"C. Sand\\n\" \n",
    "                \"D. Flour\\n\"\n",
    "                \"E. Plastic\\n\"\n",
    "                \"Let's think step by step. A cake is a baked food item. To make it, I need ingredients that can be mixed and baked. Flour is a basic ingredient in baking. Cement, sand, gravel and plastic are construction materials and not edible. So the answer is: D\\n\\n\"\n",
    "            )\n",
    "\n",
    "            prompt = (\n",
    "                few_shot_example +\n",
    "                f\"Q: {question}\\nChoices:\\n{formatted_choices}\\nLet's think step by step.\"\n",
    "            )\n",
    "\n",
    "            batch_questions.append(question)\n",
    "            batch_prompts.append(prompt)\n",
    "            batch_answer_keys.append(answer_key)\n",
    "            batch_indices.append(i)\n",
    "\n",
    "        inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=False,\n",
    "                    temperature=0.0,\n",
    "                    top_p=1.0,\n",
    "                    num_beams=1,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "            responses = []\n",
    "            is_from_fallback_batch = []\n",
    "            pattern_matched_batch = []\n",
    "            raw_responses_batch = []\n",
    "            \n",
    "            patterns = [\n",
    "                r\"So the answer is[:\\s]*([A-E])\\b\",\n",
    "                r\"[Tt]he answer is[:\\s]*([A-E])\\b\", \n",
    "                r\"[Tt]herefore,? the answer is[:\\s]*([A-E])\\b\",\n",
    "                r\"I choose[:\\s]*([A-E])\\b\",\n",
    "                r\"I select[:\\s]*([A-E])\\b\",\n",
    "                r\"Option[:\\s]*([A-E])\\b\",\n",
    "                r\"Choice[:\\s]*([A-E])\\b\",\n",
    "                r\"correct option is[:\\s]*([A-E])\\b\",\n",
    "                r\"correct answer must be[:\\s]*([A-E])\\b\",\n",
    "                r\"answer is ([A-E])[.,]\",\n",
    "                r\"([A-E]) is correct\\b\",\n",
    "                r\"correct choice is ([A-E])\\b\",\n",
    "                r\"best answer is ([A-E])\\b\"\n",
    "            ]\n",
    "            \n",
    "            for prompt, out in zip(batch_prompts, decoded):\n",
    "                response = out[len(prompt):].strip() if out.startswith(prompt) else out.strip()\n",
    "                raw_responses_batch.append(response)\n",
    "                \n",
    "                final_answer = None\n",
    "                matched_pattern = \"None\"\n",
    "                \n",
    "                for i, pattern in enumerate(patterns):\n",
    "                    match = re.search(pattern, response)\n",
    "                    if match:\n",
    "                        final_answer = match.group(1)\n",
    "                        matched_pattern = f\"Pattern {i}: {pattern}\"\n",
    "                        count_direct_matches += 1\n",
    "                        break\n",
    "                \n",
    "                used_fallback = False\n",
    "                if final_answer is None:\n",
    "                    count_fallbacks += 1\n",
    "                    used_fallback = True\n",
    "                    fallback = re.search(r'\\b([A-E])\\b', response)\n",
    "                    final_answer = fallback.group(1) if fallback else random.choice(['A', 'B', 'C', 'D', 'E'])\n",
    "                    matched_pattern = \"Fallback\" if fallback else \"Random\"\n",
    "                \n",
    "                responses.append(final_answer)\n",
    "                is_from_fallback_batch.append(used_fallback)\n",
    "                pattern_matched_batch.append(matched_pattern)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            responses = [random.choice(['A', 'B', 'C', 'D', 'E']) for _ in range(len(batch_prompts))]\n",
    "            is_from_fallback_batch = [True for _ in range(len(batch_prompts))]\n",
    "            pattern_matched_batch = [f\"Error: {str(e)}\" for _ in range(len(batch_prompts))]\n",
    "            raw_responses_batch = [f\"Error: {str(e)}\" for _ in range(len(batch_prompts))]\n",
    "\n",
    "        batch_is_correct = [resp == ans for resp, ans in zip(responses, batch_answer_keys)]\n",
    "        \n",
    "        for is_correct, is_fallback in zip(batch_is_correct, is_from_fallback_batch):\n",
    "            if is_correct and is_fallback:\n",
    "                correct_from_fallback += 1\n",
    "            elif is_correct and not is_fallback:\n",
    "                correct_from_direct += 1\n",
    "\n",
    "        results['questions'].extend(batch_questions)\n",
    "        results['prompts'].extend(batch_prompts)\n",
    "        results['responses'].extend(responses)\n",
    "        results['raw_responses'].extend(raw_responses_batch)\n",
    "        results['correct_answers'].extend(batch_answer_keys)\n",
    "        results['is_correct'].extend(batch_is_correct)\n",
    "        results['is_from_fallback'].extend(is_from_fallback_batch)\n",
    "        results['pattern_matched'].extend(pattern_matched_batch)\n",
    "        results['index_in_dataset'].extend(batch_indices)\n",
    "\n",
    "        if 'inputs' in locals():\n",
    "            del inputs\n",
    "        if 'outputs' in locals():\n",
    "            del outputs\n",
    "        gc.collect()\n",
    "        pbar.update(len(batch_prompts))\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    accuracy = sum(results['is_correct']) / len(results['is_correct']) if results['is_correct'] else 0\n",
    "\n",
    "    direct_predictions = [not fb for fb in results['is_from_fallback']]\n",
    "    direct_count = sum(direct_predictions)\n",
    "    fallback_count = len(results['is_from_fallback']) - direct_count\n",
    "    \n",
    "    direct_accuracy = correct_from_direct / direct_count if direct_count > 0 else 0\n",
    "    fallback_accuracy = correct_from_fallback / fallback_count if fallback_count > 0 else 0\n",
    "    \n",
    "    print(f\"Overall Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Direct Pattern Matches: {direct_count} ({direct_count/len(results['responses']):.1%})\")\n",
    "    print(f\"Fallback Matches: {fallback_count} ({fallback_count/len(results['responses']):.1%})\")\n",
    "    print(f\"Accuracy from Direct Matches: {direct_accuracy:.2%}\")\n",
    "    print(f\"Accuracy from Fallbacks: {fallback_accuracy:.2%}\")\n",
    "\n",
    "    class_total = {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'E': 0}\n",
    "    class_correct = {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'E': 0}\n",
    "    class_from_fallback = {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'E': 0}\n",
    "    class_from_direct = {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'E': 0}\n",
    "    \n",
    "    for ans, pred, is_correct, is_fallback in zip(\n",
    "        results['correct_answers'], \n",
    "        results['responses'], \n",
    "        results['is_correct'], \n",
    "        results['is_from_fallback']\n",
    "    ):\n",
    "        class_total[ans] = class_total.get(ans, 0) + 1\n",
    "        if is_correct:\n",
    "            class_correct[ans] = class_correct.get(ans, 0) + 1\n",
    "        \n",
    "        if is_fallback:\n",
    "            class_from_fallback[pred] = class_from_fallback.get(pred, 0) + 1\n",
    "        else:\n",
    "            class_from_direct[pred] = class_from_direct.get(pred, 0) + 1\n",
    "    \n",
    "    print(\"\\nPer-class accuracy:\")\n",
    "    for cls in ['A', 'B', 'C', 'D', 'E']:\n",
    "        if class_total[cls] > 0:\n",
    "            cls_acc = class_correct[cls] / class_total[cls]\n",
    "            print(f\"  Choice {cls}: {cls_acc:.2%} ({class_correct[cls]}/{class_total[cls]})\")\n",
    "    \n",
    "    print(\"\\nPrediction source distribution:\")\n",
    "    for cls in ['A', 'B', 'C', 'D', 'E']:\n",
    "        total_pred = class_from_direct[cls] + class_from_fallback[cls]\n",
    "        if total_pred > 0:\n",
    "            print(f\"  {cls}: {total_pred} predictions - {class_from_direct[cls]} direct ({class_from_direct[cls]/total_pred:.1%}), \"\n",
    "                  f\"{class_from_fallback[cls]} fallback ({class_from_fallback[cls]/total_pred:.1%})\")\n",
    "\n",
    "    analysis_data = []\n",
    "    for i in range(len(results['responses'])):\n",
    "        analysis_data.append({\n",
    "            'index': results['index_in_dataset'][i],\n",
    "            'question': results['questions'][i],\n",
    "            'correct_answer': results['correct_answers'][i],\n",
    "            'predicted': results['responses'][i],\n",
    "            'is_correct': results['is_correct'][i],\n",
    "            'from_fallback': results['is_from_fallback'][i],\n",
    "            'pattern_matched': results['pattern_matched'][i],\n",
    "            'raw_response': results['raw_responses'][i][:100] + \"...\" if len(results['raw_responses'][i]) > 100 else results['raw_responses'][i]\n",
    "        })\n",
    "\n",
    "    results['analysis_data'] = analysis_data\n",
    "\n",
    "    print(\"\\nSample of incorrect predictions (first 3):\")\n",
    "    incorrect_samples = [item for item in analysis_data if not item['is_correct']][:3]\n",
    "    for i, sample in enumerate(incorrect_samples):\n",
    "        print(f\"\\nIncorrect Example {i+1}:\")\n",
    "        print(f\"Question: {sample['question']}\")\n",
    "        print(f\"Predicted: {sample['predicted']} (Expected: {sample['correct_answer']})\")\n",
    "        print(f\"From Fallback: {sample['from_fallback']}\")\n",
    "        print(f\"Pattern Matched: {sample['pattern_matched']}\")\n",
    "        print(f\"Raw Response: {sample['raw_response']}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41073d29",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183cd80",
   "metadata": {},
   "source": [
    "I define all the needed hyperparameters for my <u>manual</u> training. Heres a description for all of the hyperparameters used:\n",
    "\n",
    "`epochs:` Number of times the model goes through the entire training dataset. Set to 5 just for testing purposes and to ensure the training process runs without issues.<br>\n",
    "`learning_rate:` Controls how much the model updates its weights with each step. Set to 1e-5, a common starting point for fine-tuning large transformer models like BERT to ensure stable convergence.<br>\n",
    "`batch_size:` Number of samples processed together before updating model weights. Set to 32 to fit well in memory while still enabling effective gradient updates.<br>\n",
    "`warmup_steps:` Gradually increases learning rate over the first training steps to avoid instability. Set to *10% of the total training steps.<br>\n",
    "`gradient_clip_val:` Limits the maximum gradient norm to prevent exploding gradients. Set to 2.0, a widely used value in transformer training to keep updates stable.<br>\n",
    "`save_interval:` Saves the model after every epoch.  Set to 1 to ensure that I always retain a recent model state and avoid accidental loss.<br>\n",
    "`gradient_accumulation_steps:` Accumulates gradients over multiple steps before updating weights, effectively increasing the batch size. Set to 4, which combined with batch_size = 32 gives an effective batch size of 128, improving generalization.<br>\n",
    "`patience:` Stops training early if validation performance doesn’t improve for the set amount of epochs. Set to 3, also mainly for testing purposes, to limit training time during development.<br>\n",
    "`weight_decay:` Applies L2 regularization to discourage overly large weights and reduce overfitting. Set to 0.01, a typical value that works well with `AdamW`to maintain generalization.<br>\n",
    "\n",
    "*For the warmup steps I found, that the value is often set to 5-10% of the total training steps (https://medium.com/better-ml/the-art-of-setting-learning-rate-eff11ac0a737). To get for example the 10%, I would use this code:  `warmup_steps = 0.1 * len(train_dataloader)`. Due to the fact, that I wanted to have all my hyperparameters in one block (including the batch size, which is needed for the dataloader after) I had to find a different way to calculate those 10%. A different way to calculate this would be: `warmup_steps = int(0.1 * len(train_features) / batch_size)`. With this approach I could create the dataloaders later, while having all necessary hyperparameters in one block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b06ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "learning_rate = 1e-5\n",
    "batch_size = 32\n",
    "warmup_steps = int(0.1 * len(train_features) / batch_size) # 0.1 = 10% of training data\n",
    "gradient_clip_val = 2.0\n",
    "save_interval = 1\n",
    "gradient_accumulation_steps = 4 # Effectively creates a batch size of 128 (batch_size * gradient_accumulation_steps)\n",
    "patience = 3\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351cbc22",
   "metadata": {},
   "source": [
    "In the next code cell, I define the data loaders for training, validation and testing. A DataLoader is responsible for efficiently loading batches of data during training or evaluation. Here's a breakdown of the configuration used for each dataset:\n",
    "- `train_features` / `val_features` / `test_features`: These are the preprocessed datasets for training, validation, and testing, respectively.\n",
    "- `batch_size`: Controls how many samples are passed through the model at once; defined earlier to ensure consistency.\n",
    "- `shuffle`:\n",
    "    - Set to `True` for the training set to ensure that the model sees a different order of examples each epoch (helps generalization).\n",
    "    - Set to `False` for validation and test sets to maintain deterministic behavior (important for consistent evaluation).\n",
    "- `num_workers=4`: Enables parallel data loading using 4 subprocesses. This speeds up data fetching, especially when I/O or preprocessing is involved.\n",
    "- `pin_memory=True`: Allows faster transfer of data from CPU to GPU by allocating the data in page-locked (pinned) memory — useful when training on a CUDA-enabled device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff26a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_features, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_features, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_features, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1dee63",
   "metadata": {},
   "source": [
    "As stated in the markdown for the hyperparameters, I used a bit of a different calculation to declare the warmup steps. In the following code block, I look at the output value of both calculations to make sure that they're the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef952a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(warmup_steps)\n",
    "print(int(0.1 * len(train_dataloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9053b80d",
   "metadata": {},
   "source": [
    "The following `train_transformer` function runs the full training process for a transformer model using PyTorch. It includes helpful features like saving progress (checkpoints), stopping early if the model stops improving, adjusting the learning rate during training, combining gradients across batches to save memory, and optionally tracking results using Weights & Biases (WandB).\n",
    "\n",
    "**Inputs:**\n",
    "- `model`: A transformer model compatible with HuggingFace Transformers API.\n",
    "- `train_dataloader`, `val_dataloader`: DataLoaders for training and validation sets.\n",
    "- `device`: The device to train on (e.g., `\"cuda\"` or `\"cpu\"`).\n",
    "- `epochs`: Number of training epochs.\n",
    "- `learning_rate`: Learning rate for the optimizer.\n",
    "- `warmup_steps`: Warm-up steps for the scheduler. If `None`, defaults to one epoch's worth of steps.\n",
    "- `log_wandb`: Whether to log metrics and configs to WandB.\n",
    "- `gradient_clip_val`: Value for gradient clipping.\n",
    "- `save_interval`: How often (in epochs) to save model checkpoints.\n",
    "- `gradient_accumulation_steps`: Number of steps to accumulate gradients before updating weights.\n",
    "- `patience`: Patience for early stopping.\n",
    "- `weight_decay`: Adds L2 regularization to the optimizer, which helps with overfitting.\n",
    "- `save_path`: Directory path to save checkpoints and the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9fec2c",
   "metadata": {},
   "source": [
    "**1. Checkpoint Saving Setup**\n",
    "- If `save_path` is provided:\n",
    "  - Create the checkpoint directory (if it doesn't exist).\n",
    "  - Define the path for saving the best model.\n",
    "\n",
    "**2. Model Preparation**\n",
    "- Move model to the specified `device` (CPU/GPU).\n",
    "- Enable gradient checkpointing (reduces memory usage, especially for large transformer models).\n",
    "\n",
    "**3. WandB Initialization**\n",
    "- If `log_wandb` is `True`:\n",
    "  - Import and initialize a Weights & Biases run.\n",
    "  - Log key hyperparameters and metadata.\n",
    "\n",
    "**4. Optimizer, Scheduler & Loss Setup**\n",
    "- Optimizer: `AdamW`\n",
    "  - I used it, because it decouples weight decay from the gradient update, improving generalization and stability for transformer-based models.\n",
    "- Scheduler: Linear with warmup\n",
    "  - `warmup_steps` defaults to 1 epoch’s steps if not specified.\n",
    "- Loss Function: `CrossEntropyLoss`\n",
    "  - I chose it, because it is the standard loss function for multi-class classification tasks and effectively penalizes incorrect predictions based on confidence.\n",
    "\n",
    "**5. Training Loop (per Epoch)**\n",
    "\n",
    "**a. Training Phase**\n",
    "- Set model to `train()` mode.\n",
    "- For each batch in `train_dataloader`:\n",
    "  - Move inputs to `device`\n",
    "  - Forward pass → compute loss and predictions\n",
    "  - Scale loss for gradient accumulation\n",
    "  - Backpropagate loss\n",
    "  - Every `gradient_accumulation_steps`:\n",
    "    - Clip gradients\n",
    "    - Optimizer and scheduler step\n",
    "    - Zero gradients\n",
    "  - Log loss and update progress\n",
    "\n",
    "- Compute epoch-level metrics:\n",
    "  - Average training loss\n",
    "  - Training accuracy\n",
    "\n",
    "**b. Validation Phase**\n",
    "- Set model to `eval()` mode\n",
    "- Disable gradients with `torch.no_grad()`\n",
    "- For each batch in `val_dataloader`:\n",
    "  - Move inputs to `device`\n",
    "  - Forward pass → compute loss and predictions\n",
    "\n",
    "- Compute:\n",
    "  - Average validation loss\n",
    "  - Validation accuracy\n",
    "\n",
    "**c. Logging**\n",
    "- If WandB is enabled:\n",
    "  - Log training/validation loss, accuracy, and learning rate\n",
    "\n",
    "**d. Checkpointing**\n",
    "- If `epoch % save_interval == 0`:\n",
    "  - Save a checkpoint with:\n",
    "    - Model state\n",
    "    - Optimizer state\n",
    "    - Scheduler state\n",
    "    - Metadata\n",
    "\n",
    "**e. Best Model Saving & Early Stopping**\n",
    "- If validation accuracy is best so far:\n",
    "  - Save model and checkpoint\n",
    "  - Reset early stopping counter\n",
    "- Else:\n",
    "  - Increment counter\n",
    "  - Stop training if counter > `patience`\n",
    "\n",
    "**6. Load Best Model**\n",
    "- If a best model was saved, reload its weights.\n",
    "\n",
    "**7. Finalize WandB Run**\n",
    "- If `log_wandb` is `True`, call `wandb.finish()`.\n",
    "\n",
    "**Return Values**\n",
    "- `model`: Trained transformer model\n",
    "- `train_losses`: List of average training losses (per epoch)\n",
    "- `val_accuracies`: List of validation accuracies (per epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c0e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(model, train_dataloader, val_dataloader, device, \n",
    "                      epochs=10, learning_rate=1e-4, warmup_steps=None,\n",
    "                      log_wandb=True, gradient_clip_val=5.0, save_interval=1,\n",
    "                      gradient_accumulation_steps=4, patience=3,\n",
    "                      weight_decay=0.001, save_path=None, existing_wandb_run=None):\n",
    "        \n",
    "    if save_path:\n",
    "        checkpoint_dir = save_path\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        best_model_path = os.path.join(checkpoint_dir, \"best_transformer_model.pt\")\n",
    "    else:\n",
    "        print(\"Warning: No save_path provided. Model and checkpoints will not be saved.\")\n",
    "        checkpoint_dir = None\n",
    "        best_model_path = None\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    is_pretrained = hasattr(model.config, 'name_or_path') and 'bert' in model.config.name_or_path.lower()\n",
    "    \n",
    "    if is_pretrained:\n",
    "        model_type_name = \"pretrained_transformer\"\n",
    "    else:\n",
    "        model_type_name = \"random_transformer\"\n",
    "    \n",
    "    if log_wandb:\n",
    "        if existing_wandb_run is None:\n",
    "            import wandb\n",
    "            if not wandb.run:\n",
    "                run_name = f\"{model_type_name}-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "                wandb.init(\n",
    "                    project=\"CommonsenseQA\",\n",
    "                    name=run_name,\n",
    "                    config={\n",
    "                        \"learning_rate\": learning_rate,\n",
    "                        \"epochs\": epochs,\n",
    "                        \"batch_size\": train_dataloader.batch_size,\n",
    "                        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "                        \"effective_batch_size\": train_dataloader.batch_size * gradient_accumulation_steps,\n",
    "                        \"weight_decay\": weight_decay,\n",
    "                        \"warmup_steps\": warmup_steps,\n",
    "                        \"gradient_clip_val\": gradient_clip_val,\n",
    "                        \"model_type\": model_type_name,\n",
    "                    })\n",
    "                print(f\"Initialized new wandb run with name: {run_name}\")\n",
    "        else:\n",
    "            wandb = existing_wandb_run\n",
    "            print(f\"Using existing wandb run: {wandb.run.name}\")\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    total_steps = len(train_dataloader) * epochs // gradient_accumulation_steps\n",
    "    \n",
    "    if warmup_steps is None:\n",
    "        warmup_steps = len(train_dataloader)\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=warmup_steps, \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    print(f\"Training {model_type_name} model...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_correct = 0\n",
    "        epoch_total = 0\n",
    "        progress_bar = tqdm(train_dataloader, desc=\"Training\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            input_ids, attention_mask, token_type_ids, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            epoch_correct += (preds == labels).sum().item()\n",
    "            epoch_total += labels.size(0)\n",
    "            \n",
    "            loss_to_backward = loss / gradient_accumulation_steps\n",
    "            loss_to_backward.backward()\n",
    "\n",
    "            if (i + 1) % gradient_accumulation_steps == 0 or (i + 1) == len(train_dataloader):\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_val)\n",
    "                \n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if log_wandb and wandb.run:\n",
    "                    wandb.log({\"learning_rate\": scheduler.get_last_lr()[0]})\n",
    "            \n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_dataloader)\n",
    "        train_accuracy = epoch_correct / epoch_total\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Training loss: {avg_train_loss:.4f}, accuracy: {train_accuracy:.4f}\")\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(val_dataloader, desc=\"Validation\")\n",
    "            \n",
    "            for batch in progress_bar:\n",
    "                input_ids, attention_mask, token_type_ids, labels = [b.to(device) for b in batch]\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids\n",
    "                )\n",
    "                logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "                \n",
    "                loss = criterion(logits, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, preds = torch.max(logits, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                progress_bar.set_postfix({\"acc\": correct/total})\n",
    "            \n",
    "        val_accuracy = correct / total\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        print(f\"Validation loss: {avg_val_loss:.4f}, accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        if log_wandb and wandb.run:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"train_accuracy\": train_accuracy,\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"val_accuracy\": val_accuracy,\n",
    "                \"learning_rate\": scheduler.get_last_lr()[0],\n",
    "                \"model_type\": model_type_name\n",
    "            })\n",
    "        \n",
    "        if checkpoint_dir and (epoch + 1) % save_interval == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"{model_type_name}_checkpoint_epoch_{epoch+1}.pt\")\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            \n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model_to_save.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'train_losses': train_losses,\n",
    "                'val_accuracies': val_accuracies,\n",
    "                'model_type': model_type_name\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "        \n",
    "        if best_model_path and val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            best_model_path_with_type = os.path.join(os.path.dirname(best_model_path), \n",
    "                                                   f\"best_{model_type_name}_model.pt\")\n",
    "            torch.save(model_to_save.state_dict(), best_model_path_with_type)\n",
    "            print(f\"Best model saved to {best_model_path_with_type}\")\n",
    "            \n",
    "            best_checkpoint_path = os.path.join(checkpoint_dir, \n",
    "                                              f\"best_{model_type_name}_checkpoint_epoch_{epoch+1}.pt\")\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model_to_save.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'train_losses': train_losses,\n",
    "                'val_accuracies': val_accuracies,\n",
    "                'model_type': model_type_name\n",
    "            }\n",
    "            torch.save(checkpoint, best_checkpoint_path)\n",
    "            \n",
    "            if log_wandb and wandb.run:\n",
    "                wandb.run.summary[\"best_accuracy\"] = best_accuracy\n",
    "                wandb.run.summary[\"best_epoch\"] = epoch + 1\n",
    "                wandb.run.summary[\"model_type\"] = model_type_name\n",
    "            \n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            print(f\"No improvement for {early_stopping_counter} epochs\")\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(f\"Early stopping after {epoch+1} epochs\")\n",
    "                if log_wandb and wandb.run:\n",
    "                    wandb.run.summary[\"stopped_epoch\"] = epoch + 1\n",
    "                break\n",
    "    \n",
    "    if best_model_path and os.path.exists(best_model_path_with_type):\n",
    "        model.load_state_dict(torch.load(best_model_path_with_type))\n",
    "        print(f\"Loaded best model from {best_model_path_with_type}\")\n",
    "    \n",
    "    return model, train_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5223d12",
   "metadata": {},
   "source": [
    "The function in the next two cells are for training the pretrained and randomly initialized BERT models using the `train_transformer` function with specified hyperparameters and checkpoint saving enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103eedf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if activate_pretrained_bert_training == True:\n",
    "    trained_pretrained_bert_model, trained_pretrained_bert_train_losses, trained_pretrained_bert_val_accuracies = train_transformer(\n",
    "        pretrained_bert_model, \n",
    "        train_dataloader, \n",
    "        val_dataloader, \n",
    "        device,\n",
    "        epochs=epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_steps=warmup_steps,\n",
    "        gradient_clip_val=gradient_clip_val,\n",
    "        save_interval=save_interval,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        patience=patience,\n",
    "        weight_decay=weight_decay,\n",
    "        save_path=f\"./checkpoints/pretrained_transformer-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1637d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if activate_random_bert_training == True:    \n",
    "    trained_random_bert_model, trained_random_bert_train_losses, trained_random_bert_val_accuracies = train_transformer(\n",
    "        random_bert_model, \n",
    "        train_dataloader, \n",
    "        val_dataloader, \n",
    "        device,\n",
    "        epochs=epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_steps=warmup_steps,\n",
    "        gradient_clip_val=gradient_clip_val,\n",
    "        save_interval=save_interval,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        patience=patience,\n",
    "        weight_decay=weight_decay,\n",
    "        save_path=f\"./checkpoints/random_transformer-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158629d",
   "metadata": {},
   "source": [
    "### Sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f1306",
   "metadata": {},
   "source": [
    "To explore optimal hyperparameters for both the pretrained and randomly initialized BERT models, I defined two sweep functions: `run_sweep_pretrained` and `run_sweep_random`. Both make use of the `train_transformer` training loop and share the same sweep configuration values (project requirement). Each sweep run initializes a fresh instance of the corresponding model (to ensure that the model is being reset correctly) and runs training and validation with its own `wandb` session. At the end of each run, the best validation accuracy is logged for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb01e4e",
   "metadata": {},
   "source": [
    "Below is a breakdown of each hyperparameter and the rationale behind the selected values or ranges:\n",
    "\n",
    "`batch_size`: `[32]` → previously defined `[8, 16, 32, 64]` (can be seen in some runs)\n",
    "- **Reasoning:** A batch size of 32 was selected as a balance between stability of gradient updates and memory efficiency.\n",
    "- **Fixed Value:** I kept it fixed in the end to reduce sweep complexity and because larger batches didn’t fit in GPU memory during preliminary testing.\n",
    "\n",
    "`learning_rate`: `log_uniform_values from 1e-6 to 1e-4`\n",
    "- **Why log scale:** Transformers are highly sensitive to the learning rate. Small changes can drastically affect convergence, making log scale more appropriate.\n",
    "- **Range Justification:**\n",
    "  - `1e-6`: For stable but slow training, often needed when fine-tuning pretrained models.\n",
    "  - `1e-4`: Allows exploration of slightly more aggressive updates, particularly relevant for training randomly initialized models.\n",
    "\n",
    "`weight_decay`: `log_uniform_values from 1e-6 to 1e-2`\n",
    "- **Purpose:** Regularizes the model by penalizing large weights, helping to avoid overfitting.\n",
    "- **Range Justification:**\n",
    "  - `1e-6`: Minimal regularization—safe starting point, especially for pretrained models.\n",
    "  - `1e-2`: Stronger regularization—useful for random initialization where overfitting can be more prevalent.\n",
    "\n",
    "`gradient_clip_val`: `uniform from 0.5 to 2.0`\n",
    "- **Reasoning:** Clipping prevents exploding gradients, especially useful in unstable early training phases (e.g., with untrained models).\n",
    "- **Range Justification:**\n",
    "  - Lower bound `0.5`: Conservative clipping.\n",
    "  - Upper bound `2.0`: Allows gradients to retain enough magnitude for effective updates.\n",
    "\n",
    "`gradient_accumulation_steps`: `[1, 2, 4]`\n",
    "- **Motivation:** Simulates larger batch sizes without exceeding GPU memory. Helps smooth out noisy gradients, especially when using a small physical batch size.\n",
    "- **Choice of Values:**\n",
    "  - `1`: Baseline (no accumulation).\n",
    "  - `2, 4`: Increased effective batch size without increasing memory usage.\n",
    "\n",
    "`warmup_ratio`: `[0.1]`\n",
    "- **Why 10%?** Warming up the learning rate over 10% of total steps helps stabilize training, especially early on.\n",
    "- **Transformer Norm:** 0.1 is a standard warmup ratio in most BERT training pipelines (e.g., from the original BERT paper and HuggingFace defaults).\n",
    "\n",
    "`epochs`: `[50]`\n",
    "- **Why 50?** Ensures sufficient training time for convergence, particularly for the randomly initialized model which typically learns more slowly.\n",
    "- **Avoiding Truncation:** Prevents early termination of sweeps before reaching peak performance.\n",
    "- **Mitigated by Early Stopping:** Since I use patience-based early stopping, long epoch counts don't necessarily waste compute.\n",
    "\n",
    "`patience`: `[5]`\n",
    "- **Function:** Early stopping prevents overfitting and saves compute by halting training if validation accuracy doesn’t improve.\n",
    "- **Why 5?** Allows for minor plateaus in learning, giving models a fair chance to recover while still avoiding prolonged stagnation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716b5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function_pretrained(config=None):\n",
    "    with wandb.init(\n",
    "    project=\"CommonsenseQA\",\n",
    "    name=f\"pretrained_transformer-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "    ) as run:\n",
    "        config = wandb.config\n",
    "\n",
    "        pretrained_bert_model = BertForMultipleChoice.from_pretrained('bert-base-cased')\n",
    "\n",
    "        train_batch_size = config.batch_size\n",
    "        \n",
    "        train_dataloader = DataLoader(train_features,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_dataloader = DataLoader(\n",
    "            val_features,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        unique_save_path = f\"./checkpoints/sweep-{wandb.run.id}-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        \n",
    "        trained_model, train_losses, val_accuracies = train_transformer(\n",
    "            model=pretrained_bert_model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            device=device,\n",
    "            epochs=config.epochs,\n",
    "            learning_rate=config.learning_rate,\n",
    "            warmup_steps=config.warmup_ratio * len(train_dataloader),\n",
    "            gradient_clip_val=config.gradient_clip_val,\n",
    "            save_interval=1,\n",
    "            gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "            patience=config.patience,\n",
    "            weight_decay=config.weight_decay,\n",
    "            save_path=unique_save_path,\n",
    "            log_wandb=True,\n",
    "            existing_wandb_run=wandb\n",
    "        )\n",
    "        \n",
    "        del trained_model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        best_val_accuracy = max(val_accuracies)\n",
    "        wandb.log({\"best_val_accuracy\": best_val_accuracy})\n",
    "        return best_val_accuracy\n",
    "\n",
    "def objective_function_random(config=None):\n",
    "    with wandb.init(\n",
    "    project=\"CommonsenseQA\",\n",
    "    name=f\"random_transformer-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "    ) as run:\n",
    "        config = wandb.config\n",
    "\n",
    "        config_bert = BertConfig.from_pretrained('bert-base-cased')\n",
    "        random_bert_model = BertForMultipleChoice(config_bert)\n",
    "        \n",
    "        train_batch_size = config.batch_size\n",
    "        \n",
    "        train_dataloader = DataLoader(train_features,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_dataloader = DataLoader(\n",
    "            val_features,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        unique_save_path = f\"./checkpoints/sweep-{wandb.run.id}-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        \n",
    "        trained_model, train_losses, val_accuracies = train_transformer(\n",
    "            model=random_bert_model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            device=device,\n",
    "            epochs=config.epochs,\n",
    "            learning_rate=config.learning_rate,\n",
    "            warmup_steps=config.warmup_ratio * len(train_dataloader),\n",
    "            gradient_clip_val=config.gradient_clip_val,\n",
    "            save_interval=1,\n",
    "            gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "            patience=config.patience,\n",
    "            weight_decay=config.weight_decay,\n",
    "            save_path=unique_save_path,\n",
    "            log_wandb=True,\n",
    "            existing_wandb_run=wandb\n",
    "        )\n",
    "        \n",
    "        del trained_model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        best_val_accuracy = max(val_accuracies)\n",
    "        wandb.log({\"best_val_accuracy\": best_val_accuracy})\n",
    "        return best_val_accuracy\n",
    "\n",
    "def run_sweep_pretrained(count):\n",
    "    sweep_config = {\n",
    "        'method': 'bayes',\n",
    "        'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
    "        'name': 'pretrained_model_sweep',\n",
    "        'parameters': {\n",
    "            'batch_size': {\n",
    "                'values': [32]\n",
    "            },\n",
    "            'learning_rate': {\n",
    "                'distribution': 'log_uniform_values',\n",
    "                'min': 1e-6,\n",
    "                'max': 1e-4\n",
    "            },\n",
    "            'weight_decay': {\n",
    "                'distribution': 'log_uniform_values',\n",
    "                'min': 1e-6,\n",
    "                'max': 1e-2\n",
    "            },\n",
    "            'gradient_clip_val': {\n",
    "                'distribution': 'uniform',\n",
    "                'min': 0.5,\n",
    "                'max': 2.0\n",
    "            },\n",
    "            'gradient_accumulation_steps': {\n",
    "                'values': [1, 2, 4]\n",
    "            },\n",
    "            'warmup_ratio': {\n",
    "                'values': [0.1]\n",
    "            },\n",
    "            'epochs': {\n",
    "                'values': [50]\n",
    "            },\n",
    "            'patience': {\n",
    "                'values': [5]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"CommonsenseQA\")\n",
    "\n",
    "    wandb.agent(sweep_id, function=objective_function_pretrained, count=count)\n",
    "\n",
    "def run_sweep_random(count):\n",
    "    sweep_config = {\n",
    "        'method': 'bayes',\n",
    "        'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
    "        'name': 'pretrained_model_sweep',\n",
    "        'parameters': {\n",
    "            'batch_size': {\n",
    "                'values': [32]\n",
    "            },\n",
    "            'learning_rate': {\n",
    "                'distribution': 'log_uniform_values',\n",
    "                'min': 1e-6,\n",
    "                'max': 1e-4\n",
    "            },\n",
    "            'weight_decay': {\n",
    "                'distribution': 'log_uniform_values',\n",
    "                'min': 1e-6,\n",
    "                'max': 1e-2\n",
    "            },\n",
    "            'gradient_clip_val': {\n",
    "                'distribution': 'uniform',\n",
    "                'min': 0.5,\n",
    "                'max': 2.0\n",
    "            },\n",
    "            'gradient_accumulation_steps': {\n",
    "                'values': [1, 2, 4]\n",
    "            },\n",
    "            'warmup_ratio': {\n",
    "                'values': [0.1]\n",
    "            },\n",
    "            'epochs': {\n",
    "                'values': [50]\n",
    "            },\n",
    "            'patience': {\n",
    "                'values': [5]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"CommonsenseQA\")\n",
    "    \n",
    "    wandb.agent(sweep_id, function=objective_function_random, count=count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec39f2",
   "metadata": {},
   "source": [
    "Run the sweep for the pretrained bert model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if activate_pretrained_bert_sweep == True:\n",
    "    print(\"Starting sweep for pretrained model...\")\n",
    "    run_sweep_pretrained(count=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7333a47a",
   "metadata": {},
   "source": [
    "Run the sweep for the random bert model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b90ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if activate_random_bert_sweep == True:\n",
    "    print(\"Starting sweep for randomly initialized model...\")\n",
    "    run_sweep_random(count=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61866a81",
   "metadata": {},
   "source": [
    "# **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8fe1f8",
   "metadata": {},
   "source": [
    "The function `evaluate_bert_transformer` in the next cell evaluates a fine-tuned BERT-like transformer model on a given dataset.\n",
    "\n",
    "**Inputs**\n",
    "- `model`: The trained transformer model to evaluate.\n",
    "- `dataloader`: A PyTorch DataLoader object providing evaluation batches.\n",
    "- `device`: The computation device (e.g., `'cuda'` or `'cpu'`).\n",
    "- `show_confusion_matrix` (`bool`, default=`True`): Whether to display a confusion matrix.\n",
    "- `save_path`: If provided, the confusion matrix will be saved to this path as a PNG image.\n",
    "\n",
    "**Outputs**<br>\n",
    "Returns a dictionary with the following metrics:\n",
    "- `'accuracy'`: Overall accuracy.\n",
    "- `'precision'`: Weighted precision (if all classes have predictions).\n",
    "- `'recall'`: Weighted recall.\n",
    "- `'f1_score'`: Weighted F1 score.\n",
    "- `'confusion_matrix'`: A raw confusion matrix (as a NumPy array).\n",
    "- `'per_class_accuracy'`: A dictionary mapping each class (A–E) to its individual accuracy.\n",
    "\n",
    "**Notes**\n",
    "- Assumes a 5-class classification task, where labels 0–4 are mapped to choices A–E.\n",
    "- Uses `torch.max()` to get predicted labels from model logits.\n",
    "- Handles class imbalance by computing weighted metrics.\n",
    "- If some classes are missing in predictions, precision/recall/F1 may fall back to `None`.\n",
    "- A confusion matrix is optionally shown and saved using Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d4be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bert_transformer(model, dataloader, device, show_confusion_matrix=True, save_path=None):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Evaluation\")\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            input_ids, attention_mask, token_type_ids, labels = [b.to(device) for b in batch]\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            logits = outputs.logits\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "    try:\n",
    "        precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "        recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "        f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    except:\n",
    "        print(\"Warning: Some classes may not have predictions. Using only accuracy.\")\n",
    "        precision = recall = f1 = None\n",
    "\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    idx_to_label = {i: chr(65 + i) for i in range(5)}\n",
    "\n",
    "    class_accuracies = {}\n",
    "    for i in range(5):\n",
    "        class_indices = np.where(np.array(true_labels) == i)[0]\n",
    "        if len(class_indices) > 0:\n",
    "            class_correct = sum([predicted_labels[j] == i for j in class_indices])\n",
    "            class_accuracies[idx_to_label[i]] = class_correct / len(class_indices)\n",
    "        else:\n",
    "            class_accuracies[idx_to_label[i]] = 0\n",
    "\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Overall Precision: {precision:.4f}\")\n",
    "    print(f\"Overall Recall: {recall:.4f}\")\n",
    "    print(f\"Overall F1-Score: {f1:.4f}\")\n",
    "    print(\"Per-class accuracy:\")\n",
    "    for label, acc in class_accuracies.items():\n",
    "        print(f\"  Choice {label}: {acc:.4f}\")\n",
    "    print\n",
    "\n",
    "    if show_confusion_matrix:\n",
    "        labels = [idx_to_label[i] for i in range(5)]\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'per_class_accuracy': class_accuracies,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b2e2c1",
   "metadata": {},
   "source": [
    "This function `evaluate_deepseek` evaluates predictions made by the `DeepSeek-V2-Lite` model using a `results` dictionary of string-based labels. It provides accuracy, precision, recall, F1 score and a confusion matrix.\n",
    "\n",
    "Unlike `evaluate_bert_transformer`, which operates on a PyTorch model and dataloader, this function assumes that predictions and true labels are already collected as strings (e.g., `'A'`, `'B'`, ...) in a dictionary format.\n",
    "\n",
    "**Parameters**\n",
    "- `results` (`dict`): A dictionary with two keys:\n",
    "  - `'correct_answers'`: List of true labels as strings (e.g., `[\"A\", \"C\", ...]`)\n",
    "  - `'responses'`: List of predicted labels as strings.\n",
    "- `show_confusion_matrix` (`bool`, default=`True`): Whether to display a confusion matrix.\n",
    "- `save_path` (`str`, default=`\"deepseek_confusion_matrix.png\"`): Where to save the matrix plot, if displayed.\n",
    "\n",
    "**Output**<br>\n",
    "Returns a dictionary with:\n",
    "- `'accuracy'`: Overall classification accuracy.\n",
    "- `'precision'`: Weighted precision.\n",
    "- `'recall'`: Weighted recall.\n",
    "- `'f1_score'`: Weighted F1 score.\n",
    "- `'confusion_matrix'`: Raw confusion matrix as a NumPy array.\n",
    "- `'per_class_accuracy'`: Accuracy per label (A–E) as a dictionary.\n",
    "\n",
    "**Notes**\n",
    "- Uses a hardcoded 5-class mapping: `'A'` → `0`, ..., `'E'` → `4`.\n",
    "- Invalid predictions (not in `'A'-'E'`) are defaulted to label `0` (`'A'`).\n",
    "- Applies the same metrics and structure as `evaluate_bert_transformer` for consistency.\n",
    "- Handles label imbalance using weighted metrics.\n",
    "- Displays a confusion matrix with class labels A–E and optionally saves it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5864b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_deepseek(results, show_confusion_matrix=True, save_path=\"deepseek_confusion_matrix.png\"):\n",
    "    label_to_idx = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
    "    idx_to_label = {i: chr(65 + i) for i in range(5)}\n",
    "    \n",
    "    true_labels = [label_to_idx[label] for label in results['correct_answers']]\n",
    "    predicted_labels = [label_to_idx[label] if label in label_to_idx else 0 for label in results['responses']]\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    \n",
    "    try:\n",
    "        precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "        recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "    except:\n",
    "        print(\"Warning: Some classes may not have predictions. Using only accuracy.\")\n",
    "        precision = recall = f1 = None\n",
    "    \n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    \n",
    "    class_accuracies = {}\n",
    "    for i in range(5):\n",
    "        class_indices = np.where(np.array(true_labels) == i)[0]\n",
    "        if len(class_indices) > 0:\n",
    "            class_correct = sum([predicted_labels[j] == i for j in class_indices])\n",
    "            class_accuracies[idx_to_label[i]] = class_correct / len(class_indices)\n",
    "        else:\n",
    "            class_accuracies[idx_to_label[i]] = 0\n",
    "    \n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    if precision is not None:\n",
    "        print(f\"Overall Precision: {precision:.4f}\")\n",
    "        print(f\"Overall Recall: {recall:.4f}\")\n",
    "        print(f\"Overall F1-Score: {f1:.4f}\")\n",
    "    print(\"Per-class accuracy:\")\n",
    "    for label, acc in class_accuracies.items():\n",
    "        print(f\"  Choice {label}: {acc:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    if show_confusion_matrix:\n",
    "        labels = [idx_to_label[i] for i in range(5)]\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.title('Confusion Matrix - DeepSeek-V2-Lite')\n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300)\n",
    "            print(f\"Confusion matrix saved to {save_path}\")\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'per_class_accuracy': class_accuracies,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733a8f3c",
   "metadata": {},
   "source": [
    "Before evaluating, I delete all the models to speed up the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22275809",
   "metadata": {},
   "outputs": [],
   "source": [
    "del pretrained_bert_model\n",
    "del random_bert_model\n",
    "del deepseek_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3a2154",
   "metadata": {},
   "source": [
    "### Random Bert Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0182dc6c",
   "metadata": {},
   "source": [
    "I initialize the random bert model again, to start with the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2b1c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_bert = BertConfig.from_pretrained('bert-base-cased')\n",
    "random_bert_model = BertForMultipleChoice(config_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfbf3d7",
   "metadata": {},
   "source": [
    "The next code cell does the following steps:\n",
    "1. Loads the model weights from a saved checkpoint.\n",
    "2. Moves the model to the specified `device` (e.g., GPU or CPU).\n",
    "3. Sets the model to evaluation mode to disable dropout and gradient tracking.\n",
    "4. Evaluates the model on the test dataset using the `evaluate_bert_transformer` function, saving the confusion matrix as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c226f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if activate_random_bert_evaluation == True:\n",
    "    best_random_model_path = \"./checkpoints/sweep-t666b3vz-2025-05-15_00-57-35/best_random_transformer_model.pt\"\n",
    "    random_bert_model.load_state_dict(torch.load(best_random_model_path))\n",
    "    random_bert_model.to(device)\n",
    "    random_bert_model.eval()\n",
    "    test_results_random_model = evaluate_bert_transformer(random_bert_model, test_dataloader, device, save_path=\"random_bert_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c5bda",
   "metadata": {},
   "source": [
    "After the evaluation is done for the random bert model, I delete it again to speed up the evaluation for the next two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9357a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del random_bert_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd87af5b",
   "metadata": {},
   "source": [
    "### Pretrained Bert Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db348fbb",
   "metadata": {},
   "source": [
    "I initialize the pretrained bert model again, to start with the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f76ca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_bert_model = BertForMultipleChoice.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8898fc6",
   "metadata": {},
   "source": [
    "The next code cell does the following steps:\n",
    "1. Loads the model weights from a saved checkpoint.\n",
    "2. Moves the model to the specified `device` (e.g., GPU or CPU).\n",
    "3. Sets the model to evaluation mode to disable dropout and gradient tracking.\n",
    "4. Evaluates the model on the test dataset using the `evaluate_bert_transformer` function, saving the confusion matrix as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e005aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if activate_pretrained_bert_evaluation == True:\n",
    "    best_pretrained_model_path = \"./checkpoints/sweep-ui8fb374-2025-05-14_01-51-35/best_pretrained_transformer_model.pt\"\n",
    "    pretrained_bert_model.load_state_dict(torch.load(best_pretrained_model_path))\n",
    "    pretrained_bert_model.to(device)\n",
    "    pretrained_bert_model.eval()\n",
    "    test_results_pretrained_model = evaluate_bert_transformer(pretrained_bert_model, test_dataloader, device, save_path=\"pretrained_bert_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d187c48",
   "metadata": {},
   "source": [
    "After the evaluation is done for the pretrained bert model, I delete it again to speed up the evaluation for the next model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b4a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "del pretrained_bert_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566dd402",
   "metadata": {},
   "source": [
    "### Deepseek-V2-Lite Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8723ab9",
   "metadata": {},
   "source": [
    "I initialize the deepseek model again, to start with the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d920a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_skip_modules=None,\n",
    "    llm_int8_enable_fp32_cpu_offload=False\n",
    ")\n",
    "\n",
    "deepseek_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-V2-Lite\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    trust_remote_code=True\n",
    ")\n",
    "deepseek_model.generation_config = GenerationConfig.from_pretrained('deepseek-ai/DeepSeek-V2-Lite')\n",
    "deepseek_model.generation_config.pad_token_id = deepseek_model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f39f23f",
   "metadata": {},
   "source": [
    "Next, I run the `Deepseek-V2-Lite` model through the `process_commonsense_qa_for_deepseek` to evaluate the accuracy on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948de111",
   "metadata": {},
   "outputs": [],
   "source": [
    "if activate_deepseek_evaluation == True:\n",
    "    test_results_deepseek = process_commonsense_qa_for_deepseek(test, deepseek_model, tokenizer_deepseek, num_examples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246466f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = evaluate_deepseek(test_results_deepseek)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb70e7f",
   "metadata": {},
   "source": [
    "After the evaluation of this model is also done, I delete it again to clean up the memory and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a65179",
   "metadata": {},
   "outputs": [],
   "source": [
    "del deepseek_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83e495",
   "metadata": {},
   "source": [
    "# **Interpretation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2942b9",
   "metadata": {},
   "source": [
    "Before the project, my expectations were much higher than for the previous project, which resulted in an accuracy of around 23% (just slightly above random chance). I anticipated that with transformer model architectures and better tuning of hyperparameters, I would achieve noticeably higher accuracy this time. Specifically, I expected the randomly initialized transformer to perform slightly better than the previous RNN or word embedding. I also believed that the pretrained transformer would reach a solid accuracy, possibly around 60%, while the implemented large language model (LLM) might push accuracy even further, potentially up to 80%, due to the fact that my model had ~16B parameters. However, I was also aware that the complexity of the architecture itself could pose a significant challenge. Overall, I aimed for a meaningful improvement over the previous baseline and hoped to gain deeper insights into model behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57f2f5f",
   "metadata": {},
   "source": [
    "I initially expected the <u>**randomly initialized**</u> transformer to perform slightly better than random guessing, given that transformers are powerful architectures capable of learning complex patterns. However, the results showed a steady accuracy below 20%, which was surprisingly poor. This likely happened because training a transformer from scratch requires large amounts of data and careful tuning, which I did not have in this project. Without pretrained weights, the model had to learn both low-level and high-level features entirely from the limited dataset, which is a very challenging task. The architecture’s complexity might also have contributed to unstable training dynamics and slower convergence, resulting in suboptimal learning and poorer-than-random performance. Essentially, the model struggled to extract meaningful representations within the limited training time and data, leading to underwhelming accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866f156e",
   "metadata": {},
   "source": [
    "For the <u>**pretrained transformer**</u>, my expectations were mostly met, as it achieved a solid accuracy of around 54%. This outcome aligns well with what is commonly observed when using pretrained models: they start with rich, general-purpose language representations learned from massive corpora, which allow them to adapt more quickly and effectively to downstream tasks. The pretrained weights give the model a strong head start, making it easier to fine-tune on smaller datasets and still achieve reasonably good performance. Although the accuracy was slightly below my initial estimate of 60%, it still demonstrates the power of transfer learning. I expected this because models like BERT are commonly used as a strong starting point in many language tasks and usually perform well. The good results from the pretrained model show how helpful it is to build on what the model has already learned from a lot of text, especially when you don’t have a lot of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff5b564",
   "metadata": {},
   "source": [
    "I expected the implemented <u>**large language model**</u> to reach a way higher accuracy, maybe around 80%, given its advanced architecture and training on vast datasets. However, the results were surprising and did not meet this optimistic estimate. Since I didn’t train the LLM but only used prompting to extract answers, its performance heavily depended on how well the prompt was designed and how suited the model was to this specific task. Factors like prompt wording, task complexity and domain mismatch likely limited its effectiveness. I struggled a lot with the bias of the LLM towards the first answer (A). I tried different methods, like increasing the few shot examples, maximum token length and even adjusting the parameters for the `generate` function. But nothing really seemed to help, it just made the bias bigger or even This experience highlights that even powerful models can have practical limitations when used off-the-shelf without fine-tuning or careful adaptation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363fea4",
   "metadata": {},
   "source": [
    "# **Tools used**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0bf1a1",
   "metadata": {},
   "source": [
    "### **Adjust this section before submitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8479f8",
   "metadata": {},
   "source": [
    "1. **Programming Environment**\n",
    "   - Python 3.12.8\n",
    "   - Jupyter Notebook\n",
    "\n",
    "2. **Machine Learning and Deep Learning**\n",
    "   - PyTorch (neural network development)\n",
    "   - Hugging Face Datasets (data management)\n",
    "   - NLTK (natural language preprocessing)\n",
    "   - FastText (pre-trained word embeddings, 300-dimensional vectors)\n",
    "\n",
    "3. **Data Manipulation and Analysis**\n",
    "   - NumPy (numerical computing)\n",
    "   - Pandas (data structuring and manipulation)\n",
    "   - Scikit-learn (potential additional machine learning utilities)\n",
    "\n",
    "4. **Visualization and Tracking**\n",
    "   - Matplotlib (basic plotting)\n",
    "   - Seaborn (statistical data visualization)\n",
    "   - Weights & Biases (experiment tracking and logging)\n",
    "     * Tracked metrics: training loss, accuracy, learning rates\n",
    "     * Logged hyperparameter configurations\n",
    "     * Enabled comparative analysis across model runs\n",
    "\n",
    "5. **Computational Infrastructure**\n",
    "   - CUDA-enabled GPU acceleration\n",
    "   - GPU-optimized PyTorch operations\n",
    "   - Efficient parallel computing for model training\n",
    "\n",
    "6. **Dataset**\n",
    "   - CommonsenseQA dataset (Hugging Face)\n",
    "\n",
    "7. **Additional Libraries**\n",
    "   - Gensim (word vector processing)\n",
    "   - tqdm (progress bar visualization)\n",
    "   - datetime (experiment timestamping)\n",
    "   - os (file and path handling)\n",
    "   - gc (manual garbage collection)\n",
    "   - re (regular expressions)\n",
    "   - random (random number generation and sampling)\n",
    "\n",
    "8. **AI-Tools**\n",
    "   - Claude 3.5 Sonnet: Utilized as a coding assistant for debugging, optimization and documentation.\n",
    "   - GPT-4-turbo: Assisted in drafting and refining documentation, helping with structure and phrasing.\n",
    "   - Copilot: Used for quick inserts, when recommendation was suitable for what I was planning to do.\n",
    "\n",
    "9. **Sources**\n",
    "   - CommonsenseQA dataset: https://huggingface.co/datasets/tau/commonsense_qa\n",
    "   - Transformer architecture: https://medium.com/data-science/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\n",
    "   - Deepseek implementation: https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite\n",
    "   - Medium blog for warmup steps: https://medium.com/better-ml/the-art-of-setting-learning-rate-eff11ac0a737"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
