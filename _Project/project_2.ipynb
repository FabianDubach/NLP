{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4357e38e",
   "metadata": {},
   "source": [
    "## Step 1: Set Up the CommonsenseQA Dataset\n",
    "\n",
    "Download and prepare the CommonsenseQA dataset\n",
    "Split the data into train/validation/test sets if not already done\n",
    "Understand the format (questions, multiple-choice answers)\n",
    "\n",
    "## Step 2: Set Up Three Models\n",
    "\n",
    "Randomly Initialized Transformer\n",
    "\n",
    "Build a transformer architecture from scratch\n",
    "Initialize weights randomly\n",
    "This will serve as your baseline\n",
    "\n",
    "\n",
    "Pretrained Transformer\n",
    "\n",
    "Use the same transformer architecture as Model 1\n",
    "Initialize with pretrained weights (e.g., BERT, RoBERTa)\n",
    "Make sure this model wasn't specifically trained on CommonsenseQA\n",
    "\n",
    "\n",
    "Large Language Model (1B+ parameters)\n",
    "\n",
    "Choose an LLM (e.g., GPT-2, LLaMA, OPT, BLOOM)\n",
    "No finetuning for this model - just prompt engineering\n",
    "\n",
    "\n",
    "\n",
    "## Step 3: Training/Finetuning\n",
    "\n",
    "Finetune Models 1 & 2 on CommonsenseQA train set\n",
    "\n",
    "Use the same hyperparameters for both\n",
    "Train for multiple epochs\n",
    "Save checkpoints and track validation performance\n",
    "\n",
    "\n",
    "For Model 3 (LLM), develop effective prompts instead of finetuning\n",
    "\n",
    "## Step 4: Prompt Engineering (for LLM)\n",
    "\n",
    "Design different prompt formats\n",
    "Test various instruction styles\n",
    "Try few-shot examples in prompts\n",
    "Experiment with temperature and other generation parameters\n",
    "\n",
    "## Step 5: Evaluation\n",
    "\n",
    "Evaluate all three models on the test set\n",
    "Calculate accuracy, F1 score, or other relevant metrics\n",
    "Compare performance across models\n",
    "\n",
    "## Step 6: Analysis\n",
    "\n",
    "Analyze which types of questions each model handles well/poorly\n",
    "Look at error patterns\n",
    "Discuss why certain approaches work better\n",
    "\n",
    "## Step 7: Create Presentation\n",
    "\n",
    "Summarize methodology\n",
    "Present results with visualizations\n",
    "Include discussion of findings\n",
    "Provide limitations and potential improvements\n",
    "\n",
    "Technical Requirements:\n",
    "\n",
    "Programming language: Python recommended\n",
    "Libraries: PyTorch/TensorFlow, Transformers (Hugging Face), etc.\n",
    "Computational resources: You'll need GPU access for training\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "TODO: Checkpointing, Early stopping works?, Log to Wandb, sweeps or other auto tool (optional), llm\n",
    "\n",
    "**Delete steps generated by Claude later**\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5cc4f4",
   "metadata": {},
   "source": [
    "# **FS25 NLP Project 1: Word Embeddings/Recurrent Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52707bad",
   "metadata": {},
   "source": [
    "Fabian Dubach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee79ba7",
   "metadata": {},
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df05906a",
   "metadata": {},
   "source": [
    "<style>\n",
    "  .container {\n",
    "    display: flex;\n",
    "    align-items: flex-start;\n",
    "    gap: 20px; /* spacing between text and ASCII art */\n",
    "    font-family: monospace;\n",
    "  }\n",
    "  .text {\n",
    "    flex: 2;\n",
    "  }\n",
    "  .ascii {\n",
    "    white-space: pre;\n",
    "    font-size: 4.5px;\n",
    "    line-height: 1.2;\n",
    "    flex: 1;\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<div class=\"container\">\n",
    "  <div class=\"text\">\n",
    "    <p>The task for my project was to perform common sense question answering using the CommonsenseQA dataset.</p><br>\n",
    "    <p>I evaluated the performance of three different Transformer-based models:</p>\n",
    "    <p>1. A randomly initialized Transformer</p>\n",
    "    <p>2. A pretrained Transformer (with the same architecture as the first Transformer)</p>\n",
    "    <p>3. A large language model (LLM) with over 1 billion parameters</p><br>\n",
    "    <p>While the first two models were finetuned on the dataset using the same hyperparameters for a fair comparison, the LLM was evaluated through prompt engineering without additional training. This setup allowed me to explore how different levels of pretraining and model scale impact common sense reasoning performance.</p>\n",
    "    <p>We had to also track the trainings with Wandb (workspace URL: <a href=\"https://wandb.ai/fabian-dubach-hochschule-luzern/CommonsenseQA/workspace?nw=nwuserfabiandubach\" target=\"_blank\">https://wandb.ai/fabian-dubach-hochschule-luzern/CommonsenseQA/workspace?nw=nwuserfabiandubach</a>).</p>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"ascii\">\n",
    "<pre>\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣶⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⠀⠀⠀⠀⠀⣤⣤⣤⠀⠀⠀⠀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⠀⣠⡶⢿⡇⢿⣿⡏⢳⣦⠀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⡛⣆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣧⣼⣿⣴⣋⡽⠮⠿⢭⣟⣏⣷⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⣧⠘⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡼⣇⣿⡿⠶⣶⣿⣟⡛⣷⣿⢠⠙⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⡈⣏⠇⢹⡀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡟⢹⠁⣿⠋⠉⢹⠉⠙⣿⡇⣾⣀⣾⠀⢀⣤⡀⢀⡀⠀⠀⢀⣠⣴⣾⠛⢻⡛⢻⡄⢀⣳⡀⢀⣠⠄⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⣷⣾⢀⣿⡇⠀⠸⠀⠀⣿⣧⡽⠿⣟⣺⣭⠴⢿⡏⣩⣷⡾⢛⣭⣴⣿⣇⠘⣿⣷⣿⡛⠉⢻⣟⣷⠄⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠿⢿⣟⣿⣿⡦⣶⣪⡭⠿⣚⣫⣭⣽⣶⡄⠀⢸⡇⣿⡙⣿⣿⣿⣿⣿⣿⣆⠹⣿⣿⣷⡀⠀⢿⡉⠁⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⣤⣶⣿⠿⠛⣉⣭⣶⣾⣿⠿⠟⠛⠉⠉⢻⠀⢸⣷⣿⣇⢻⡿⣿⣿⣿⣿⠟⠀⠹⣿⣿⠃⠀⠘⣷⡀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣤⣦⣼⣿⠿⠛⣋⡁⣼⢠⣿⡿⠛⠉⠁⠀⠀⢀⡀⢀⣴⣾⠀⢸⣿⡇⢻⡄⠙⠿⠻⠛⠁⠀⢀⣠⣽⣿⣇⡀⠀⠸⣧⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⠿⣛⣭⣴⡾⠟⠛⣧⣿⢸⡿⠀⠀⠀⠀⣰⣿⣿⣷⣾⣿⣿⠀⢸⡏⣇⢸⣷⡀⠀⢀⣠⣴⣾⠿⠛⣿⢻⣿⣹⡀⠀⢻⣆⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣴⡟⣦⠀⠀⠀⢀⡿⣵⡿⠛⠉⣡⣶⣤⣄⣿⣯⢸⣇⠀⠀⢠⣾⣿⡿⣿⣿⣿⣿⡿⠀⢸⡇⢻⡼⣿⣷⣶⠿⠛⠉⠀⠀⠀⠸⡇⣿⣿⣧⠀⠘⣿⡀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⢹⠀⢀⣠⣼⣿⣿⠀⢀⣼⣿⣿⣿⣿⡇⣿⢸⣿⣀⣀⣿⡿⠿⠶⠚⠛⠉⠉⠀⠀⢸⡇⠀⢻⣾⣝⣿⡆⠀⢀⣠⡴⠖⠛⢻⡾⣿⣿⣆⠀⢹⡇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣇⣼⡾⠟⠋⣿⢻⣇⣤⣌⠻⢿⣿⣿⣿⠃⢿⠀⠉⠉⠁⠀⠀⠀⣀⣤⡤⠶⠶⠒⠚⣻⣷⣄⠈⣿⣿⣿⣿⡞⠉⠀⠀⠀⠀⠀⣿⢿⣿⣾⣋⣽⠇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣹⠏⠀⠀⠀⣿⢿⣿⣿⣯⡴⠾⠛⢋⣡⠶⠛⠛⠋⣉⣉⣉⣙⢻⣿⠀⠀⠀⠀⠀⢠⡟⠀⠈⠻⢦⣈⣿⣿⣧⠀⠀⢀⣠⣴⡾⢿⣿⣿⣿⣿⣿⡀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡟⣿⡟⠀⠀⠀⣿⠈⠋⠉⢀⣠⠴⣛⣩⣤⣶⣞⣭⣿⢿⣿⣿⣻⣼⣿⣆⣀⣤⣤⣴⣿⣄⣠⣶⣦⣀⣙⣿⣿⣿⡶⣿⠟⠋⣁⣶⠟⢻⣽⣿⣿⣿⠇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⢠⣿⣇⠀⠀⠀⢹⣠⡴⠖⢻⣷⢫⣿⣿⣿⣯⣿⣟⣿⣿⣭⣽⣿⡿⣿⣿⣿⠿⠿⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⣿⠋⠉⣿⠀⢸⣿⣿⣿⣿⣷⡀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣼⣿⣿⣤⣴⣾⢿⡅⠀⣀⣾⢿⣿⣿⣿⣿⣿⣿⡿⣿⣷⣿⣿⣿⡇⣿⣿⡇⠀⠀⢸⣿⣿⡟⢿⣿⣿⣿⣿⣿⣣⣿⠁⣿⣀⣤⡿⠀⢀⣿⣿⣿⣿⣿⡇⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⠻⣿⠛⠉⠀⠈⣿⠛⢽⣿⢻⣿⣿⢿⣿⣿⣿⡇⣿⠿⣶⣶⣚⣧⣿⣿⡇⠀⠀⣸⣿⣿⣿⣄⣈⢿⣿⢿⣷⣿⣿⠀⠉⠉⠀⠀⠀⠘⡇⣿⣿⣿⣿⡇⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⡀⣷⡆⠀⠀⠀⠸⣧⣻⣿⢸⣿⣿⡿⢿⣾⣻⡇⣿⣿⣿⣿⣿⣿⣿⠿⠷⠾⠛⠛⠿⢿⣿⣿⣿⣄⣿⠿⠋⢸⣿⠀⠀⠀⠀⠀⠀⠀⡇⣿⣿⣿⣿⣿⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣷⡇⣿⡇⠀⠀⠀⠀⣿⣿⣿⡾⢿⣿⣿⣿⣿⡶⠷⠾⠛⠛⠉⠁⢀⣠⠤⠴⠒⡆⢠⠀⢰⡉⠻⣿⣽⡏⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⠀⡇⣿⡿⣿⣿⣿⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣧⣿⠿⢀⣀⣤⣴⣿⣿⣿⡷⠾⠛⠋⠉⢀⣀⣠⠤⠴⠒⠻⡆⢸⠀⠀⢀⡠⠇⠸⡄⠈⣇⠀⠈⡻⢦⡀⠀⢸⡇⠀⠀⠀⠀⠀⠀⠀⡇⣿⣧⡘⠿⢻⡆\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠻⣆⣿⣿⣿⣿⣿⡿⠛⣉⣀⡀⣠⠴⠒⠋⠉⠁⠀⠀⠀⠀⠀⡇⢸⣠⠴⣫⡄⠀⠀⡇⠀⢹⠀⠀⣿⠦⢿⡀⢸⡇⠀⠀⣀⣤⣤⣿⠀⡇⣿⣿⣿⣆⢸⡇\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⢿⡟⣽⣿⠀⣏⠁⠀⡇⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣇⠀⡖⣻⠋⠀⠀⠈⢻⠀⢈⡇⠀⠸⡄⠘⣧⢸⡇⠀⢸⣷⣾⣿⠏⠀⡇⣿⣿⣿⣿⢸⡇\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣾⠏⠛⠋⢡⣿⠀⠸⣿⣟⡃⣇⠀⠀⠀⠀⠀⣀⣠⡤⠶⠒⠋⠀⠛⠁⠀⣀⣤⣶⣿⣿⣿⣿⣷⣤⡈⠁⢻⡞⣿⠀⠈⠻⣴⠏⠀⠀⠿⢹⣿⣎⢻⣿⡇\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣾⡟⠀⠀⢀⡿⣿⠀⠀⠈⠳⡇⠻⠤⠶⠚⠋⠉⠁⠀⠀⠀⠀⠀⣀⣤⣶⣿⣿⣿⣿⣿⠿⠛⠻⣿⣿⣿⣷⣜⣷⣿⠀⠀⢀⣀⣤⣤⣶⣾⣶⣿⣿⠃⢸⡇\n",
    "⠀⠀⠀⠀⠀⠀⣀⣤⡶⠶⠖⠚⢛⠛⠳⢶⣼⡟⠀⠀⢀⣼⣹⣿⢀⠀⠀⠀⠀⡀⠀⠀⠀⠀⠀⢀⣀⣠⡤⢤⣾⣿⣿⣿⡿⠿⠛⠉⠹⡇⠀⠀⣿⣿⣟⢿⣿⣿⠹⣶⣿⡿⠛⠻⣏⠀⠉⠉⡛⣿⡿⣾⡇\n",
    "⠀⠀⠀⢀⣴⠞⠋⢰⡇⢰⣿⢻⢻⢻⢶⣦⠙⣷⡀⠀⣸⢧⠟⢿⣿⣿⣿⣷⣶⣶⣤⣴⣲⡾⠿⠟⠒⠒⠛⡇⠙⣿⠉⠀⢧⠀⠀⠀⠀⣧⠀⠀⢸⣿⣿⡎⣿⠁⢀⣼⣏⢀⣠⣤⣸⣶⠀⠀⣿⣿⣿⠛⠁\n",
    "⠀⠀⠀⣾⠃⠀⣠⡬⣤⣼⣛⠾⣼⣞⡾⡟⠀⠘⣧⣠⣏⡞⠀⠈⠻⣿⡏⢹⡟⠛⠻⣿⠁⠀⠀⠀⠀⠀⠀⣇⠀⣿⠀⠀⢸⡄⠀⠀⠀⢸⠀⠀⠘⣿⣿⣇⣿⣴⡞⢣⣽⣿⣿⣿⣿⣿⠀⠀⣿⣿⡟⠀⠀\n",
    "⠀⠀⠀⣿⡶⣿⣿⣸⣿⣿⣿⠿⠷⠾⢽⣅⡲⠶⢻⣿⣼⢁⣠⣤⣶⣿⣿⠘⡇⠀⠀⢻⡆⠀⠀⠀⠀⠀⢀⣸⡀⢹⡇⠀⠈⡇⠀⠀⠀⠈⡇⠀⠀⢿⣿⣿⢹⣿⣤⣿⣿⣿⣿⡿⢿⣟⡀⠀⣿⣿⡇⠀⠀\n",
    "⠀⠀⠀⠈⠛⠿⢯⣜⣿⠏⠀⠀⠀⢀⡿⣨⣿⣶⣤⣿⣷⣯⣿⣿⣿⣿⣿⠀⡇⠀⠀⠐⡿⣦⣰⣒⣶⣿⣿⣿⣷⣾⣇⠀⠀⢻⠀⠀⠀⠀⢷⠀⠀⢸⣿⣿⣾⣿⣸⣿⡏⢠⠟⣠⣿⣿⣿⣦⡈⢹⡇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⢸⡟⣾⠄⠀⠀⣸⡇⣿⣿⣿⠟⠋⠛⢿⣿⣿⣿⣿⣿⡄⢻⠀⠀⠀⡇⠈⠙⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⢸⡆⠀⠀⠀⢸⡄⠀⠀⣿⣿⣇⣿⠛⠛⠻⣿⣺⣿⣿⣿⣿⣿⣿⡿⠃⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⣼⢧⡇⠀⠀⠀⣿⢸⣿⣿⡿⢦⣴⣿⣿⣷⡿⣿⡿⣿⡇⢸⡄⠀⠀⢹⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⡆⠀⠀⣇⠀⠀⠀⠀⣇⠀⠀⢸⣿⣟⢿⡀⠀⠀⠈⠉⠀⠉⠉⠉⠁⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⣿⣨⡧⠤⠤⢤⣇⡾⣿⣿⣠⣿⣿⣿⣿⣿⣿⣽⣿⣿⣷⠀⣇⠀⠀⢸⠀⠀⢸⢻⣿⣿⣿⣿⡇⣿⣿⠀⠀⢹⡄⠀⠀⢀⣸⠀⠀⠸⣿⣿⣼⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⢀⡿⣧⣤⠶⠦⣼⣿⣿⣿⡏⠈⣿⣿⢿⣿⣿⣿⣏⠉⢹⣿⡀⢻⠀⠀⠘⡇⠀⠸⡄⠙⢿⣿⣿⠇⣿⣿⡄⠀⠈⠓⠒⠋⠉⠀⠀⠀⠀⢿⠹⣯⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⣸⣿⢃⡏⠀⠀⢻⣿⣿⣽⣿⣦⠘⣿⣿⣿⣿⣿⢻⣿⣾⣿⡇⠘⡇⠀⠀⣇⠀⠀⣇⠀⠀⠙⢿⡇⣿⢸⣧⠀⠀⠀⠀⡴⠒⢶⠀⠀⠀⠘⣆⠀⢻⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⡿⡅⣸⢁⣄⡄⣾⣿⢿⣿⠿⣿⣿⢻⣿⣿⣟⣿⣸⣻⡿⣿⣧⠀⠙⠒⠛⠛⠀⠀⢿⣿⣄⠀⠀⠀⣿⠈⣿⡄⠀⠀⠀⡇⠀⠘⡇⠀⠀⠀⢿⣦⢸⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⢸⣧⡇⣿⣼⣿⠃⣿⣿⣾⣿⣷⣤⡿⠿⢿⣿⣿⣇⣿⡟⠋⠀⣿⡀⠀⣴⠲⡆⠀⠀⠸⣿⣿⣦⠀⠀⢸⡀⢹⣧⠀⠀⠀⣇⠀⠀⢹⠀⠀⠀⠸⣿⡟⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⢽⡿⣷⠏⠛⠿⢠⣿⣿⣿⣿⢿⣯⡇⠀⠀⠈⠁⠀⠀⠀⠀⠀⢸⣇⠀⢻⠀⢳⠀⠀⠀⣿⣿⣿⣷⣾⢸⡇⠈⣿⡀⠀⠀⢸⠀⠀⠈⡇⠀⠀⢀⣿⣿⣷⣀⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠘⣧⡙⣀⣀⣀⣸⣿⣽⣿⣿⠀⠈⠙⣶⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡀⢸⡀⠸⡄⠀⠀⢻⣿⣿⣿⣿⡼⡇⠀⢘⣧⣤⡴⠾⠷⠶⠖⠛⠛⢛⠋⠉⢿⢹⠉⣭⡿⠿⠷⠶⢦⡄⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠹⣟⣁⣸⣿⣿⣧⡿⠿⣿⣀⡀⠀⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣇⣈⣧⣘⣷⣤⣤⣼⠿⠿⣿⣿⣧⣧⡀⣸⢹⡏⠀⠀⠀⠀⠀⠀⠀⠈⡇⠀⢸⢸⡄⡿⠖⠚⠉⡉⠓⢿⡀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⣠⡴⣾⠋⠉⢙⣻⣷⠛⠛⠳⠶⠶⠽⠿⠃⠀⠀⠀⠀⠀⣀⡤⣼⡿⠋⠉⠁⠀⠀⣠⠀⣿⣿⠀⠀⠀⠀⠈⠉⠻⣿⢸⣷⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠸⡏⡇⣿⠀⠀⠀⢻⣷⢸⡇⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⡟⠀⡟⠀⠀⢸⣿⣿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⡥⢺⠏⡆⠀⠀⠀⠀⠀⡏⠀⡟⡇⠀⠀⠀⠀⠀⠀⢀⡇⢸⣿⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⡇⡇⢿⠀⠀⠀⢸⣿⡌⣷⠀⠀⠀⠀\n",
    "⠀⠀⠀⢸⠇⢠⡇⠀⠀⢰⣿⣯⣏⣻⡆⠀⠀⠀⠀⠀⠀⠀⠀⣸⠃⢀⡿⢸⡇⠀⠀⠀⠀⢠⡇⠀⡇⡇⠀⠀⠀⠀⠀⠀⢸⡇⢸⣿⡆⠀⠀⠀⠀⠀⠀⠀⣧⠀⠀⡇⢿⢸⠀⠀⠀⠈⣿⡇⢹⡀⠀⠀⠀\n",
    "⠀⠀⠀⡟⡄⣼⠀⠀⢀⣿⣿⣿⣿⣿⣷⠀⠀⠀⠀⠀⠀⠀⠀⣿⠀⢸⡇⣸⡇⠀⠀⠀⠀⢸⠁⢸⣷⡇⠀⠀⠀⠀⠀⠀⢸⡇⢸⣿⡇⠀⠀⠀⠀⠀⠀⠀⢻⠀⠀⢹⢸⣼⡀⠀⣀⣀⣿⣧⣸⡇⠀⠀⠀\n",
    "⠀⠀⢰⢧⣇⡏⠀⠀⣸⣿⠿⢭⣿⣿⡏⠀⠀⠀⠀⠀⠀⠀⢰⡏⠀⣿⠀⣿⡇⠀⠀⠀⠀⢸⠀⢸⢸⠁⠀⠀⠀⠀⠀⠀⢸⡇⢸⣿⣿⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⢸⢸⣿⡏⢉⣁⣤⣤⣄⢈⡇⠀⠀⠀\n",
    "⠀⠀⣼⢼⣿⠃⠀⠀⣿⣿⠀⢸⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⢸⡇⢠⡿⢰⣿⠃⠀⠀⠀⠀⣼⠀⢸⢸⠀⠀⠀⠀⠀⠀⠀⢸⡇⢸⢹⣸⣦⣤⣤⣤⣶⣶⣶⡿⠀⠀⢸⡄⡇⣧⣽⣿⣿⣿⡽⠟⠁⠀⠀⠀\n",
    "⠀⠀⢿⢻⡏⠀⠀⢰⣿⣿⣟⠛⢿⣿⡇⠀⠀⠀⠀⠀⠀⠀⢸⠗⣻⡇⢸⢹⣆⣀⣀⣀⣤⡏⠀⢸⢸⠀⠀⠀⠀⠀⠀⠀⢸⡇⢸⠈⠉⠉⠉⠉⠉⠉⠀⠀⠀⠀⠀⠈⡇⣿⠘⣿⣿⣿⣇⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⢸⠛⠤⢤⣤⣘⢺⣿⣿⣿⣿⡿⠃⠀⠀⠀⠀⠀⠀⠀⠸⢧⣿⠃⠘⠓⠛⠛⠛⠋⠉⠁⠀⢼⢸⠀⢰⡾⠿⠛⠛⠿⢿⡇⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⢸⠀⠙⣿⣿⣿⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⢘⣶⡶⠚⠿⢿⣿⣩⢿⢿⡏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣸⠀⢸⡇⠀⠀⠀⠀⣿⡇⡏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢷⢸⡀⠀⠈⠁⢸⡇⠀⠀⠀⠀⠀\n",
    "⠀⠀⣼⣹⠃⠀⢰⣷⢻⠁⠈⠛⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡟⣹⠀⢸⠃⠀⠀⠀⠀⣿⠇⠜⠀⣤⠶⠖⠛⠛⠋⠉⠉⢩⣿⡇⠀⢸⠸⡇⠀⠀⠀⠘⡇⠀⠀⠀⠀⠀\n",
    "⠀⢠⡟⠏⠀⠀⣾⣿⣼⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⡇⠀⢀⣴⠶⠞⠛⠛⣻⣷⠀⡏⣿⠀⢸⢀⣴⣷⣦⡀⣿⠇⡇⠀⡟⠀⣀⣀⣀⣀⣀⣀⣸⣿⡇⠀⢸⡆⡇⠀⠀⠀⠀⣷⠀⠀⠀⠀⠀\n",
    "⠀⣸⠇⠀⠀⢸⣿⡇⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⣾⣀⣀⣤⣤⣶⣿⡿⠀⡇⣿⠀⢸⣿⣿⣿⣫⣾⣿⠀⡇⢠⣟⣿⣿⣿⡿⠿⠿⠿⠿⠁⡇⠀⠈⡇⣷⢀⡀⠀⠀⢻⠀⠀⠀⠀⠀\n",
    "⠀⣿⡼⠀⠀⡟⣿⣷⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⣿⠀⢀⡟⡿⠿⠟⠛⠛⣃⡇⠀⡇⣿⠀⢸⣿⣿⣿⣿⣿⣿⡄⡇⢸⡇⠀⠀⠀⠀⠀⠀⠀⢰⣶⡇⠀⠀⣇⢹⣾⣿⠀⣰⢾⡆⠀⠀⠀⠀\n",
    "⢠⣿⡇⠀⢸⣷⣿⣹⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⠀⢸⡇⠀⠀⠀⠀⢰⣿⡇⠀⡇⣿⠀⣾⣿⣿⣿⣿⣿⣿⡃⡇⢸⣧⣤⣤⣴⣶⣶⣶⣶⣾⣿⡇⠀⠀⢿⢸⣿⣿⣾⣿⣸⡇⠀⠀⠀⠀\n",
    "⢸⢭⠥⠦⣬⣽⣧⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣸⣿⠀⢸⢵⣶⣾⣿⣿⣿⡿⡇⠀⡇⣿⠀⣿⣿⣿⣿⣿⣿⣿⡇⡇⢸⡏⠿⠟⠛⠛⠛⠛⠛⠛⣧⣷⠀⠀⢸⠀⣿⣿⣿⣿⠛⣇⠀⠀⠀⠀\n",
    "⢸⣸⠁⢠⣿⣿⣹⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⢸⠉⠉⠉⠁⠀⢠⣾⡇⠀⡇⣿⠀⣿⣿⣿⣿⣿⣿⣿⠇⡇⢸⡇⠀⣀⣀⣀⣀⣀⣀⣰⣿⣿⠀⠀⠸⠀⣿⣿⣿⣵⡇⣿⠀⠀⠀⠀\n",
    "⠘⣧⣰⠞⣞⣷⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⢸⣀⣀⣠⣤⣤⣼⣿⡇⠀⡇⣿⠀⢈⣭⣭⠭⠽⠭⣿⡇⡇⢸⣟⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡀⠀⠀⠀⢻⣟⣾⣿⣿⢻⠀⠀⠀⠀\n",
    "⠀⠈⠛⠛⠛⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⣿⠿⠿⠿⠿⠟⢛⣻⡇⠀⡇⢻⠀⢸⠁⠀⠀⠀⠀⣿⡇⡇⠸⡏⠉⠀⠀⠀⠀⠀⠀⠀⣼⣿⡇⠀⠀⠀⢸⣿⣿⣿⣿⢸⡆⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣇⠀⣿⣀⣤⣤⣤⣤⣼⣿⡇⠀⡇⢸⠀⢸⠀⣠⣶⣄⠀⣿⡇⣇⠀⡇⣴⣶⣶⣾⣿⣿⣿⣿⣿⣿⣇⣀⣂⠀⢸⣿⣿⣿⣿⣿⡇⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⡿⠴⠿⠿⠿⠿⠿⠿⠿⠿⠷⣦⡄⢸⠀⢸⣾⣿⣿⢟⣴⣿⣷⣼⠶⠗⠛⠛⠛⠛⠛⠛⠛⠋⠉⠉⠉⢉⡟⣧⠈⣿⣿⣿⣿⡿⣧⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣴⣿⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⣴⣿⡇⢸⣴⢾⣿⡿⣻⣿⣿⣿⣿⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⣿⣿⡇⢸⣿⢸⣿⣿⣿⣿⣿⣿⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⣿⣿⡀⣿⣿⣿⣿⣿⣿⡄⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣿⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣾⣿⣿⣿⡇⢸⣿⢸⣿⣿⣿⣿⣿⠃⠀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣿⣿⣿⡇⢸⣿⣿⣿⣿⢻⡇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣿⣷⣶⣶⣶⣶⣶⣶⣶⡶⠶⠦⠤⣾⣿⣿⣿⣿⣷⢘⣿⢸⣿⣿⣿⣿⡏⣭⠭⠭⠭⠤⠤⠤⠴⠶⠶⠶⠶⠶⠶⠶⠱⣌⢻⣿⣧⢸⣿⣿⣿⣿⣾⣇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⡾⠟⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⣉⣽⣿⣾⣿⣿⣿⣿⣿⠀⣿⢸⣿⣿⣿⡟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⡞⣿⢻⠈⣿⣿⣿⣿⣿⣿⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣶⠟⠋⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠⣴⣾⣿⣿⣿⣿⣿⣿⣿⠛⢹⠀⣿⣾⣿⣿⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⣿⣿⣿⢻⣿⡀⣿⣿⣿⣿⣿⣿⡄⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣾⣿⣀⣤⣄⣤⣤⣄⣀⣀⣀⣀⣶⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣅⢸⠀⣿⡿⣿⣿⣤⣤⣤⡤⠤⠤⠶⠶⠶⠖⠒⠒⠒⠚⠛⠛⠛⠺⣿⣿⣿⡇⠹⡇⣿⣿⣿⣿⣿⣿⡇⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⣸⣿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⣿⣿⣿⣿⡟⠉⢹⣿⣿⣿⣿⡿⠿⡾⠀⣿⡇⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⣿⠰⠇⣿⣿⣿⣿⡿⣿⡇⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⢿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡟⠛⠉⠁⠀⠀⠀⠙⠛⠉⠁⠀⠀⠁⠀⣛⣁⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⠟⣹⡇⢀⣙⣿⣯⡷⠿⠛⠁⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠉⠉⠉⠉⠉⠉⠹⠷⣦⣤⣤⣤⣤⣤⣤⣤⣤⣤⣶⣶⣶⡶⠶⠶⠶⠶⠾⠿⠛⠛⠋⠉⠉⠁⠀⠀\n",
    "</pre>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48b5c1",
   "metadata": {},
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d849338",
   "metadata": {},
   "source": [
    "Import all libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a12ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMultipleChoice,\n",
    "    BertTokenizer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    GenerationConfig,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d266c9",
   "metadata": {},
   "source": [
    "Setup random seed function to ensure reproducibility.\n",
    "\n",
    "_Info about the seed value: The field of natural language processing began in the 1940s, after World War II. At this time, people recognized the importance of translation from one language to another and hoped to create a machine that could do this sort of translation automatically. → Seed value is mostly set to 42_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a7ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1940\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a438be37",
   "metadata": {},
   "source": [
    "In the next step I import and split the dataset. For the split I take off the last 1000 entries from the train-split and use it as validation, the rest of this is of course used for the training. Then I use the validation-part as the test, since the real test-split has no answer keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22c7f692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8741 1000 1221\n"
     ]
    }
   ],
   "source": [
    "train = load_dataset(\"tau/commonsense_qa\", split=\"train[:-1000]\")\n",
    "valid = load_dataset(\"tau/commonsense_qa\", split=\"train[-1000:]\")\n",
    "test = load_dataset(\"tau/commonsense_qa\", split=\"validation\")\n",
    "\n",
    "print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff2820",
   "metadata": {},
   "source": [
    "Login for the experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "492738f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfabian-dubach\u001b[0m (\u001b[33mfabian-dubach-hochschule-luzern\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f8b6b6",
   "metadata": {},
   "source": [
    "# **Data Exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[4m\" + \"Dataset Features\" + \"\\033[0m\")\n",
    "for feature in train.features:\n",
    "    print(feature)\n",
    "print(\"\\n\" + \"\\033[4m\" + \"Example\" + \"\\033[0m\")\n",
    "for feature in train.features:\n",
    "    print(feature + \":\", train[2][str(feature)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec5d2d0",
   "metadata": {},
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d03e9f7",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c9643",
   "metadata": {},
   "source": [
    "During the preprocessing phase of my NLP project, I carefully considered several common text-cleaning and preparation techniques. Below is a breakdown of each step, whether I used it, and the reasoning behind my decision.\n",
    "\n",
    "1. **Tokenization**  \n",
    "   ✅ *Used*  \n",
    "   I used the `BertTokenizer` from Hugging Face to tokenize all text inputs. This tokenizer breaks text into subword units and adds special tokens, ensuring compatibility with the BERT model architecture.\n",
    "\n",
    "2. **Lowercasing, Stemming, Lemmatizing, Stopword/Punctuation Removal**  \n",
    "   ❌ *Not used*  \n",
    "   These steps are common in traditional NLP pipelines but not necessary when using a pre-trained transformer like BERT. I specifically used the `'bert-base-cased'` model, which is sensitive to letter casing. Applying lowercasing or stripping punctuation could disrupt the model's understanding of context. Similarly, stemming or lemmatizing would interfere with subword tokenization, which already handles morphological variations effectively.\n",
    "\n",
    "3. **Removal of Unknown/Other Words**  \n",
    "   ❌ *Not explicitly used*  \n",
    "   Instead of manually removing unknown words, I relied on the tokenizer to handle them. Words not in the vocabulary are broken into subword tokens or mapped to the `[UNK]` token if completely unrecognized. BERT is designed to handle such cases gracefully.\n",
    "\n",
    "4. **Format Cleaning (e.g., HTML-extracted text)**  \n",
    "   ✅ *Used when necessary*  \n",
    "   While my dataset (CommonsenseQA) was fairly clean, I included basic text normalization steps to remove potential noise (e.g., HTML entities) as a precaution in other stages of the pipeline.\n",
    "\n",
    "5. **Truncation**  \n",
    "   ✅ *Used*  \n",
    "   To fit input sequences into BERT's maximum input size constraint, I applied truncation during tokenization. This ensures that long question-choice pairs are trimmed to 128 tokens, which balances performance and memory usage.\n",
    "\n",
    "6. **Feature Selection**  \n",
    "   ✅ *Used implicitly*  \n",
    "   Rather than traditional feature engineering, I relied on the tokenized outputs (`input_ids`, `attention_mask`, `token_type_ids`) generated by the tokenizer. These features are optimized for transformer models and encapsulate the essential linguistic structure needed for training.\n",
    "\n",
    "By tailoring preprocessing to suit BERT’s architecture, I avoided redundant or harmful steps while retaining the ones critical for accurate and efficient model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bed1da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76239e3c",
   "metadata": {},
   "source": [
    "The BertTokenizer is a class from the Hugging Face `transformers` library that handles the conversion of raw text into tokens that BERT can understand. Specifically, it tokenizes the input text into subword tokens (e.g., \"playing\" becomes [\"play\", \"##ing\"]). This subword tokenization allows the model to process both common and out-of-vocabulary words more effectively. The `from_pretrained('bert-base-cased')` method loads a pre-trained tokenizer that corresponds to the BERT model. The `'bert-base-cased'` model refers to a base-sized BERT model (with 12 layers and 768 hidden units) that has been trained on cased text, meaning it differentiates between uppercase and lowercase letters which is important for distinguishing meaning in proper nouns or acronyms (e.g., “US” vs “us”).\n",
    "\n",
    "**Why I use BertTokenizer:** I use the BertTokenizer to ensure that the text is processed in the exact way BERT was originally trained. \n",
    "\n",
    "**The tokenizer will:** \n",
    "- Split the text into subword tokens. \n",
    "- Add special tokens such as `[CLS]` and `[SEP]` that BERT requires. \n",
    "- Handle padding and truncation to ensure the input is the correct length for the model.\n",
    "\n",
    "I use the following line of code to initialize the BERT tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18589408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0449f7",
   "metadata": {},
   "source": [
    "The `preprocess_commonsenseqa` function is designed to preprocess the CommonsenseQA dataset for input into a BERT-based model. The goal is to tokenize the questions and their corresponding multiple-choice answers into a format compatible with BERT, and then convert the correct answer's label into a numerical value.\n",
    "\n",
    "1. **Extracting Questions and Choices:** The function starts by extracting the questions and their multiple-choice options from the `examples` object.\n",
    "\n",
    "2. **Initialize Data Structures:** It then initializes empty lists to hold the tokenized inputs, attention masks, and token type IDs (used for differentiating between sentence pairs in models like BERT).\n",
    "\n",
    "3. **Convert Answer Labels to Indices:** The answer choices are labeled with letters (A, B, C, D, E), but the model requires numerical labels. This step converts the letters into indices (A → 0, B → 1, etc.).\n",
    "\n",
    "4. **Processing Each Question-Choice Pair:** For each question and its corresponding choices: Each choice is paired with the question. Both the question and the choice are tokenized using the BERT tokenizer (`tokenizer_bert`), which converts the text into `input_ids`, `attention_mask` and `token_type_ids` tensors that BERT can understand.\n",
    "\n",
    "5. **Stacking Tokens for Each Choice:** After tokenizing each choice for a question, the function stacks the resulting tensors (for all choices) into single tensors for input to the model.\n",
    "\n",
    "6. **Returning Tokenized Data:** Finally, the function returns a dictionary containing the tokenized inputs (`input_ids`, `attention_mask` and `token_type_ids`) along with the numerical labels corresponding to the correct answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd469731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_commonsenseqa(examples):\n",
    "\n",
    "    questions = [q for q in examples['question']]\n",
    "    \n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_token_type_ids = []\n",
    "    \n",
    "    answerkeys = examples['answerKey']\n",
    "    labels = []\n",
    "    \n",
    "    for key in answerkeys:\n",
    "        labels.append(ord(key) - ord('A'))\n",
    "    \n",
    "    for i, (question, choices) in enumerate(zip(questions, examples['choices'])):\n",
    "        inputs = []\n",
    "\n",
    "        for choice in choices['text']:\n",
    "            text_a = question\n",
    "            text_b = choice\n",
    "            \n",
    "            encoded = tokenizer_bert(\n",
    "                text_a, text_b,\n",
    "                add_special_tokens=True,\n",
    "                max_length=128,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            inputs.append({\n",
    "                'input_ids': encoded['input_ids'],\n",
    "                'attention_mask': encoded['attention_mask'],\n",
    "                'token_type_ids': encoded['token_type_ids']\n",
    "            })\n",
    "        \n",
    "        input_ids = torch.cat([x['input_ids'] for x in inputs])\n",
    "        attention_mask = torch.cat([x['attention_mask'] for x in inputs])\n",
    "        token_type_ids = torch.cat([x['token_type_ids'] for x in inputs])\n",
    "        \n",
    "        all_input_ids.append(input_ids)\n",
    "        all_attention_mask.append(attention_mask)\n",
    "        all_token_type_ids.append(token_type_ids)\n",
    "\n",
    "    return {\n",
    "        'input_ids': all_input_ids,\n",
    "        'attention_mask': all_attention_mask,\n",
    "        'token_type_ids': all_token_type_ids,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01af1037",
   "metadata": {},
   "source": [
    "Next, I apply the preprocessing to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a49eb21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = preprocess_commonsenseqa(train)\n",
    "validation_dataset = preprocess_commonsenseqa(valid)\n",
    "test_dataset = preprocess_commonsenseqa(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b832460a",
   "metadata": {},
   "source": [
    "After preprocessing the CommonsenseQA dataset into tokenized inputs and labels, I convert the data into PyTorch `TensorDataset` objects. I do this because this groups all of the input tensors, so they can be iterated over together. It seamlessly integrates with PyTorch’s `DataLoader` for tasks like batching, shuffling and parallel data loading, ensuring that the data pipeline runs efficiently. Additionally, it provides synchronized indexing, ensuring that each input tensor corresponds correctly to its label, making the dataset ready for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9181ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = TensorDataset(\n",
    "    torch.stack(train_dataset['input_ids']),\n",
    "    torch.stack(train_dataset['attention_mask']),\n",
    "    torch.stack(train_dataset['token_type_ids']),\n",
    "    torch.tensor(train_dataset['labels'])\n",
    ")\n",
    "\n",
    "val_features = TensorDataset(\n",
    "    torch.stack(validation_dataset['input_ids']),\n",
    "    torch.stack(validation_dataset['attention_mask']),\n",
    "    torch.stack(validation_dataset['token_type_ids']),\n",
    "    torch.tensor(validation_dataset['labels'])\n",
    ")\n",
    "\n",
    "test_features = TensorDataset(\n",
    "    torch.stack(test_dataset['input_ids']),\n",
    "    torch.stack(test_dataset['attention_mask']),\n",
    "    torch.stack(test_dataset['token_type_ids']),\n",
    "    torch.tensor(test_dataset['labels'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20912123",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2f857d",
   "metadata": {},
   "source": [
    "For tokenizing my LLM I used the AutoTokenizer from Hugging Face to load the tokenizer for the DeepSeek-V2-Lite model. This tokenizer is specifically designed to be compatible with the DeepSeek model architecture. Since DeepSeek is a large language model (LLM), it expects input in a specific tokenized format, including proper handling of special tokens, padding and prompt formatting. By using the `AutoTokenizer` and loading the tokenizer directly from the model’s Hugging Face repository, I ensure that the text is processed exactly as the model was trained on.\n",
    "\n",
    "The `trust_remote_code=True` argument is necessary because DeepSeek uses custom model/tokenizer code not yet fully integrated into the standard Transformers library. This option allows the tokenizer to load correctly and function as intended.\n",
    "\n",
    "In short, I use this tokenizer to guarantee consistency between my input prompts and the expectations of the DeepSeek model, which is crucial for generating accurate and meaningful responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ffc6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_deepseek = AutoTokenizer.from_pretrained('deepseek-ai/DeepSeek-V2-Lite', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399fe1e",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04cdf8d",
   "metadata": {},
   "source": [
    "To see the number of parameters for my models a bit better, I first implemented a function which adds an apostrophe after every three digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29092cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_number(num):\n",
    "    return f\"{num:,}\".replace(\",\", \"'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bfa7cb",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f5dbfa",
   "metadata": {},
   "source": [
    "For the random initialized and the pretrained transformer architechture, I used BERT (Bidirectional Encoder Representations from Transformers) with a classification head specifically designed for multiple-choice inputs called `BertForMultipleChoice`. I used the Hugging Face checkpoint `bert-base-cased`, which is a pretrained transformer model developed by Google with **108'311'041 parameters**. This variant is trained on large English corpora (BooksCorpus and English Wikipedia) and maintains case sensitivity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b8d948",
   "metadata": {},
   "source": [
    "BERT is well-suited for classification tasks like CommonsenseQA due to its deep bidirectional attention, which helps capture the nuanced relationships between the question and each answer option. The pretrained bert-base-cased weights provide strong language understanding out of the box, significantly improving performance over training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f0c030",
   "metadata": {},
   "source": [
    "The BERT model is composed of an embedding layer, encoder, pooling layer, dropout layer and of course a classifier:\n",
    "\n",
    "1. **Embedding Layer (`BertEmbeddings`):**\n",
    "- Word Embeddings: `Embedding(28996, 768)` → Maps each token to a 768-dimensional vector. The vocabulary size is 28,996 tokens.\n",
    "- Position Embeddings: `Embedding(512, 768)` → Adds position information to each token, allowing the model to distinguish word order up to 512 tokens.\n",
    "- Token Type Embeddings: `Embedding(2, 768)` → Distinguishes between sentence pairs (e.g., question vs. answer).\n",
    "- Layer Normalization + Dropout: Normalizes embeddings and applies dropout (`p=0.1`) for regularization.\n",
    "\n",
    "2. **Encoder (`BertEncoder`):**\n",
    "- 12 Transformer Layers (stacked) → Each layer includes:\n",
    "    - Multi-Head Self-Attention (BertSelfAttention)\n",
    "        - Projects inputs into queries, keys and values using linear layers.\n",
    "        - Attention mechanism allows each token to attend to all others.\n",
    "        - Output passed through a linear layer, then dropout + layer norm.\n",
    "    - Feed-Forward Network\n",
    "        - First Linear: `768 → 3072`\n",
    "        - GELU activation\n",
    "        - Second Linear: `3072 → 768`\n",
    "        - Followed by LayerNorm and Dropout.\n",
    "- Each of these layers processes the tokenized question-choice pair, allowing the model to capture deep contextual relationships.\n",
    "\n",
    "3. **Pooling Layer (`BertPooler`):**\n",
    "- Extracts the `[CLS]` token output from the final encoder layer.\n",
    "- Applies a linear layer + `tanh` activation to produce a fixed-size sentence representation.\n",
    "\n",
    "4. **Dropout Layer:**\n",
    "- Applied before classification to reduce overfitting (`p=0.1`).\n",
    "\n",
    "5. **Classifier (`Linear(768 → 1)`):**\n",
    "- For each choice, outputs a single logit.\n",
    "- During training/evaluation, logits for all choices are grouped and passed through softmax to compute the predicted answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6b1756",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361fb529",
   "metadata": {},
   "source": [
    "In the following code block I load the configuration of the `'bert-base-cased'` model, but without the model weights. I just load the architecture details like number of layers, hidden size and so on. After I create a `BertForMultipleChoice` model using that configuration. This model is randomly initialized, meaining it hasn't learned anything yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84eac71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_bert = BertConfig.from_pretrained('bert-base-cased')\n",
    "random_bert_model = BertForMultipleChoice(config_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2012b6e5",
   "metadata": {},
   "source": [
    "Next, I calculate and print the total number of parameters of the `random_bert_model` and then print its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7d9b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of parameters: {format_number(sum(p.numel() for p in random_bert_model.parameters()))}\\n\")\n",
    "print(random_bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec07ade",
   "metadata": {},
   "source": [
    "In the next cell I create a `BertForMultipleChoice` model with already pretrained weights. These pretrained weights allow my model to already understand some word meanings, grammar and general language patterns. This helps the model perform better and converge faster when fine-tuned on my specific downstream task, such as multiple-choice question answering with CommonsenseQA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f741a679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pretrained_bert_model = BertForMultipleChoice.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c4b83",
   "metadata": {},
   "source": [
    "I calculate and print the total number of parameters of the `pretrained_bert_model` and then print its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f91a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of parameters: {format_number(sum(p.numel() for p in pretrained_bert_model.parameters()))}\\n\")\n",
    "print(pretrained_bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac8c42a",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febbc8ca",
   "metadata": {},
   "source": [
    "The DeepSeek-V2-Lite is a cutting-edge decoder-only transformer model optimized for causal language modeling, so for generating or completing text. With **15'706'484'224 parameters**, it is orders of magnitude larger than BERT and is specifically designed for autoregressive generation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a1b9b",
   "metadata": {},
   "source": [
    "For the DeepSeek-V2-Lite model, the architecture is composed of an embedding layer, a stack of decoder layers (transformer blocks), normalization layers and a final language modeling head for token prediction:\n",
    "\n",
    "1. **Token Embedding Layer:**\n",
    "- `Embedding(102400, 2048)` → Maps tokens from a very large vocabulary (102,400 tokens) to 2048-dimensional embeddings.\n",
    "\n",
    "2. **Stack of 27 Decoder Layers (`DeepseekV2DecoderLayer`):**\n",
    "- Each decoder layer includes:\n",
    "    - Self-Attention Mechanism (`DeepseekV2Attention`)\n",
    "        - Query projection: `Linear(2048 → 3072)`\n",
    "        - KV projections:\n",
    "            - `kv_a_proj_with_mqa`: `Linear(2048 → 576)` — Multi-query attention (MQA), a memory-efficient variant.\n",
    "            - `kv_b_proj`: `Linear(512 → 4096)` — Advanced attention processing.\n",
    "        - RMSNorm on KV inputs: Normalizes activations to improve stability.\n",
    "        - Rotary Embeddings (`DeepseekV2YarnRotaryEmbedding`) → Positional encoding mechanism that enables extrapolation to longer sequences.\n",
    "        - Output projection: `Linear(2048 → 2048)`\n",
    "    - Feed-Forward Layer\n",
    "        - Layer 0 uses:\n",
    "            - Standard MLP (`DeepseekV2MLP`)\n",
    "                - `gate_proj`, `up_proj`: `2048 → 10944`\n",
    "                - `down_proj`: `10944 → 2048`\n",
    "                - Activation: SiLU (a smooth, non-monotonic function similar to Swish)\n",
    "        - Layers 1–26 use:\n",
    "            - Mixture-of-Experts (MoE) Layer (`DeepseekV2MoE`)\n",
    "                - 64 expert MLPs (`DeepseekV2MLP`) with `2048 → 1408 → 2048`\n",
    "                - Gating mechanism: `MoEGate()` dynamically selects top-k experts per token.\n",
    "                - Shared expert also included: `2048 → 2816 → 2048`\n",
    "                - This makes computation sparse but increases capacity massively.\n",
    "    - Normalization\n",
    "        - RMSNorm instead of LayerNorm, used both before attention and before MLP. RMSNorm scales activations based on root mean square, which is more numerically stable in large-scale training.\n",
    "\n",
    "3. **Final LayerNorm + Output Head:**\n",
    "- Final RMSNorm applied to the output of the last decoder layer.\n",
    "- LM Head: `Linear(2048 → 102400)`\n",
    "    - Maps model outputs back into the token vocabulary for prediction.\n",
    "    - Weight sharing is likely applied with the token embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d01066",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82674b6c",
   "metadata": {},
   "source": [
    "With the following code cell I load the `DeepSeek-V2-Lite` large language model (https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite). This model is designed for causal language modeling tasks such as text generation. The loading configuration includes:\n",
    "- `AutoModelForCausalLM.from_pretrained(...)`: Loads the pretrained DeepSeek model.\n",
    "- `trust_remote_code=True`: Allows loading custom model code from the model's repository.\n",
    "- `torch_dtype=torch.bfloat16`: Uses the more efficient `bfloat16` precision for faster inference.\n",
    "- `device_map=\"cpu\"`: Loads the model onto the CPU.\n",
    "- `GenerationConfig.from_pretrained(...)`: Loads the model's default generation configuration (e.g., max tokens, sampling strategy).\n",
    "- `pad_token_id = eos_token_id`: Sets the padding token to be the same as the end-of-sequence token for compatibility during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d6325",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_model = AutoModelForCausalLM.from_pretrained('deepseek-ai/DeepSeek-V2-Lite', trust_remote_code=True, torch_dtype=torch.bfloat16, device_map=\"cpu\", low_cpu_mem_usage=True)\n",
    "deepseek_model.generation_config = GenerationConfig.from_pretrained('deepseek-ai/DeepSeek-V2-Lite')\n",
    "deepseek_model.generation_config.pad_token_id = deepseek_model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b70bd",
   "metadata": {},
   "source": [
    "I calculate and print the total number of parameters of the `deepseek_model` and then print its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96093558",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of parameters: {format_number(sum(p.numel() for p in deepseek_model.parameters()))}\\n\")\n",
    "print(deepseek_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaf3928",
   "metadata": {},
   "source": [
    "The function `process_commonsense_qa_for_deepseek(...)` in the next code cell is designed to process and evaluate the performance from the LLM on the CommonsenseQA dataset. Rather than using the dataset in a traditional classification setup (with logits over classes), this function reformulates each question into a text prompt that simulates a real-world use case: prompting a language model to directly generate the correct answer letter (e.g., \"A\", \"B\", \"C\"). This approach enables zero-shot evaluation of decoder-based language models (like DeepSeek, GPT-style models, etc.) by leveraging natural language prompts.\n",
    "\n",
    "Key features of the function include:\n",
    "- Prompt Construction: Each question is converted into a formatted prompt including the question, concept (if available) and multiple choice options labeled A–E.\n",
    "- Text Generation: The model is prompted to generate a single token representing the answer choice, using `max_new_tokens=1` to constrain output.\n",
    "- Answer Decoding: The raw model output is post-processed to extract the predicted answer letter.\n",
    "- Result Logging: The function collects questions, prompts, generated answers, and ground truth labels for easy evaluation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05105fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_commonsense_qa_for_deepseek(dataset, model, tokenizer, num_examples, max_new_tokens=1, batch_size=8):\n",
    "    results = {\n",
    "        'questions': [],\n",
    "        'prompts': [],\n",
    "        'responses': [],\n",
    "        'correct_answers': []\n",
    "    }\n",
    "\n",
    "    if num_examples is not None:\n",
    "        limited_dataset = dataset.select(range(min(num_examples, len(dataset))))\n",
    "    else:\n",
    "        limited_dataset = dataset\n",
    "\n",
    "    has_question_concept = 'question_concept' in limited_dataset.column_names\n",
    "    total = len(limited_dataset)\n",
    "\n",
    "    pbar = tqdm(total=total, desc=\"Generating answers\")\n",
    "\n",
    "    for batch_start in range(0, total, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, total)\n",
    "\n",
    "        batch_questions = []\n",
    "        batch_prompts = []\n",
    "        batch_answer_keys = []\n",
    "\n",
    "        for i in range(batch_start, batch_end):\n",
    "            question = limited_dataset['question'][i]\n",
    "            question_concept = limited_dataset['question_concept'][i] if has_question_concept else ''\n",
    "            choices = limited_dataset['choices'][i]\n",
    "            choice_labels = choices['label']\n",
    "            choice_texts = choices['text']\n",
    "            choices_text = \", \".join([f\"{label}. {text}\" for label, text in zip(choice_labels, choice_texts)])\n",
    "            answer_key = limited_dataset['answerKey'][i] if 'answerKey' in limited_dataset.column_names else \"N/A\"\n",
    "\n",
    "            prompt = (\n",
    "                f\"Answer the following multiple choice question with only the correct letter (A, B, C, D, or E). Do not explain your answer.\\n\"\n",
    "                f\"Question: {question}\\n\"\n",
    "                f\"Concept: {question_concept}\\n\"\n",
    "                f\"Choices: {choices_text}\\n\"\n",
    "                f\"Answer:\"\n",
    "            )\n",
    "\n",
    "            batch_questions.append(question)\n",
    "            batch_prompts.append(prompt)\n",
    "            batch_answer_keys.append(answer_key)\n",
    "\n",
    "        inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs, \n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            responses = [\n",
    "                out[len(prompt):].strip() if out.startswith(prompt) else out.strip()\n",
    "                for prompt, out in zip(batch_prompts, decoded)\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            responses = [f\"Error: {str(e)}\"] * len(batch_prompts)\n",
    "\n",
    "        results['questions'].extend(batch_questions)\n",
    "        results['prompts'].extend(batch_prompts)\n",
    "        results['responses'].extend(responses)\n",
    "        results['correct_answers'].extend(batch_answer_keys)\n",
    "\n",
    "        del inputs, outputs\n",
    "        gc.collect()\n",
    "\n",
    "        pbar.update(len(batch_prompts))\n",
    "\n",
    "    pbar.close()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41073d29",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596c651e",
   "metadata": {},
   "source": [
    "First, I define my GPU as the device to increase the speed of my training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4add2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183cd80",
   "metadata": {},
   "source": [
    "Next, I define all the needed hyperparameters for my <u>manual</u> training. Heres a description for all of the hyperparameters used:\n",
    "\n",
    "`epochs:` Number of times the model goes through the entire training dataset.<br>\n",
    "`learning_rate:` Controls how much the model updates its weights with each step.<br>\n",
    "`batch_size:` Number of samples processed together before updating model weights.<br>\n",
    "`warmup_steps:` Gradually increases learning rate over the first training steps to avoid instability.<br>\n",
    "`gradient_clip_val:` Limits the maximum gradient norm to prevent exploding gradients.<br>\n",
    "`save_interval:` Saves the model after every epoch.<br>\n",
    "`gradient_accumulation_steps:` Accumulates gradients over multiple steps before updating weights, effectively increasing the batch size.<br>\n",
    "`patience:` Stops training early if validation performance doesn’t improve for the set amount of epochs.<br>\n",
    "`weight_decay:` Applies L2 regularization to discourage overly large weights and reduce overfitting.<br>\n",
    "\n",
    "For the warmup steps I found, that the value is often set to 5-10% of the total training steps (https://medium.com/better-ml/the-art-of-setting-learning-rate-eff11ac0a737). To get for example the 10%, I would use this code:  `warmup_steps = 0.1 * len(train_dataloader)`. Due to the fact, that I wanted to have all my hyperparameters in one block (including the batch size, which is needed for the dataloader after) I had to find a different way to calculate those 10%. A different way to calculate this would be: `warmup_steps = int(0.1 * len(train_features) / batch_size)`. With this approach I could create the dataloaders later, while having all necessary hyperparameters in one block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34b06ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "learning_rate = 1e-5\n",
    "batch_size = 16\n",
    "warmup_steps = int(0.1 * len(train_features) / batch_size) # 0.1 = 10% of training data\n",
    "gradient_clip_val = 5.0\n",
    "save_interval = 1\n",
    "gradient_accumulation_steps = 4 # Effectively creates a batch size of 128 (batch_size * gradient_accumulation_steps)\n",
    "patience = 3\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351cbc22",
   "metadata": {},
   "source": [
    "In the next code cell, I define the data loaders for training, validation and testing. A DataLoader is responsible for efficiently loading batches of data during training or evaluation. Here's a breakdown of the configuration used for each dataset:\n",
    "- `train_features` / `val_features` / `test_features`: These are the preprocessed datasets for training, validation, and testing, respectively.\n",
    "- `batch_size`: Controls how many samples are passed through the model at once; defined earlier to ensure consistency.\n",
    "- `shuffle`:\n",
    "    - Set to `True` for the training set to ensure that the model sees a different order of examples each epoch (helps generalization).\n",
    "    - Set to `False` for validation and test sets to maintain deterministic behavior (important for consistent evaluation).\n",
    "- `num_workers=4`: Enables parallel data loading using 4 subprocesses. This speeds up data fetching, especially when I/O or preprocessing is involved.\n",
    "- `pin_memory=True`: Allows faster transfer of data from CPU to GPU by allocating the data in page-locked (pinned) memory — useful when training on a CUDA-enabled device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ff26a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_features, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_features, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_features, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1dee63",
   "metadata": {},
   "source": [
    "As stated in the markdown for the hyperparameters, I used a bit of a different calculation to declare the warmup steps. In the following code block, I look at the output value of both calculations to make sure that they're the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef952a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "print(warmup_steps)\n",
    "print(int(0.1 * len(train_dataloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9053b80d",
   "metadata": {},
   "source": [
    "The following `train_transformer` function runs the full training process for a transformer model using PyTorch. It includes helpful features like saving progress (checkpoints), stopping early if the model stops improving, adjusting the learning rate during training, combining gradients across batches to save memory, and optionally tracking results using Weights & Biases (WandB).\n",
    "\n",
    "**Inputs:**\n",
    "- `model`: A transformer model compatible with HuggingFace Transformers API.\n",
    "- `train_dataloader`, `val_dataloader`: DataLoaders for training and validation sets.\n",
    "- `device`: The device to train on (e.g., `\"cuda\"` or `\"cpu\"`).\n",
    "- `epochs`: Number of training epochs.\n",
    "- `learning_rate`: Learning rate for the optimizer.\n",
    "- `warmup_steps`: Warm-up steps for the scheduler. If `None`, defaults to one epoch's worth of steps.\n",
    "- `log_wandb`: Whether to log metrics and configs to WandB.\n",
    "- `gradient_clip_val`: Value for gradient clipping.\n",
    "- `save_interval`: How often (in epochs) to save model checkpoints.\n",
    "- `gradient_accumulation_steps`: Number of steps to accumulate gradients before updating weights.\n",
    "- `patience`: Patience for early stopping.\n",
    "- `weight_decay`: Adds L2 regularization to the optimizer, which helps with overfitting.\n",
    "- `save_path`: Directory path to save checkpoints and the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9fec2c",
   "metadata": {},
   "source": [
    "### Training Function\n",
    "\n",
    "**1. Checkpoint Saving Setup**\n",
    "- If `save_path` is provided:\n",
    "  - Create the checkpoint directory (if it doesn't exist).\n",
    "  - Define the path for saving the best model.\n",
    "\n",
    "**2. Model Preparation**\n",
    "- Move model to the specified `device` (CPU/GPU).\n",
    "- Enable gradient checkpointing (reduces memory usage, especially for large transformer models).\n",
    "\n",
    "**3. WandB Initialization**\n",
    "- If `log_wandb` is `True`:\n",
    "  - Import and initialize a Weights & Biases run.\n",
    "  - Log key hyperparameters and metadata.\n",
    "\n",
    "**4. Optimizer, Scheduler & Loss Setup**\n",
    "- Optimizer: `AdamW`\n",
    "- Scheduler: Linear with warmup\n",
    "  - `warmup_steps` defaults to 1 epoch’s steps if not specified.\n",
    "- Loss Function: `CrossEntropyLoss`\n",
    "\n",
    "**5. Training Loop (per Epoch)**\n",
    "\n",
    "**a. Training Phase**\n",
    "- Set model to `train()` mode.\n",
    "- For each batch in `train_dataloader`:\n",
    "  - Move inputs to `device`\n",
    "  - Forward pass → compute loss and predictions\n",
    "  - Scale loss for gradient accumulation\n",
    "  - Backpropagate loss\n",
    "  - Every `gradient_accumulation_steps`:\n",
    "    - Clip gradients\n",
    "    - Optimizer and scheduler step\n",
    "    - Zero gradients\n",
    "  - Log loss and update progress\n",
    "\n",
    "- Compute epoch-level metrics:\n",
    "  - Average training loss\n",
    "  - Training accuracy\n",
    "\n",
    "**b. Validation Phase**\n",
    "- Set model to `eval()` mode\n",
    "- Disable gradients with `torch.no_grad()`\n",
    "- For each batch in `val_dataloader`:\n",
    "  - Move inputs to `device`\n",
    "  - Forward pass → compute loss and predictions\n",
    "\n",
    "- Compute:\n",
    "  - Average validation loss\n",
    "  - Validation accuracy\n",
    "\n",
    "**c. Logging**\n",
    "- If WandB is enabled:\n",
    "  - Log training/validation loss, accuracy, and learning rate\n",
    "\n",
    "**d. Checkpointing**\n",
    "- If `epoch % save_interval == 0`:\n",
    "  - Save a checkpoint with:\n",
    "    - Model state\n",
    "    - Optimizer state\n",
    "    - Scheduler state\n",
    "    - Metadata\n",
    "\n",
    "**e. Best Model Saving & Early Stopping**\n",
    "- If validation accuracy is best so far:\n",
    "  - Save model and checkpoint\n",
    "  - Reset early stopping counter\n",
    "- Else:\n",
    "  - Increment counter\n",
    "  - Stop training if counter > `patience`\n",
    "\n",
    "**6. Load Best Model**\n",
    "- If a best model was saved, reload its weights.\n",
    "\n",
    "**7. Finalize WandB Run**\n",
    "- If `log_wandb` is `True`, call `wandb.finish()`.\n",
    "\n",
    "**Return Values**\n",
    "- `model`: Trained transformer model\n",
    "- `train_losses`: List of average training losses (per epoch)\n",
    "- `val_accuracies`: List of validation accuracies (per epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c0e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(model, train_dataloader, val_dataloader, device, \n",
    "                      epochs=10, learning_rate=1e-4, warmup_steps=None,\n",
    "                      log_wandb=True, gradient_clip_val=5.0, save_interval=1,\n",
    "                      gradient_accumulation_steps=4, patience=3,\n",
    "                      weight_decay=0.001, save_path=None, existing_wandb_run=None):\n",
    "        \n",
    "    if save_path:\n",
    "        checkpoint_dir = save_path\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        best_model_path = os.path.join(checkpoint_dir, \"best_transformer_model.pt\")\n",
    "    else:\n",
    "        print(\"Warning: No save_path provided. Model and checkpoints will not be saved.\")\n",
    "        checkpoint_dir = None\n",
    "        best_model_path = None\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    is_pretrained = hasattr(model.config, 'name_or_path') and 'bert' in model.config.name_or_path.lower()\n",
    "    \n",
    "    if is_pretrained:\n",
    "        model_type_name = \"pretrained_transformer\"\n",
    "    else:\n",
    "        model_type_name = \"random_transformer\"\n",
    "    \n",
    "    if log_wandb:\n",
    "        if existing_wandb_run is None:\n",
    "            import wandb\n",
    "            if not wandb.run:\n",
    "                run_name = f\"{model_type_name}-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "                wandb.init(\n",
    "                    project=\"CommonsenseQA\",\n",
    "                    name=run_name,\n",
    "                    config={\n",
    "                        \"learning_rate\": learning_rate,\n",
    "                        \"epochs\": epochs,\n",
    "                        \"batch_size\": train_dataloader.batch_size,\n",
    "                        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "                        \"effective_batch_size\": train_dataloader.batch_size * gradient_accumulation_steps,\n",
    "                        \"weight_decay\": weight_decay,\n",
    "                        \"warmup_steps\": warmup_steps,\n",
    "                        \"gradient_clip_val\": gradient_clip_val,\n",
    "                        \"model_type\": model_type_name,\n",
    "                    })\n",
    "                print(f\"Initialized new wandb run with name: {run_name}\")\n",
    "        else:\n",
    "            wandb = existing_wandb_run\n",
    "            print(f\"Using existing wandb run: {wandb.run.name}\")\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    total_steps = len(train_dataloader) * epochs // gradient_accumulation_steps\n",
    "    \n",
    "    if warmup_steps is None:\n",
    "        warmup_steps = len(train_dataloader)\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=warmup_steps, \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    print(f\"Training {model_type_name} model...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_correct = 0\n",
    "        epoch_total = 0\n",
    "        progress_bar = tqdm(train_dataloader, desc=\"Training\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            input_ids, attention_mask, token_type_ids, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            epoch_correct += (preds == labels).sum().item()\n",
    "            epoch_total += labels.size(0)\n",
    "            \n",
    "            loss_to_backward = loss / gradient_accumulation_steps\n",
    "            loss_to_backward.backward()\n",
    "\n",
    "            if (i + 1) % gradient_accumulation_steps == 0 or (i + 1) == len(train_dataloader):\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_val)\n",
    "                \n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if log_wandb and wandb.run:\n",
    "                    wandb.log({\"learning_rate\": scheduler.get_last_lr()[0]})\n",
    "            \n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_dataloader)\n",
    "        train_accuracy = epoch_correct / epoch_total\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Training loss: {avg_train_loss:.4f}, accuracy: {train_accuracy:.4f}\")\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(val_dataloader, desc=\"Validation\")\n",
    "            \n",
    "            for batch in progress_bar:\n",
    "                input_ids, attention_mask, token_type_ids, labels = [b.to(device) for b in batch]\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids\n",
    "                )\n",
    "                logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "                \n",
    "                loss = criterion(logits, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, preds = torch.max(logits, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                progress_bar.set_postfix({\"acc\": correct/total})\n",
    "            \n",
    "        val_accuracy = correct / total\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        print(f\"Validation loss: {avg_val_loss:.4f}, accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        if log_wandb and wandb.run:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"train_accuracy\": train_accuracy,\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"val_accuracy\": val_accuracy,\n",
    "                \"learning_rate\": scheduler.get_last_lr()[0],\n",
    "                \"model_type\": model_type_name\n",
    "            })\n",
    "        \n",
    "        if checkpoint_dir and (epoch + 1) % save_interval == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"{model_type_name}_checkpoint_epoch_{epoch+1}.pt\")\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            \n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model_to_save.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'train_losses': train_losses,\n",
    "                'val_accuracies': val_accuracies,\n",
    "                'model_type': model_type_name\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "        \n",
    "        if best_model_path and val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            best_model_path_with_type = os.path.join(os.path.dirname(best_model_path), \n",
    "                                                   f\"best_{model_type_name}_model.pt\")\n",
    "            torch.save(model_to_save.state_dict(), best_model_path_with_type)\n",
    "            print(f\"Best model saved to {best_model_path_with_type}\")\n",
    "            \n",
    "            best_checkpoint_path = os.path.join(checkpoint_dir, \n",
    "                                              f\"best_{model_type_name}_checkpoint_epoch_{epoch+1}.pt\")\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model_to_save.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'train_losses': train_losses,\n",
    "                'val_accuracies': val_accuracies,\n",
    "                'model_type': model_type_name\n",
    "            }\n",
    "            torch.save(checkpoint, best_checkpoint_path)\n",
    "            \n",
    "            if log_wandb and wandb.run:\n",
    "                wandb.run.summary[\"best_accuracy\"] = best_accuracy\n",
    "                wandb.run.summary[\"best_epoch\"] = epoch + 1\n",
    "                wandb.run.summary[\"model_type\"] = model_type_name\n",
    "            \n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            print(f\"No improvement for {early_stopping_counter} epochs\")\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(f\"Early stopping after {epoch+1} epochs\")\n",
    "                if log_wandb and wandb.run:\n",
    "                    wandb.run.summary[\"stopped_epoch\"] = epoch + 1\n",
    "                break\n",
    "    \n",
    "    if best_model_path and os.path.exists(best_model_path_with_type):\n",
    "        model.load_state_dict(torch.load(best_model_path_with_type))\n",
    "        print(f\"Loaded best model from {best_model_path_with_type}\")\n",
    "    \n",
    "    return model, train_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5223d12",
   "metadata": {},
   "source": [
    "The function in the next two cells are for training the pretrained and randomly initialized BERT models using the `train_transformer` function with specified hyperparameters and checkpoint saving enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103eedf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_pretrained_bert_model, trained_pretrained_bert_train_losses, trained_pretrained_bert_val_accuracies = train_transformer(\n",
    "    pretrained_bert_model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    device,\n",
    "    epochs=epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_steps=warmup_steps,\n",
    "    gradient_clip_val=gradient_clip_val,\n",
    "    save_interval=save_interval,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    patience=patience,\n",
    "    weight_decay=weight_decay,\n",
    "    save_path=f\"./checkpoints/pretrained_transformer-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1637d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_random_bert_model, trained_random_bert_train_losses, trained_random_bert_val_accuracies = train_transformer(\n",
    "    random_bert_model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    device,\n",
    "    epochs=epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_steps=warmup_steps,\n",
    "    gradient_clip_val=gradient_clip_val,\n",
    "    save_interval=save_interval,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    patience=patience,\n",
    "    weight_decay=weight_decay,\n",
    "    save_path=f\"./checkpoints/random_transformer-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158629d",
   "metadata": {},
   "source": [
    "### Sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f1306",
   "metadata": {},
   "source": [
    "To explore optimal hyperparameters for both the pretrained and randomly initialized BERT models, I defined two sweep functions: `run_sweep_pretrained` and `run_sweep_random`. Both make use of the `train_transformer` training loop and share the same sweep configuration values (project requirement). Each sweep run initializes a fresh instance of the corresponding model (to ensure that the model is being reset correctly) and runs training and validation with its own `wandb` session. At the end of each run, the best validation accuracy is logged for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb01e4e",
   "metadata": {},
   "source": [
    "### ⚙️ Explanation of Hyperparameter Sweep Settings\n",
    "\n",
    "To optimize the performance of both the **pretrained** and **randomly initialized** BERT models on CommonsenseQA, we used a shared sweep configuration. This design choice ensures a **fair comparison** between models by evaluating them under **identical training conditions**.\n",
    "\n",
    "Below is a breakdown of each hyperparameter and the rationale behind the selected values or ranges:\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔢 `batch_size`: `[32]`\n",
    "\n",
    "- **Reasoning:** A batch size of 32 was selected as a balance between stability of gradient updates and memory efficiency.\n",
    "- **Fixed Value:** We kept it fixed to reduce sweep complexity and because larger batches didn’t fit in GPU memory during preliminary testing.\n",
    "- **Consistency:** Ensures that both models are trained under the same computational budget and batch size semantics.\n",
    "\n",
    "---\n",
    "\n",
    "#### 📉 `learning_rate`: `log_uniform_values from 1e-6 to 1e-4`\n",
    "\n",
    "- **Why log scale?** Transformers are highly sensitive to the learning rate. Small changes can drastically affect convergence, making log scale more appropriate.\n",
    "- **Range Justification:**\n",
    "  - `1e-6`: For stable but slow training, often needed when fine-tuning pretrained models.\n",
    "  - `1e-4`: Allows exploration of slightly more aggressive updates, particularly relevant for training randomly initialized models.\n",
    "- **Empirical Range:** These bounds are widely used in BERT fine-tuning literature and cover both safe and exploratory values.\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚖️ `weight_decay`: `log_uniform_values from 1e-6 to 1e-2`\n",
    "\n",
    "- **Purpose:** Regularizes the model by penalizing large weights, helping to avoid overfitting.\n",
    "- **Range Justification:**\n",
    "  - `1e-6`: Minimal regularization—safe starting point, especially for pretrained models.\n",
    "  - `1e-2`: Stronger regularization—useful for random initialization where overfitting can be more prevalent.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✂️ `gradient_clip_val`: `uniform from 0.5 to 2.0`\n",
    "\n",
    "- **Reasoning:** Clipping prevents exploding gradients, especially useful in unstable early training phases (e.g., with untrained models).\n",
    "- **Range Justification:**\n",
    "  - Lower bound `0.5`: Conservative clipping.\n",
    "  - Upper bound `2.0`: Allows gradients to retain enough magnitude for effective updates.\n",
    "- **Empirically Effective:** These values were drawn from common transformer training practices.\n",
    "\n",
    "---\n",
    "\n",
    "#### 📊 `gradient_accumulation_steps`: `[1, 2, 4]`\n",
    "\n",
    "- **Motivation:** Simulates larger batch sizes without exceeding GPU memory. Helps smooth out noisy gradients, especially when using a small physical batch size.\n",
    "- **Choice of Values:**\n",
    "  - `1`: Baseline (no accumulation).\n",
    "  - `2, 4`: Increased effective batch size without increasing memory usage.\n",
    "- **Trade-off:** Higher values improve stability but slow down per-step learning.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔥 `warmup_ratio`: `[0.1]`\n",
    "\n",
    "- **Why 10%?** Warming up the learning rate over 10% of total steps helps stabilize training, especially early on.\n",
    "- **Transformer Norm:** 0.1 is a standard warmup ratio in most BERT training pipelines (e.g., from the original BERT paper and HuggingFace defaults).\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔁 `epochs`: `[50]`\n",
    "\n",
    "- **Why 50?** Ensures sufficient training time for convergence, particularly for the randomly initialized model which typically learns more slowly.\n",
    "- **Avoiding Truncation:** Prevents early termination of sweeps before reaching peak performance.\n",
    "- **Mitigated by Early Stopping:** Since we use patience-based early stopping, long epoch counts don't necessarily waste compute.\n",
    "\n",
    "---\n",
    "\n",
    "#### ⏳ `patience`: `[5]`\n",
    "\n",
    "- **Function:** Early stopping prevents overfitting and saves compute by halting training if validation accuracy doesn’t improve.\n",
    "- **Why 5?** Allows for minor plateaus in learning, giving models a fair chance to recover while still avoiding prolonged stagnation.\n",
    "- **Balance:** Offers a good trade-off between learning opportunities and training efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Final Note\n",
    "\n",
    "All hyperparameters are swept across **both model types using the exact same configuration**. This assumption is based on the **project goal of evaluating architecture performance differences**, not training strategy differences. Uniform sweep settings make comparison results more interpretable and scientifically valid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "716b5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function_pretrained(config=None):\n",
    "    \"\"\"Objective function for hyperparameter optimization of pretrained model\"\"\"\n",
    "    \n",
    "    with wandb.init(\n",
    "    project=\"CommonsenseQA\",\n",
    "    name=f\"pretrained_transformer-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "    ) as run:\n",
    "        config = wandb.config\n",
    "\n",
    "        # Always create a fresh model instance for each run\n",
    "        pretrained_bert_model = BertForMultipleChoice.from_pretrained('bert-base-cased')\n",
    "        \n",
    "        # Create dataloaders with the batch size from config\n",
    "        train_batch_size = config.batch_size\n",
    "        \n",
    "        train_dataloader = DataLoader(train_features,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_dataloader = DataLoader(\n",
    "            val_features,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Generate a unique save path for this run\n",
    "        unique_save_path = f\"./checkpoints/sweep-{wandb.run.id}-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        \n",
    "        # Use the training function with model_pretrained and pass the existing wandb run\n",
    "        trained_model, train_losses, val_accuracies = train_transformer(\n",
    "            model=pretrained_bert_model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            device=device,\n",
    "            epochs=config.epochs,\n",
    "            learning_rate=config.learning_rate,\n",
    "            warmup_steps=config.warmup_ratio * len(train_dataloader),\n",
    "            gradient_clip_val=config.gradient_clip_val,\n",
    "            save_interval=1,\n",
    "            gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "            patience=config.patience,\n",
    "            weight_decay=config.weight_decay,\n",
    "            save_path=unique_save_path,\n",
    "            log_wandb=True,\n",
    "            existing_wandb_run=wandb  # Pass existing wandb run\n",
    "        )\n",
    "        \n",
    "        # Make sure to clean up properly\n",
    "        del trained_model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        # Return the best validation accuracy\n",
    "        best_val_accuracy = max(val_accuracies)\n",
    "        wandb.log({\"best_val_accuracy\": best_val_accuracy})\n",
    "        return best_val_accuracy\n",
    "\n",
    "def objective_function_random(config=None):\n",
    "    \"\"\"Objective function for hyperparameter optimization of random model\"\"\"\n",
    "    \n",
    "    with wandb.init(\n",
    "    project=\"CommonsenseQA\",\n",
    "    name=f\"random_transformer-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "    ) as run:\n",
    "        config = wandb.config\n",
    "\n",
    "        # Always create a fresh model instance for each run\n",
    "        config_bert = BertConfig.from_pretrained('bert-base-cased')\n",
    "        random_bert_model = BertForMultipleChoice(config_bert)\n",
    "        \n",
    "        # Create dataloaders with the batch size from config\n",
    "        train_batch_size = config.batch_size\n",
    "        \n",
    "        train_dataloader = DataLoader(train_features,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_dataloader = DataLoader(\n",
    "            val_features,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Generate a unique save path for this run\n",
    "        unique_save_path = f\"./checkpoints/sweep-{wandb.run.id}-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        \n",
    "        # Use the training function with model_random and pass the existing wandb run\n",
    "        trained_model, train_losses, val_accuracies = train_transformer(\n",
    "            model=random_bert_model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            device=device,\n",
    "            epochs=config.epochs,\n",
    "            learning_rate=config.learning_rate,\n",
    "            warmup_steps=config.warmup_ratio * len(train_dataloader),\n",
    "            gradient_clip_val=config.gradient_clip_val,\n",
    "            save_interval=1,\n",
    "            gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "            patience=config.patience,\n",
    "            weight_decay=config.weight_decay,\n",
    "            save_path=unique_save_path,\n",
    "            log_wandb=True,\n",
    "            existing_wandb_run=wandb  # Pass existing wandb run\n",
    "        )\n",
    "        \n",
    "        # Make sure to clean up properly\n",
    "        del trained_model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Return the best validation accuracy\n",
    "        best_val_accuracy = max(val_accuracies)\n",
    "        wandb.log({\"best_val_accuracy\": best_val_accuracy})\n",
    "        return best_val_accuracy\n",
    "\n",
    "def run_sweep_pretrained(count):\n",
    "    \"\"\"Run hyperparameter sweep for the pretrained model\"\"\"\n",
    "    # Define the parameter search space\n",
    "    sweep_config = {\n",
    "        'method': 'bayes',\n",
    "        'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
    "        'name': 'pretrained_model_sweep',\n",
    "        'parameters': {\n",
    "            'batch_size': {\n",
    "                'values': [32]\n",
    "            },\n",
    "            'learning_rate': {\n",
    "                'distribution': 'log_uniform_values',\n",
    "                'min': 1e-6,\n",
    "                'max': 1e-4\n",
    "            },\n",
    "            'weight_decay': {\n",
    "                'distribution': 'log_uniform_values',\n",
    "                'min': 1e-6,\n",
    "                'max': 1e-2\n",
    "            },\n",
    "            'gradient_clip_val': {\n",
    "                'distribution': 'uniform',\n",
    "                'min': 0.5,\n",
    "                'max': 2.0\n",
    "            },\n",
    "            'gradient_accumulation_steps': {\n",
    "                'values': [1, 2, 4]\n",
    "            },\n",
    "            'warmup_ratio': {\n",
    "                'values': [0.1]\n",
    "            },\n",
    "            'epochs': {\n",
    "                'values': [50]\n",
    "            },\n",
    "            'patience': {\n",
    "                'values': [5]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create the sweep\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"CommonsenseQA\")\n",
    "    \n",
    "    # Run the sweep\n",
    "    wandb.agent(sweep_id, function=objective_function_pretrained, count=count)\n",
    "\n",
    "def run_sweep_random(count):\n",
    "    \"\"\"Run hyperparameter sweep for the randomly initialized model\"\"\"\n",
    "    # Define the parameter search space\n",
    "    # For random initialization, we might want to explore different learning rates\n",
    "    sweep_config = {\n",
    "        'method': 'bayes',\n",
    "        'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
    "        'name': 'pretrained_model_sweep',\n",
    "        'parameters': {\n",
    "            'batch_size': {\n",
    "                'values': [32]\n",
    "            },\n",
    "            'learning_rate': {\n",
    "                'distribution': 'log_uniform_values',\n",
    "                'min': 1e-6,\n",
    "                'max': 1e-4\n",
    "            },\n",
    "            'weight_decay': {\n",
    "                'distribution': 'log_uniform_values',\n",
    "                'min': 1e-6,\n",
    "                'max': 1e-2\n",
    "            },\n",
    "            'gradient_clip_val': {\n",
    "                'distribution': 'uniform',\n",
    "                'min': 0.5,\n",
    "                'max': 2.0\n",
    "            },\n",
    "            'gradient_accumulation_steps': {\n",
    "                'values': [1, 2, 4]\n",
    "            },\n",
    "            'warmup_ratio': {\n",
    "                'values': [0.1]\n",
    "            },\n",
    "            'epochs': {\n",
    "                'values': [50]\n",
    "            },\n",
    "            'patience': {\n",
    "                'values': [5]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create the sweep\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"CommonsenseQA\")\n",
    "    \n",
    "    # Run the sweep\n",
    "    wandb.agent(sweep_id, function=objective_function_random, count=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting sweep for pretrained model...\")\n",
    "run_sweep_pretrained(count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b90ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting sweep for randomly initialized model...\")\n",
    "run_sweep_random(count=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61866a81",
   "metadata": {},
   "source": [
    "# **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8fe1f8",
   "metadata": {},
   "source": [
    "Important: Use test split for eval, not validation (& ofc no train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "650d4be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    all_logits = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Evaluation\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            input_ids, attention_mask, token_type_ids, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = outputs.logits\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            \n",
    "            # Collect labels and predictions\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(preds.cpu().numpy())\n",
    "            all_logits.append(logits.cpu().numpy())\n",
    "    \n",
    "    # Combine all logits\n",
    "    all_logits = np.vstack(all_logits) if all_logits else np.array([])\n",
    "    \n",
    "    # Compute metrics\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score, \n",
    "        precision_score, \n",
    "        recall_score, \n",
    "        f1_score, \n",
    "        confusion_matrix,\n",
    "        classification_report\n",
    "    )\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    \n",
    "    # Only calculate these metrics if there are predictions for each class\n",
    "    try:\n",
    "        precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "        recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "        f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    except:\n",
    "        print(\"Warning: Some classes may not have predictions. Using only accuracy.\")\n",
    "        precision = recall = f1 = None\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    \n",
    "    # Class-wise report \n",
    "    class_report = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "    \n",
    "    # Create a list to map index to answer choice label (A-E)\n",
    "    idx_to_label = {i: chr(65 + i) for i in range(5)}  # 0->A, 1->B, etc.\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    class_accuracies = {}\n",
    "    for i in range(5):\n",
    "        class_indices = np.where(np.array(true_labels) == i)[0]\n",
    "        if len(class_indices) > 0:\n",
    "            class_correct = sum([predicted_labels[j] == i for j in class_indices])\n",
    "            class_accuracies[idx_to_label[i]] = class_correct / len(class_indices)\n",
    "        else:\n",
    "            class_accuracies[idx_to_label[i]] = 0\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'class_report': class_report,\n",
    "        'per_class_accuracy': class_accuracies,\n",
    "        'logits': all_logits\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Per-class accuracy:\")\n",
    "    for label, acc in class_accuracies.items():\n",
    "        print(f\"  Choice {label}: {acc:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f76ca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pretrained_bert_model = BertForMultipleChoice.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49e005aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 77/77 [00:08<00:00,  9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.5397\n",
      "Per-class accuracy:\n",
      "  Choice A: 0.5732\n",
      "  Choice B: 0.5686\n",
      "  Choice C: 0.4855\n",
      "  Choice D: 0.5538\n",
      "  Choice E: 0.5149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# best model in theory\n",
    "best_pretrained_model_path = \"./checkpoints/sweep-ui8fb374-2025-05-14_01-51-35/best_pretrained_transformer_model.pt\"\n",
    "pretrained_bert_model.load_state_dict(torch.load(best_pretrained_model_path))\n",
    "pretrained_bert_model.to(device)\n",
    "pretrained_bert_model.eval()\n",
    "test_results_pretrained_model = evaluate(pretrained_bert_model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "953ac9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the confusion matrix\n",
    "cm = test_results_pretrained_model['confusion_matrix']\n",
    "\n",
    "# Define class labels (A-E)\n",
    "labels = ['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca2b1c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config_bert = BertConfig.from_pretrained('bert-base-cased')\n",
    "random_bert_model = BertForMultipleChoice(config_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c226f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 77/77 [00:08<00:00,  9.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.1818\n",
      "Per-class accuracy:\n",
      "  Choice A: 0.1548\n",
      "  Choice B: 0.1843\n",
      "  Choice C: 0.1618\n",
      "  Choice D: 0.2311\n",
      "  Choice E: 0.1745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# best model in theory\n",
    "best_random_model_path = \"./checkpoints/sweep-t666b3vz-2025-05-15_00-57-35/best_random_transformer_model.pt\"\n",
    "random_bert_model.load_state_dict(torch.load(best_random_model_path))\n",
    "random_bert_model.to(device)\n",
    "random_bert_model.eval()\n",
    "test_results_random_model = evaluate(random_bert_model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf93b93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the confusion matrix\n",
    "cm = test_results_random_model['confusion_matrix']\n",
    "\n",
    "# Define class labels (A-E)\n",
    "labels = ['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2730b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1221\n"
     ]
    }
   ],
   "source": [
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "948de111",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_commonsense_qa_for_deepseek' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# For the validation set (1 example)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[43mprocess_commonsense_qa_for_deepseek\u001b[49m(test, deepseek_model, tokenizer_deepseek, num_examples=\u001b[32m100\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'process_commonsense_qa_for_deepseek' is not defined"
     ]
    }
   ],
   "source": [
    "# For the validation set (1 example)\n",
    "results = process_commonsense_qa_for_deepseek(test, deepseek_model, tokenizer_deepseek, num_examples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b95e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results['questions'])\n",
    "print(results['prompts'])\n",
    "print(results['responses'])\n",
    "print(results['correct_answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bd0ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for i in range(len(results['responses'])):\n",
    "    if results['correct_answers'][i] == results['responses'][i]:\n",
    "        count += 1\n",
    "print(f\"Accuracy: {count / len(results['responses']) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83e495",
   "metadata": {},
   "source": [
    "# **Interpretation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363fea4",
   "metadata": {},
   "source": [
    "# **Tools used**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0bf1a1",
   "metadata": {},
   "source": [
    "### **Adjust this section before submitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8479f8",
   "metadata": {},
   "source": [
    "1. **Programming Environment**\n",
    "   - Python 3.12.8\n",
    "   - Jupyter Notebook\n",
    "\n",
    "2. **Machine Learning and Deep Learning**\n",
    "   - PyTorch (neural network development)\n",
    "   - Hugging Face Datasets (data management)\n",
    "   - NLTK (natural language preprocessing)\n",
    "   - FastText (pre-trained word embeddings, 300-dimensional vectors)\n",
    "\n",
    "3. **Data Manipulation and Analysis**\n",
    "   - NumPy (numerical computing)\n",
    "   - Pandas (data structuring and manipulation)\n",
    "   - Scikit-learn (potential additional machine learning utilities)\n",
    "\n",
    "4. **Visualization and Tracking**\n",
    "   - Matplotlib (basic plotting)\n",
    "   - Seaborn (statistical data visualization)\n",
    "   - Weights & Biases (experiment tracking and logging)\n",
    "     * Tracked metrics: training loss, accuracy, learning rates\n",
    "     * Logged hyperparameter configurations\n",
    "     * Enabled comparative analysis across model runs\n",
    "\n",
    "5. **Computational Infrastructure**\n",
    "   - CUDA-enabled GPU acceleration\n",
    "   - GPU-optimized PyTorch operations\n",
    "   - Efficient parallel computing for model training\n",
    "\n",
    "6. **Dataset and Benchmarking**\n",
    "   - CommonsenseQA dataset (Hugging Face)\n",
    "   - Standard benchmark for commonsense reasoning tasks\n",
    "\n",
    "7. **Additional Libraries**\n",
    "   - Gensim (word vector processing)\n",
    "   - tqdm (progress bar visualization)\n",
    "   - datetime (experiment timestamping)\n",
    "\n",
    "8. **AI-Tools**\n",
    "   - Claude 3.5 Sonnet: Utilized as a coding assistant for debugging, optimization and documentation.\n",
    "   - GPT-4-turbo: Assisted in drafting and refining documentation, helping with structure and phrasing.\n",
    "   - Copilot: Used for quick inserts, when recommendation was suitable for what I was planning to do.\n",
    "\n",
    "9. **Sources**\n",
    "   - Transformer architecture: https://medium.com/data-science/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\n",
    "   - Deepseek implementation: https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite\n",
    "   - Medium blog for warmup steps: https://medium.com/better-ml/the-art-of-setting-learning-rate-eff11ac0a737"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
