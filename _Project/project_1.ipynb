{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wandb workspace URL: https://wandb.ai/fabian-dubach-hochschule-luzern/experiment-tracking/runs/gtg68kwg/workspace?nw=nwuserfabiandubach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setup**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WandDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "wandb.login(key=\"\")\n",
    "wandb_logger = WandbLogger(project=\"experiment-tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Dateien\\Github_FabianDubach\\NLP\\_Project\\wandb\\run-20250320_172214-2o5jk1mt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fabian-dubach-hochschule-luzern/experiment-tracking/runs/2o5jk1mt' target=\"_blank\">experiment_1</a></strong> to <a href='https://wandb.ai/fabian-dubach-hochschule-luzern/experiment-tracking' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fabian-dubach-hochschule-luzern/experiment-tracking' target=\"_blank\">https://wandb.ai/fabian-dubach-hochschule-luzern/experiment-tracking</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fabian-dubach-hochschule-luzern/experiment-tracking/runs/2o5jk1mt' target=\"_blank\">https://wandb.ai/fabian-dubach-hochschule-luzern/experiment-tracking/runs/2o5jk1mt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def init_wandb_run(project_name, run_name, config_dict):\n",
    "    run = wandb.init(\n",
    "        project=project_name,\n",
    "        name=run_name,\n",
    "        config=config_dict,\n",
    "    )\n",
    "    return run\n",
    "\n",
    "\n",
    "#### Example Run\n",
    "experiment_1_config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 64,\n",
    "    \"optimizer\": \"adam\"\n",
    "}\n",
    "run1 = init_wandb_run(\"experiment-tracking\", \"experiment_1\", experiment_1_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import trange\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8741 1000 1221\n"
     ]
    }
   ],
   "source": [
    "train = load_dataset(\"tau/commonsense_qa\", split=\"train[:-1000]\")\n",
    "valid = load_dataset(\"tau/commonsense_qa\", split=\"train[-1000:]\")\n",
    "test = load_dataset(\"tau/commonsense_qa\", split=\"validation\")\n",
    "\n",
    "print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\fabia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(sentence):\n",
    "    \"\"\"Convert a sentence into a fixed-size vector by averaging word embeddings.\"\"\"\n",
    "    tokens = preprocessing(sentence)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    word_vectors = []\n",
    "    for token in tokens:\n",
    "        # If the word exists in the GloVe model, get its vector\n",
    "        if token in glove_model:\n",
    "            word_vectors.append(glove_model[token])\n",
    "        else:\n",
    "            # Use a zero vector for missing words\n",
    "            word_vectors.append(np.zeros(glove_model.vector_size))\n",
    "    \n",
    "    # Return the mean of the word vectors\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(glove_model.vector_size)  # Return a zero vector if no tokens were found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8741/8741 [00:08<00:00, 973.40 examples/s] \n",
      "Map: 100%|██████████| 1000/1000 [00:01<00:00, 962.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def compute_embeddings(example):\n",
    "    question_emb = get_embedding(example[\"question\"])\n",
    "    choice_embs = [get_embedding(choice) for choice in example[\"choices\"][\"text\"]]\n",
    "    \n",
    "    # Save embeddings as lists so they can be stored in the dataset\n",
    "    example[\"question_emb\"] = question_emb.tolist()\n",
    "    example[\"choice_embs\"] = [emb.tolist() for emb in choice_embs]\n",
    "    return example\n",
    "\n",
    "train = train.map(compute_embeddings)\n",
    "valid = valid.map(compute_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_key_to_index(answer_key):\n",
    "  \"\"\"Answer key (A-E) to index (0-4)\"\"\"\n",
    "  return ord(answer_key) - ord(\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonsenseQADataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        question_tensor = torch.tensor(example[\"question_emb\"]).float()\n",
    "        choices_tensor = torch.tensor(example[\"choice_embs\"]).float()\n",
    "        answer_index = answer_key_to_index(example[\"answerKey\"])\n",
    "        return question_tensor, choices_tensor, torch.tensor(answer_index).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CommonsenseQADataset(train)\n",
    "valid_dataset = CommonsenseQADataset(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 300]) torch.Size([32, 5, 300]) torch.Size([32]) tensor([3, 3, 1, 4, 3, 4, 4, 4, 0, 2, 2, 3, 1, 3, 1, 4, 3, 4, 4, 4, 0, 1, 4, 2,\n",
      "        3, 4, 3, 0, 1, 3, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch[0].shape, batch[1].shape, batch[2].shape, batch[2])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingQAClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim=128):\n",
    "        super(WordEmbeddingQAClassifier, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(2 * embedding_dim, hidden_dim) # the input dimension is doubled due to concatenation (question + choice)\n",
    "        self.relu = nn.ReLU() # non-linearity\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1) # single score per candidate\n",
    "\n",
    "    def forward(self, question, choices):\n",
    "        # question: (batch_size, embedding_dim)\n",
    "        # choices: (batch_size, 5, embedding_dim)\n",
    "\n",
    "        # expand question to match the choices dimension\n",
    "        question_expanded = question.unsqueeze(1).expand(-1, choices.size(1), -1) # (batch_size, 5, embedding_dim)\n",
    "        \n",
    "        # concatenate question and choice embeddings\n",
    "        combined = torch.cat((question_expanded, choices), dim=2) # (batch_size, 5, 2*embedding_dim)\n",
    "\n",
    "        # pass through the classifier\n",
    "        x = self.fc1(combined)  # (batch_size, 5, hidden_dim)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)         # (batch_size, 5, 1)\n",
    "        return x.squeeze(-1)    # (batch_size, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordEmbeddingQAClassifier(\n",
      "  (fc1): Linear(in_features=600, out_features=32, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = WordEmbeddingQAClassifier(embedding_dim=300, hidden_dim=32)\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=run1.config.learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100/100: 100%|██████████| 100/100 [34:04<00:00, 20.44s/it, train_loss=0.408, train_acc=0.87, val_acc=0.251]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▃▃▃▄▄▄▄▄▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>train_loss</td><td>████▇▇▆▆▆▅▅▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>█▆▇▄▄▆▄▄▃▂▆▆▃▄▃▄▄▆▄▅▅▆▄▄▄▃▃▂▂▃▄▃▃▃▂▂▂▁▃▁</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>train_accuracy</td><td>0.8705</td></tr><tr><td>train_loss</td><td>0.40829</td></tr><tr><td>val_accuracy</td><td>0.251</td></tr><tr><td>val_loss</td><td>3.94493</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experiment_1</strong> at: <a href='https://wandb.ai/fabian-dubach-hochschule-luzern/experiment-tracking/runs/2o5jk1mt' target=\"_blank\">https://wandb.ai/fabian-dubach-hochschule-luzern/experiment-tracking/runs/2o5jk1mt</a><br> View project at: <a href='https://wandb.ai/fabian-dubach-hochschule-luzern/experiment-tracking' target=\"_blank\">https://wandb.ai/fabian-dubach-hochschule-luzern/experiment-tracking</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250320_172214-2o5jk1mt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = run1.config.epochs\n",
    "\n",
    "for epoch in (pbar := trange(num_epochs)):\n",
    "    pbar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    model.train()\n",
    "    train_total_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for question_batch, choices_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad() # zero all the parameter gradients\n",
    "\n",
    "        question_batch, choices_batch, y_batch = question_batch.to(device), choices_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(question_batch, choices_batch) # (batch_size, 5)\n",
    "\n",
    "        # Compute loss\n",
    "        train_batch_loss = criterion(outputs, y_batch)\n",
    "        train_total_loss += train_batch_loss.item()\n",
    "\n",
    "        # Compute accuracy\n",
    "        train_predictions = torch.argmax(outputs, dim=1) # get most likely class\n",
    "        train_correct += (train_predictions == y_batch).sum().item() # count correct predictions\n",
    "        train_total += y_batch.size(0) # count total number of examples\n",
    "\n",
    "        # Backward pass\n",
    "        train_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate train statistics\n",
    "    avg_train_loss = train_total_loss / len(train_loader) # average loss per batch\n",
    "    train_accuracy = train_correct / train_total # overall accuracy\n",
    "\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total  = 0\n",
    "    val_total_loss  = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for question_batch, choices_batch, y_batch in valid_loader:\n",
    "            question_batch, choices_batch, y_batch = question_batch.to(device), choices_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            val_outputs = model(question_batch, choices_batch) # (batch_size, 5)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_batch_loss = criterion(val_outputs, y_batch)\n",
    "            val_total_loss += val_batch_loss.item()\n",
    "\n",
    "            val_predictions = torch.argmax(val_outputs, dim=1) # get most likely class\n",
    "            val_correct += (val_predictions == y_batch).sum().item() # count correct predictions\n",
    "            val_total += y_batch.size(0) # count total number of examples\n",
    "\n",
    "    # Calculate validation statistics\n",
    "    avg_val_loss = val_total_loss / len(valid_loader) # average loss per batch\n",
    "    val_accuracy = val_correct / val_total # overall accuracy\n",
    "    \n",
    "    pbar.set_postfix({\"train_loss\": avg_train_loss, \"train_acc\": train_accuracy, \"val_acc\": val_accuracy})\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": avg_train_loss,\n",
    "        \"train_accuracy\": train_accuracy,\n",
    "        \"val_loss\": avg_val_loss,\n",
    "        \"val_accuracy\": val_accuracy,\n",
    "        \"learning_rate\": optimizer.param_groups[0]['lr'], # current learning rate\n",
    "    })\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
