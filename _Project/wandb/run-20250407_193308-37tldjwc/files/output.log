
Epoch 1/50
Training:   0%|          | 0/137 [00:00<?, ?it/s]C:\Users\fabia\AppData\Local\Temp\ipykernel_9932\665787113.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  question_tensors = [torch.tensor(q, dtype=torch.float32) for q in questions]
C:\Users\fabia\AppData\Local\Temp\ipykernel_9932\665787113.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  choice_tensors = [[torch.tensor(c, dtype=torch.float32) for c in cs] for cs in choices]
c:\Users\fabia\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\nn\modules\rnn.py:1136: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\cudnn\RNN.cpp:1412.)
  result = _VF.lstm(
Training:   0%|          | 0/137 [00:00<?, ?it/s]
cuda
QARNNModel(
  (question_encoder): Sequential(
    (0): Linear(in_features=300, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
  )
  (choice_encoder): Sequential(
    (0): Linear(in_features=300, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
  )
  (scoring): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=1, bias=True)
  )
)
